{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MP5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy_3o_xuArab"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YbmihoOp560x",
        "outputId": "d9f46835-7aaa-49e9-bd85-455ee08b5220"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fDzG61B_A6OE",
        "outputId": "68057cce-c957-4439-9f0f-552d7a917614"
      },
      "source": [
        "cd drive/MyDrive/assignment5_materials/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/assignment5_materials\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE62jIy5Arab"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xfPbUh0IArab",
        "outputId": "ef0125a3-c901-4886-c60e-90200f1a5fbc"
      },
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-cuHXOKPArab",
        "outputId": "467676cd-d0b2-446e-ebe4-7f97c03d63b3"
      },
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.0.0)\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.6/dist-packages (0.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDILdeJcArab"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uyYpFOVArac"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXR2qtSvArac"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yodnDPLdArac"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttojls3aArac"
      },
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DGrcarJArac"
      },
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlYXAGwiArac"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWRuiFpArac"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b50DX_7cArac"
      },
      "source": [
        "double_dqn = True # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "  from agent_double import Agent\n",
        "else:\n",
        "  from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XZakbYOArac"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWeXoPJkArac"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36442
        },
        "id": "HzSbpRVkArac",
        "outputId": "ae24aa1f-9cf6-40a2-bff7-5e3d1eee29c3"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 8 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 8 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n",
        "                sys.exit()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 1.0   memory length: 170   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 1   score: 1.0   memory length: 339   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 2   score: 7.0   memory length: 737   epsilon: 1.0    steps: 398    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 3   score: 2.0   memory length: 934   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 4   score: 2.0   memory length: 1132   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 5   score: 3.0   memory length: 1395   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
            "episode: 6   score: 0.0   memory length: 1518   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.2857142857142856\n",
            "episode: 7   score: 0.0   memory length: 1641   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 8   score: 2.0   memory length: 1839   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 9   score: 1.0   memory length: 2010   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 10   score: 0.0   memory length: 2133   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7272727272727273\n",
            "episode: 11   score: 2.0   memory length: 2331   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 12   score: 2.0   memory length: 2510   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.7692307692307692\n",
            "episode: 13   score: 1.0   memory length: 2661   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
            "episode: 14   score: 0.0   memory length: 2784   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 15   score: 3.0   memory length: 3032   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6875\n",
            "episode: 16   score: 1.0   memory length: 3202   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6470588235294117\n",
            "episode: 17   score: 1.0   memory length: 3370   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
            "episode: 18   score: 2.0   memory length: 3549   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.631578947368421\n",
            "episode: 19   score: 4.0   memory length: 3829   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 20   score: 0.0   memory length: 3952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
            "episode: 21   score: 1.0   memory length: 4103   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
            "episode: 22   score: 1.0   memory length: 4273   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.608695652173913\n",
            "episode: 23   score: 2.0   memory length: 4491   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.625\n",
            "episode: 24   score: 3.0   memory length: 4764   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 25   score: 2.0   memory length: 4981   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
            "episode: 26   score: 2.0   memory length: 5179   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7037037037037037\n",
            "episode: 27   score: 2.0   memory length: 5397   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
            "episode: 28   score: 0.0   memory length: 5520   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6551724137931034\n",
            "episode: 29   score: 0.0   memory length: 5643   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 30   score: 1.0   memory length: 5794   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5806451612903225\n",
            "episode: 31   score: 2.0   memory length: 5992   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59375\n",
            "episode: 32   score: 1.0   memory length: 6142   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5757575757575757\n",
            "episode: 33   score: 0.0   memory length: 6264   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5294117647058822\n",
            "episode: 34   score: 2.0   memory length: 6482   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.542857142857143\n",
            "episode: 35   score: 1.0   memory length: 6651   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5277777777777777\n",
            "episode: 36   score: 3.0   memory length: 6896   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5675675675675675\n",
            "episode: 37   score: 1.0   memory length: 7064   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5526315789473684\n",
            "episode: 38   score: 1.0   memory length: 7232   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
            "episode: 39   score: 2.0   memory length: 7432   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 40   score: 4.0   memory length: 7743   epsilon: 1.0    steps: 311    lr: 0.0001     evaluation reward: 1.6097560975609757\n",
            "episode: 41   score: 3.0   memory length: 7971   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
            "episode: 42   score: 2.0   memory length: 8172   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6511627906976745\n",
            "episode: 43   score: 4.0   memory length: 8465   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.7045454545454546\n",
            "episode: 44   score: 1.0   memory length: 8616   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6888888888888889\n",
            "episode: 45   score: 3.0   memory length: 8842   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.7173913043478262\n",
            "episode: 46   score: 2.0   memory length: 9023   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.7234042553191489\n",
            "episode: 47   score: 0.0   memory length: 9146   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6875\n",
            "episode: 48   score: 2.0   memory length: 9364   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6938775510204083\n",
            "episode: 49   score: 0.0   memory length: 9486   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 50   score: 0.0   memory length: 9609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6274509803921569\n",
            "episode: 51   score: 1.0   memory length: 9760   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
            "episode: 52   score: 0.0   memory length: 9883   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5849056603773586\n",
            "episode: 53   score: 1.0   memory length: 10034   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5740740740740742\n",
            "episode: 54   score: 0.0   memory length: 10157   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
            "episode: 55   score: 1.0   memory length: 10308   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5357142857142858\n",
            "episode: 56   score: 3.0   memory length: 10554   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5614035087719298\n",
            "episode: 57   score: 2.0   memory length: 10753   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5689655172413792\n",
            "episode: 58   score: 2.0   memory length: 10951   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.576271186440678\n",
            "episode: 59   score: 1.0   memory length: 11121   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5666666666666667\n",
            "episode: 60   score: 1.0   memory length: 11290   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5573770491803278\n",
            "episode: 61   score: 0.0   memory length: 11413   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.532258064516129\n",
            "episode: 62   score: 0.0   memory length: 11536   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.507936507936508\n",
            "episode: 63   score: 2.0   memory length: 11734   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.515625\n",
            "episode: 64   score: 0.0   memory length: 11857   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4923076923076923\n",
            "episode: 65   score: 2.0   memory length: 12060   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 66   score: 1.0   memory length: 12211   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.492537313432836\n",
            "episode: 67   score: 0.0   memory length: 12334   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4705882352941178\n",
            "episode: 68   score: 0.0   memory length: 12456   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4492753623188406\n",
            "episode: 69   score: 4.0   memory length: 12712   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.4857142857142858\n",
            "episode: 70   score: 3.0   memory length: 12958   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5070422535211268\n",
            "episode: 71   score: 0.0   memory length: 13081   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4861111111111112\n",
            "episode: 72   score: 0.0   memory length: 13204   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4657534246575343\n",
            "episode: 73   score: 2.0   memory length: 13422   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.472972972972973\n",
            "episode: 74   score: 3.0   memory length: 13686   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.4933333333333334\n",
            "episode: 75   score: 1.0   memory length: 13855   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.486842105263158\n",
            "episode: 76   score: 2.0   memory length: 14071   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4935064935064934\n",
            "episode: 77   score: 4.0   memory length: 14349   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.5256410256410255\n",
            "episode: 78   score: 0.0   memory length: 14471   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5063291139240507\n",
            "episode: 79   score: 2.0   memory length: 14671   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5125\n",
            "episode: 80   score: 1.0   memory length: 14822   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5061728395061729\n",
            "episode: 81   score: 2.0   memory length: 15019   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5121951219512195\n",
            "episode: 82   score: 1.0   memory length: 15170   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5060240963855422\n",
            "episode: 83   score: 1.0   memory length: 15321   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 84   score: 0.0   memory length: 15443   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4823529411764707\n",
            "episode: 85   score: 2.0   memory length: 15641   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4883720930232558\n",
            "episode: 86   score: 2.0   memory length: 15842   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.4942528735632183\n",
            "episode: 87   score: 2.0   memory length: 16040   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 88   score: 0.0   memory length: 16162   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4831460674157304\n",
            "episode: 89   score: 0.0   memory length: 16285   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
            "episode: 90   score: 6.0   memory length: 16630   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.5164835164835164\n",
            "episode: 91   score: 3.0   memory length: 16856   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5326086956521738\n",
            "episode: 92   score: 2.0   memory length: 17074   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5376344086021505\n",
            "episode: 93   score: 1.0   memory length: 17244   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5319148936170213\n",
            "episode: 94   score: 2.0   memory length: 17462   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5368421052631578\n",
            "episode: 95   score: 0.0   memory length: 17584   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5208333333333333\n",
            "episode: 96   score: 2.0   memory length: 17784   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5257731958762886\n",
            "episode: 97   score: 0.0   memory length: 17907   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.510204081632653\n",
            "episode: 98   score: 2.0   memory length: 18124   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5151515151515151\n",
            "episode: 99   score: 0.0   memory length: 18247   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 100   score: 1.0   memory length: 18416   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 101   score: 2.0   memory length: 18631   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 102   score: 0.0   memory length: 18753   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 103   score: 1.0   memory length: 18923   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 104   score: 1.0   memory length: 19095   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 105   score: 1.0   memory length: 19245   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 106   score: 2.0   memory length: 19443   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 107   score: 0.0   memory length: 19566   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 108   score: 1.0   memory length: 19717   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 109   score: 0.0   memory length: 19839   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 110   score: 2.0   memory length: 20039   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 111   score: 0.0   memory length: 20161   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 112   score: 3.0   memory length: 20426   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 113   score: 2.0   memory length: 20623   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 114   score: 3.0   memory length: 20872   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 115   score: 0.0   memory length: 20995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 116   score: 7.0   memory length: 21411   epsilon: 1.0    steps: 416    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 117   score: 3.0   memory length: 21674   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 118   score: 1.0   memory length: 21846   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 119   score: 0.0   memory length: 21969   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 120   score: 1.0   memory length: 22137   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 121   score: 3.0   memory length: 22346   epsilon: 1.0    steps: 209    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 122   score: 0.0   memory length: 22469   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 123   score: 1.0   memory length: 22637   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 124   score: 3.0   memory length: 22884   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 125   score: 0.0   memory length: 23007   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 126   score: 2.0   memory length: 23225   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 127   score: 2.0   memory length: 23406   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 128   score: 0.0   memory length: 23529   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 129   score: 1.0   memory length: 23697   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 130   score: 4.0   memory length: 23976   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 131   score: 2.0   memory length: 24177   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 132   score: 3.0   memory length: 24445   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 133   score: 1.0   memory length: 24614   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 134   score: 0.0   memory length: 24736   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 135   score: 3.0   memory length: 24983   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 136   score: 2.0   memory length: 25199   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 137   score: 2.0   memory length: 25397   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 138   score: 3.0   memory length: 25644   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 139   score: 1.0   memory length: 25795   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 140   score: 1.0   memory length: 25964   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 141   score: 4.0   memory length: 26261   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 142   score: 1.0   memory length: 26430   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 143   score: 0.0   memory length: 26553   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 144   score: 5.0   memory length: 26862   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 145   score: 3.0   memory length: 27113   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 146   score: 2.0   memory length: 27311   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 147   score: 1.0   memory length: 27482   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 148   score: 2.0   memory length: 27680   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 149   score: 0.0   memory length: 27803   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 150   score: 4.0   memory length: 28078   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 151   score: 2.0   memory length: 28293   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 152   score: 1.0   memory length: 28444   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 153   score: 1.0   memory length: 28613   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 154   score: 2.0   memory length: 28828   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 155   score: 0.0   memory length: 28951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 156   score: 2.0   memory length: 29168   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 157   score: 3.0   memory length: 29394   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 158   score: 1.0   memory length: 29563   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 159   score: 1.0   memory length: 29713   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 160   score: 1.0   memory length: 29881   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 161   score: 3.0   memory length: 30128   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 162   score: 0.0   memory length: 30251   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 163   score: 2.0   memory length: 30431   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 164   score: 1.0   memory length: 30600   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 165   score: 3.0   memory length: 30848   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 166   score: 3.0   memory length: 31077   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 167   score: 3.0   memory length: 31344   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 168   score: 2.0   memory length: 31561   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 169   score: 1.0   memory length: 31713   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 170   score: 0.0   memory length: 31836   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 171   score: 0.0   memory length: 31959   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 172   score: 1.0   memory length: 32109   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 173   score: 2.0   memory length: 32289   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 174   score: 2.0   memory length: 32490   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 175   score: 5.0   memory length: 32814   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 176   score: 1.0   memory length: 32983   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 177   score: 2.0   memory length: 33201   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 178   score: 2.0   memory length: 33398   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 179   score: 0.0   memory length: 33520   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 180   score: 1.0   memory length: 33689   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 181   score: 0.0   memory length: 33811   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 182   score: 2.0   memory length: 34030   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 183   score: 3.0   memory length: 34255   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 184   score: 0.0   memory length: 34377   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 185   score: 1.0   memory length: 34545   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 186   score: 2.0   memory length: 34762   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 187   score: 1.0   memory length: 34932   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 188   score: 1.0   memory length: 35083   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 189   score: 3.0   memory length: 35351   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 190   score: 1.0   memory length: 35519   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 191   score: 5.0   memory length: 35844   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 192   score: 2.0   memory length: 36061   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 193   score: 0.0   memory length: 36184   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 194   score: 0.0   memory length: 36306   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 195   score: 3.0   memory length: 36573   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 196   score: 1.0   memory length: 36726   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 197   score: 1.0   memory length: 36895   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 198   score: 0.0   memory length: 37018   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 199   score: 1.0   memory length: 37186   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 200   score: 3.0   memory length: 37412   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 201   score: 1.0   memory length: 37582   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 202   score: 2.0   memory length: 37779   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 203   score: 0.0   memory length: 37901   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 204   score: 0.0   memory length: 38024   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 205   score: 0.0   memory length: 38147   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 206   score: 0.0   memory length: 38270   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 207   score: 0.0   memory length: 38393   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 208   score: 0.0   memory length: 38516   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 209   score: 1.0   memory length: 38684   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 210   score: 0.0   memory length: 38806   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 211   score: 0.0   memory length: 38929   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 212   score: 0.0   memory length: 39052   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 213   score: 0.0   memory length: 39175   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 214   score: 3.0   memory length: 39402   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 215   score: 1.0   memory length: 39553   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 216   score: 0.0   memory length: 39676   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 217   score: 0.0   memory length: 39798   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 218   score: 3.0   memory length: 40029   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 219   score: 0.0   memory length: 40152   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 220   score: 0.0   memory length: 40275   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 221   score: 1.0   memory length: 40444   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 222   score: 2.0   memory length: 40661   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 223   score: 3.0   memory length: 40910   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 224   score: 2.0   memory length: 41107   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 225   score: 3.0   memory length: 41355   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 226   score: 2.0   memory length: 41552   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 227   score: 0.0   memory length: 41675   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 228   score: 1.0   memory length: 41844   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 229   score: 1.0   memory length: 42013   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 230   score: 2.0   memory length: 42214   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 231   score: 0.0   memory length: 42336   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 232   score: 3.0   memory length: 42586   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 233   score: 0.0   memory length: 42708   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 234   score: 1.0   memory length: 42876   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 235   score: 4.0   memory length: 43173   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 236   score: 1.0   memory length: 43324   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 237   score: 4.0   memory length: 43619   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 238   score: 3.0   memory length: 43866   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 239   score: 3.0   memory length: 44114   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 240   score: 3.0   memory length: 44359   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 241   score: 0.0   memory length: 44481   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 242   score: 3.0   memory length: 44745   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 243   score: 0.0   memory length: 44867   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 244   score: 2.0   memory length: 45065   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 245   score: 2.0   memory length: 45282   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 246   score: 2.0   memory length: 45499   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 247   score: 0.0   memory length: 45622   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 248   score: 4.0   memory length: 45896   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 249   score: 1.0   memory length: 46064   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 250   score: 0.0   memory length: 46187   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 251   score: 4.0   memory length: 46462   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 252   score: 5.0   memory length: 46778   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 253   score: 2.0   memory length: 46995   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 254   score: 2.0   memory length: 47193   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 255   score: 0.0   memory length: 47315   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 256   score: 1.0   memory length: 47485   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 257   score: 1.0   memory length: 47653   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 258   score: 2.0   memory length: 47852   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 259   score: 6.0   memory length: 48195   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 260   score: 0.0   memory length: 48317   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 261   score: 3.0   memory length: 48563   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 262   score: 2.0   memory length: 48779   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 263   score: 1.0   memory length: 48948   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 264   score: 0.0   memory length: 49070   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 265   score: 0.0   memory length: 49192   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 266   score: 0.0   memory length: 49314   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 267   score: 0.0   memory length: 49437   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 268   score: 2.0   memory length: 49655   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 269   score: 2.0   memory length: 49852   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 270   score: 0.0   memory length: 49974   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 271   score: 0.0   memory length: 50097   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 272   score: 3.0   memory length: 50344   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 273   score: 0.0   memory length: 50466   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 274   score: 1.0   memory length: 50616   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 275   score: 0.0   memory length: 50738   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 276   score: 1.0   memory length: 50889   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 277   score: 2.0   memory length: 51071   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 278   score: 1.0   memory length: 51239   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 279   score: 4.0   memory length: 51531   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 280   score: 4.0   memory length: 51793   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 281   score: 0.0   memory length: 51915   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 282   score: 3.0   memory length: 52161   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 283   score: 1.0   memory length: 52330   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 284   score: 2.0   memory length: 52553   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 285   score: 1.0   memory length: 52703   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 286   score: 2.0   memory length: 52903   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 287   score: 1.0   memory length: 53075   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 288   score: 1.0   memory length: 53246   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 289   score: 1.0   memory length: 53418   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 290   score: 2.0   memory length: 53616   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 291   score: 2.0   memory length: 53838   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 292   score: 3.0   memory length: 54084   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 293   score: 2.0   memory length: 54284   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 294   score: 2.0   memory length: 54503   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 295   score: 0.0   memory length: 54625   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 296   score: 2.0   memory length: 54842   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 297   score: 1.0   memory length: 54992   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 298   score: 5.0   memory length: 55337   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 299   score: 1.0   memory length: 55506   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 300   score: 0.0   memory length: 55629   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 301   score: 4.0   memory length: 55945   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 302   score: 4.0   memory length: 56263   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 303   score: 3.0   memory length: 56530   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 304   score: 2.0   memory length: 56727   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 305   score: 0.0   memory length: 56850   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 306   score: 0.0   memory length: 56973   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 307   score: 1.0   memory length: 57142   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 308   score: 3.0   memory length: 57388   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 309   score: 0.0   memory length: 57511   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 310   score: 0.0   memory length: 57633   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 311   score: 3.0   memory length: 57878   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 312   score: 0.0   memory length: 58000   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 313   score: 3.0   memory length: 58228   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 314   score: 1.0   memory length: 58379   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 315   score: 0.0   memory length: 58502   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 316   score: 2.0   memory length: 58699   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 317   score: 2.0   memory length: 58917   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 318   score: 3.0   memory length: 59143   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 319   score: 2.0   memory length: 59361   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 320   score: 1.0   memory length: 59533   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 321   score: 1.0   memory length: 59703   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 322   score: 0.0   memory length: 59825   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 323   score: 0.0   memory length: 59948   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 324   score: 2.0   memory length: 60128   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 325   score: 2.0   memory length: 60326   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 326   score: 3.0   memory length: 60555   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 327   score: 2.0   memory length: 60754   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 328   score: 1.0   memory length: 60926   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 329   score: 2.0   memory length: 61144   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 330   score: 2.0   memory length: 61342   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 331   score: 3.0   memory length: 61588   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 332   score: 2.0   memory length: 61786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 333   score: 1.0   memory length: 61957   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 334   score: 1.0   memory length: 62126   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 335   score: 2.0   memory length: 62324   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 336   score: 1.0   memory length: 62495   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 337   score: 1.0   memory length: 62646   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 338   score: 2.0   memory length: 62828   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 339   score: 5.0   memory length: 63162   epsilon: 1.0    steps: 334    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 340   score: 2.0   memory length: 63360   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 341   score: 1.0   memory length: 63511   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 342   score: 0.0   memory length: 63634   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 343   score: 1.0   memory length: 63784   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 344   score: 4.0   memory length: 64081   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 345   score: 1.0   memory length: 64250   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 346   score: 2.0   memory length: 64448   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 347   score: 2.0   memory length: 64646   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 348   score: 1.0   memory length: 64815   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 349   score: 0.0   memory length: 64937   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 350   score: 4.0   memory length: 65211   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 351   score: 1.0   memory length: 65362   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 352   score: 2.0   memory length: 65583   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 353   score: 1.0   memory length: 65734   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 354   score: 1.0   memory length: 65903   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 355   score: 1.0   memory length: 66054   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 356   score: 2.0   memory length: 66273   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 357   score: 1.0   memory length: 66424   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 358   score: 2.0   memory length: 66606   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 359   score: 2.0   memory length: 66804   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 360   score: 2.0   memory length: 67001   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 361   score: 1.0   memory length: 67169   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 362   score: 0.0   memory length: 67292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 363   score: 1.0   memory length: 67461   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 364   score: 0.0   memory length: 67583   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 365   score: 3.0   memory length: 67813   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 366   score: 2.0   memory length: 68030   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 367   score: 1.0   memory length: 68199   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 368   score: 1.0   memory length: 68371   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 369   score: 1.0   memory length: 68522   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 370   score: 2.0   memory length: 68720   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 371   score: 3.0   memory length: 68990   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 372   score: 2.0   memory length: 69209   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 373   score: 1.0   memory length: 69360   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 374   score: 1.0   memory length: 69528   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 375   score: 2.0   memory length: 69749   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 376   score: 1.0   memory length: 69918   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 377   score: 2.0   memory length: 70116   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 378   score: 3.0   memory length: 70383   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 379   score: 1.0   memory length: 70534   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 380   score: 1.0   memory length: 70684   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 381   score: 8.0   memory length: 71139   epsilon: 1.0    steps: 455    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 382   score: 6.0   memory length: 71390   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 383   score: 2.0   memory length: 71587   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 384   score: 3.0   memory length: 71835   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 385   score: 0.0   memory length: 71958   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 386   score: 1.0   memory length: 72130   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 387   score: 10.0   memory length: 72566   epsilon: 1.0    steps: 436    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 388   score: 0.0   memory length: 72688   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 389   score: 2.0   memory length: 72886   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 390   score: 2.0   memory length: 73104   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 391   score: 0.0   memory length: 73227   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 392   score: 1.0   memory length: 73377   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 393   score: 0.0   memory length: 73499   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 394   score: 0.0   memory length: 73621   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 395   score: 3.0   memory length: 73868   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 396   score: 0.0   memory length: 73990   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 397   score: 4.0   memory length: 74268   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 398   score: 1.0   memory length: 74418   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 399   score: 2.0   memory length: 74616   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 400   score: 1.0   memory length: 74785   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 401   score: 0.0   memory length: 74908   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 402   score: 4.0   memory length: 75200   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 403   score: 0.0   memory length: 75323   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 404   score: 3.0   memory length: 75549   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 405   score: 5.0   memory length: 75875   epsilon: 1.0    steps: 326    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 406   score: 4.0   memory length: 76135   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 407   score: 2.0   memory length: 76333   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 408   score: 0.0   memory length: 76456   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 409   score: 1.0   memory length: 76607   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 410   score: 3.0   memory length: 76814   epsilon: 1.0    steps: 207    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 411   score: 3.0   memory length: 77080   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 412   score: 3.0   memory length: 77329   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 413   score: 0.0   memory length: 77452   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 414   score: 2.0   memory length: 77649   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 415   score: 2.0   memory length: 77867   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 416   score: 1.0   memory length: 78036   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 417   score: 0.0   memory length: 78158   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 418   score: 0.0   memory length: 78280   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 419   score: 1.0   memory length: 78448   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 420   score: 3.0   memory length: 78692   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 421   score: 1.0   memory length: 78863   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 422   score: 2.0   memory length: 79061   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 423   score: 3.0   memory length: 79327   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 424   score: 1.0   memory length: 79496   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 425   score: 3.0   memory length: 79724   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 426   score: 0.0   memory length: 79847   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 427   score: 0.0   memory length: 79970   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 428   score: 0.0   memory length: 80092   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 429   score: 1.0   memory length: 80242   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 430   score: 4.0   memory length: 80537   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 431   score: 2.0   memory length: 80735   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 432   score: 2.0   memory length: 80953   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 433   score: 2.0   memory length: 81169   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 434   score: 1.0   memory length: 81320   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 435   score: 1.0   memory length: 81489   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 436   score: 3.0   memory length: 81758   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 437   score: 1.0   memory length: 81908   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 438   score: 1.0   memory length: 82077   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 439   score: 2.0   memory length: 82278   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 440   score: 0.0   memory length: 82401   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 441   score: 3.0   memory length: 82628   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 442   score: 0.0   memory length: 82750   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 443   score: 3.0   memory length: 82998   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 444   score: 1.0   memory length: 83169   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 445   score: 0.0   memory length: 83292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 446   score: 1.0   memory length: 83460   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 447   score: 2.0   memory length: 83675   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 448   score: 1.0   memory length: 83844   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 449   score: 3.0   memory length: 84090   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 450   score: 0.0   memory length: 84212   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 451   score: 2.0   memory length: 84412   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 452   score: 4.0   memory length: 84708   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 453   score: 1.0   memory length: 84877   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 454   score: 0.0   memory length: 85000   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 455   score: 3.0   memory length: 85243   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 456   score: 1.0   memory length: 85394   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 457   score: 2.0   memory length: 85591   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 458   score: 3.0   memory length: 85837   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 459   score: 0.0   memory length: 85959   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 460   score: 2.0   memory length: 86139   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 461   score: 1.0   memory length: 86308   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 462   score: 3.0   memory length: 86573   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 463   score: 0.0   memory length: 86696   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 464   score: 3.0   memory length: 86962   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 465   score: 0.0   memory length: 87085   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 466   score: 1.0   memory length: 87236   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 467   score: 6.0   memory length: 87613   epsilon: 1.0    steps: 377    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 468   score: 3.0   memory length: 87877   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 469   score: 0.0   memory length: 87999   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 470   score: 1.0   memory length: 88149   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 471   score: 0.0   memory length: 88272   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 472   score: 0.0   memory length: 88394   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 473   score: 1.0   memory length: 88545   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 474   score: 1.0   memory length: 88695   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 475   score: 0.0   memory length: 88818   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 476   score: 1.0   memory length: 88987   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 477   score: 3.0   memory length: 89254   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 478   score: 1.0   memory length: 89405   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 479   score: 3.0   memory length: 89667   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 480   score: 0.0   memory length: 89790   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 481   score: 2.0   memory length: 90006   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 482   score: 2.0   memory length: 90221   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 483   score: 2.0   memory length: 90438   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 484   score: 1.0   memory length: 90588   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 485   score: 0.0   memory length: 90711   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 486   score: 2.0   memory length: 90909   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 487   score: 1.0   memory length: 91078   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 488   score: 1.0   memory length: 91248   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 489   score: 3.0   memory length: 91494   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 490   score: 1.0   memory length: 91645   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 491   score: 0.0   memory length: 91767   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 492   score: 0.0   memory length: 91890   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 493   score: 0.0   memory length: 92012   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 494   score: 1.0   memory length: 92164   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 495   score: 0.0   memory length: 92286   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 496   score: 2.0   memory length: 92504   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 497   score: 0.0   memory length: 92627   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 498   score: 1.0   memory length: 92778   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 499   score: 4.0   memory length: 93073   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 500   score: 2.0   memory length: 93288   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 501   score: 2.0   memory length: 93506   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 502   score: 2.0   memory length: 93704   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 503   score: 0.0   memory length: 93827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 504   score: 5.0   memory length: 94150   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 505   score: 1.0   memory length: 94300   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 506   score: 0.0   memory length: 94422   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 507   score: 0.0   memory length: 94544   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 508   score: 0.0   memory length: 94667   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 509   score: 2.0   memory length: 94887   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 510   score: 0.0   memory length: 95009   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 511   score: 3.0   memory length: 95275   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 512   score: 2.0   memory length: 95472   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 513   score: 2.0   memory length: 95670   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 514   score: 3.0   memory length: 95897   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 515   score: 0.0   memory length: 96020   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 516   score: 2.0   memory length: 96218   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 517   score: 0.0   memory length: 96341   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 518   score: 4.0   memory length: 96636   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 519   score: 0.0   memory length: 96759   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 520   score: 0.0   memory length: 96882   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 521   score: 3.0   memory length: 97129   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 522   score: 3.0   memory length: 97375   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 523   score: 0.0   memory length: 97498   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 524   score: 0.0   memory length: 97621   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 525   score: 1.0   memory length: 97793   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 526   score: 1.0   memory length: 97944   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 527   score: 2.0   memory length: 98141   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 528   score: 1.0   memory length: 98292   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 529   score: 1.0   memory length: 98461   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 530   score: 2.0   memory length: 98658   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 531   score: 2.0   memory length: 98839   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 532   score: 1.0   memory length: 99008   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 533   score: 2.0   memory length: 99226   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 534   score: 4.0   memory length: 99520   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 535   score: 0.0   memory length: 99643   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 536   score: 3.0   memory length: 99893   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/assignment5_materials/agent_double.py:99: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "  next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 537   score: 3.0   memory length: 100141   epsilon: 0.9997188400000061    steps: 248    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 538   score: 0.0   memory length: 100264   epsilon: 0.9994753000000114    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 539   score: 3.0   memory length: 100510   epsilon: 0.998988220000022    steps: 246    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 540   score: 0.0   memory length: 100633   epsilon: 0.9987446800000273    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 541   score: 1.0   memory length: 100802   epsilon: 0.9984100600000345    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 542   score: 1.0   memory length: 100972   epsilon: 0.9980734600000418    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 543   score: 4.0   memory length: 101290   epsilon: 0.9974438200000555    steps: 318    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 544   score: 0.0   memory length: 101413   epsilon: 0.9972002800000608    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 545   score: 2.0   memory length: 101611   epsilon: 0.9968082400000693    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 546   score: 2.0   memory length: 101808   epsilon: 0.9964181800000778    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 547   score: 1.0   memory length: 101977   epsilon: 0.996083560000085    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 548   score: 0.0   memory length: 102100   epsilon: 0.9958400200000903    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 549   score: 0.0   memory length: 102223   epsilon: 0.9955964800000956    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 550   score: 1.0   memory length: 102393   epsilon: 0.9952598800001029    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 551   score: 0.0   memory length: 102516   epsilon: 0.9950163400001082    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 552   score: 1.0   memory length: 102684   epsilon: 0.9946837000001154    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 553   score: 0.0   memory length: 102807   epsilon: 0.9944401600001207    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 554   score: 3.0   memory length: 103057   epsilon: 0.9939451600001314    steps: 250    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 555   score: 0.0   memory length: 103179   epsilon: 0.9937036000001367    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 556   score: 0.0   memory length: 103302   epsilon: 0.993460060000142    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 557   score: 8.0   memory length: 103640   epsilon: 0.9927908200001565    steps: 338    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 558   score: 0.0   memory length: 103763   epsilon: 0.9925472800001618    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 559   score: 2.0   memory length: 103964   epsilon: 0.9921493000001704    steps: 201    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 560   score: 2.0   memory length: 104145   epsilon: 0.9917909200001782    steps: 181    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 561   score: 3.0   memory length: 104392   epsilon: 0.9913018600001888    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 562   score: 0.0   memory length: 104515   epsilon: 0.9910583200001941    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 563   score: 0.0   memory length: 104638   epsilon: 0.9908147800001994    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 564   score: 4.0   memory length: 104936   epsilon: 0.9902247400002122    steps: 298    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 565   score: 0.0   memory length: 105059   epsilon: 0.9899812000002175    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 566   score: 0.0   memory length: 105182   epsilon: 0.9897376600002228    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 567   score: 1.0   memory length: 105333   epsilon: 0.9894386800002293    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 568   score: 1.0   memory length: 105484   epsilon: 0.9891397000002358    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 569   score: 1.0   memory length: 105635   epsilon: 0.9888407200002423    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 570   score: 0.0   memory length: 105757   epsilon: 0.9885991600002475    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 571   score: 2.0   memory length: 105958   epsilon: 0.9882011800002561    steps: 201    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 572   score: 0.0   memory length: 106081   epsilon: 0.9879576400002614    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 573   score: 2.0   memory length: 106261   epsilon: 0.9876012400002692    steps: 180    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 574   score: 0.0   memory length: 106383   epsilon: 0.9873596800002744    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 575   score: 4.0   memory length: 106665   epsilon: 0.9868013200002865    steps: 282    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 576   score: 3.0   memory length: 106891   epsilon: 0.9863538400002962    steps: 226    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 577   score: 1.0   memory length: 107062   epsilon: 0.9860152600003036    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 578   score: 3.0   memory length: 107290   epsilon: 0.9855638200003134    steps: 228    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 579   score: 1.0   memory length: 107458   epsilon: 0.9852311800003206    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 580   score: 1.0   memory length: 107627   epsilon: 0.9848965600003279    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 581   score: 1.0   memory length: 107797   epsilon: 0.9845599600003352    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 582   score: 3.0   memory length: 108043   epsilon: 0.9840728800003458    steps: 246    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 583   score: 2.0   memory length: 108260   epsilon: 0.9836432200003551    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 584   score: 0.0   memory length: 108383   epsilon: 0.9833996800003604    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 585   score: 0.0   memory length: 108506   epsilon: 0.9831561400003657    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 586   score: 1.0   memory length: 108677   epsilon: 0.982817560000373    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 587   score: 1.0   memory length: 108847   epsilon: 0.9824809600003803    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 588   score: 0.0   memory length: 108970   epsilon: 0.9822374200003856    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 589   score: 2.0   memory length: 109187   epsilon: 0.9818077600003949    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 590   score: 5.0   memory length: 109511   epsilon: 0.9811662400004089    steps: 324    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 591   score: 0.0   memory length: 109634   epsilon: 0.9809227000004141    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 592   score: 0.0   memory length: 109757   epsilon: 0.9806791600004194    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 593   score: 2.0   memory length: 109954   epsilon: 0.9802891000004279    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 594   score: 3.0   memory length: 110205   epsilon: 0.9797921200004387    steps: 251    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 595   score: 3.0   memory length: 110457   epsilon: 0.9792931600004495    steps: 252    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 596   score: 5.0   memory length: 110802   epsilon: 0.9786100600004644    steps: 345    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 597   score: 0.0   memory length: 110925   epsilon: 0.9783665200004696    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 598   score: 1.0   memory length: 111076   epsilon: 0.9780675400004761    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 599   score: 0.0   memory length: 111199   epsilon: 0.9778240000004814    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 600   score: 2.0   memory length: 111397   epsilon: 0.9774319600004899    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 601   score: 1.0   memory length: 111567   epsilon: 0.9770953600004972    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 602   score: 2.0   memory length: 111767   epsilon: 0.9766993600005058    steps: 200    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 603   score: 0.0   memory length: 111890   epsilon: 0.9764558200005111    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 604   score: 1.0   memory length: 112058   epsilon: 0.9761231800005183    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 605   score: 1.0   memory length: 112226   epsilon: 0.9757905400005256    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 606   score: 2.0   memory length: 112424   epsilon: 0.9753985000005341    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 607   score: 3.0   memory length: 112670   epsilon: 0.9749114200005446    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 608   score: 1.0   memory length: 112821   epsilon: 0.9746124400005511    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 609   score: 3.0   memory length: 113067   epsilon: 0.9741253600005617    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 610   score: 1.0   memory length: 113218   epsilon: 0.9738263800005682    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 611   score: 0.0   memory length: 113341   epsilon: 0.9735828400005735    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 612   score: 1.0   memory length: 113509   epsilon: 0.9732502000005807    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 613   score: 4.0   memory length: 113824   epsilon: 0.9726265000005943    steps: 315    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 614   score: 1.0   memory length: 113996   epsilon: 0.9722859400006016    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 615   score: 1.0   memory length: 114147   epsilon: 0.9719869600006081    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 616   score: 2.0   memory length: 114363   epsilon: 0.9715592800006174    steps: 216    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 617   score: 2.0   memory length: 114561   epsilon: 0.9711672400006259    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 618   score: 2.0   memory length: 114779   epsilon: 0.9707356000006353    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 619   score: 1.0   memory length: 114931   epsilon: 0.9704346400006418    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 620   score: 0.0   memory length: 115054   epsilon: 0.9701911000006471    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 621   score: 0.0   memory length: 115177   epsilon: 0.9699475600006524    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 622   score: 2.0   memory length: 115374   epsilon: 0.9695575000006609    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 623   score: 1.0   memory length: 115526   epsilon: 0.9692565400006674    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 624   score: 0.0   memory length: 115649   epsilon: 0.9690130000006727    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 625   score: 2.0   memory length: 115865   epsilon: 0.968585320000682    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 626   score: 0.0   memory length: 115987   epsilon: 0.9683437600006872    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 627   score: 1.0   memory length: 116137   epsilon: 0.9680467600006937    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 628   score: 0.0   memory length: 116260   epsilon: 0.967803220000699    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 629   score: 0.0   memory length: 116383   epsilon: 0.9675596800007042    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 630   score: 1.0   memory length: 116553   epsilon: 0.9672230800007116    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 631   score: 1.0   memory length: 116722   epsilon: 0.9668884600007188    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 632   score: 1.0   memory length: 116872   epsilon: 0.9665914600007253    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 633   score: 2.0   memory length: 117069   epsilon: 0.9662014000007337    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 634   score: 1.0   memory length: 117238   epsilon: 0.965866780000741    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 635   score: 4.0   memory length: 117533   epsilon: 0.9652826800007537    steps: 295    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 636   score: 0.0   memory length: 117656   epsilon: 0.965039140000759    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 637   score: 3.0   memory length: 117883   epsilon: 0.9645896800007687    steps: 227    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 638   score: 1.0   memory length: 118034   epsilon: 0.9642907000007752    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 639   score: 0.0   memory length: 118157   epsilon: 0.9640471600007805    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 640   score: 4.0   memory length: 118455   epsilon: 0.9634571200007933    steps: 298    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 641   score: 1.0   memory length: 118606   epsilon: 0.9631581400007998    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 642   score: 5.0   memory length: 118909   epsilon: 0.9625582000008128    steps: 303    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 643   score: 2.0   memory length: 119111   epsilon: 0.9621582400008215    steps: 202    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 644   score: 0.0   memory length: 119234   epsilon: 0.9619147000008268    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 645   score: 1.0   memory length: 119402   epsilon: 0.961582060000834    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 646   score: 2.0   memory length: 119599   epsilon: 0.9611920000008425    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 647   score: 2.0   memory length: 119797   epsilon: 0.960799960000851    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 648   score: 0.0   memory length: 119920   epsilon: 0.9605564200008563    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 649   score: 0.0   memory length: 120043   epsilon: 0.9603128800008616    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 650   score: 2.0   memory length: 120241   epsilon: 0.9599208400008701    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 651   score: 2.0   memory length: 120457   epsilon: 0.9594931600008794    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 652   score: 2.0   memory length: 120637   epsilon: 0.9591367600008871    steps: 180    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 653   score: 2.0   memory length: 120835   epsilon: 0.9587447200008956    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 654   score: 0.0   memory length: 120957   epsilon: 0.9585031600009009    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 655   score: 2.0   memory length: 121155   epsilon: 0.9581111200009094    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 656   score: 0.0   memory length: 121277   epsilon: 0.9578695600009146    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 657   score: 1.0   memory length: 121446   epsilon: 0.9575349400009219    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 658   score: 2.0   memory length: 121663   epsilon: 0.9571052800009312    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 659   score: 1.0   memory length: 121814   epsilon: 0.9568063000009377    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 660   score: 2.0   memory length: 122014   epsilon: 0.9564103000009463    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 661   score: 1.0   memory length: 122166   epsilon: 0.9561093400009528    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 662   score: 1.0   memory length: 122317   epsilon: 0.9558103600009593    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 663   score: 2.0   memory length: 122517   epsilon: 0.9554143600009679    steps: 200    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 664   score: 0.0   memory length: 122640   epsilon: 0.9551708200009732    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 665   score: 0.0   memory length: 122762   epsilon: 0.9549292600009784    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 666   score: 0.0   memory length: 122884   epsilon: 0.9546877000009837    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 667   score: 2.0   memory length: 123081   epsilon: 0.9542976400009922    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 668   score: 3.0   memory length: 123308   epsilon: 0.9538481800010019    steps: 227    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 669   score: 2.0   memory length: 123506   epsilon: 0.9534561400010104    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 670   score: 4.0   memory length: 123761   epsilon: 0.9529512400010214    steps: 255    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 671   score: 0.0   memory length: 123884   epsilon: 0.9527077000010267    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 672   score: 3.0   memory length: 124110   epsilon: 0.9522602200010364    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 673   score: 0.0   memory length: 124232   epsilon: 0.9520186600010416    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 674   score: 0.0   memory length: 124354   epsilon: 0.9517771000010469    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 675   score: 0.0   memory length: 124477   epsilon: 0.9515335600010522    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 676   score: 0.0   memory length: 124599   epsilon: 0.9512920000010574    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 677   score: 2.0   memory length: 124796   epsilon: 0.9509019400010659    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 678   score: 0.0   memory length: 124918   epsilon: 0.9506603800010711    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 679   score: 2.0   memory length: 125116   epsilon: 0.9502683400010796    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 680   score: 0.0   memory length: 125239   epsilon: 0.9500248000010849    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 681   score: 2.0   memory length: 125437   epsilon: 0.9496327600010934    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 682   score: 1.0   memory length: 125588   epsilon: 0.9493337800010999    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 683   score: 1.0   memory length: 125758   epsilon: 0.9489971800011072    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 684   score: 5.0   memory length: 126097   epsilon: 0.9483259600011218    steps: 339    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 685   score: 1.0   memory length: 126266   epsilon: 0.9479913400011291    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 686   score: 4.0   memory length: 126542   epsilon: 0.9474448600011409    steps: 276    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 687   score: 2.0   memory length: 126764   epsilon: 0.9470053000011505    steps: 222    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 688   score: 5.0   memory length: 127109   epsilon: 0.9463222000011653    steps: 345    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 689   score: 2.0   memory length: 127307   epsilon: 0.9459301600011738    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 690   score: 0.0   memory length: 127430   epsilon: 0.9456866200011791    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 691   score: 2.0   memory length: 127648   epsilon: 0.9452549800011885    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 692   score: 1.0   memory length: 127798   epsilon: 0.9449579800011949    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 693   score: 0.0   memory length: 127921   epsilon: 0.9447144400012002    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 694   score: 2.0   memory length: 128138   epsilon: 0.9442847800012095    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 695   score: 2.0   memory length: 128335   epsilon: 0.943894720001218    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 696   score: 2.0   memory length: 128533   epsilon: 0.9435026800012265    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 697   score: 2.0   memory length: 128730   epsilon: 0.943112620001235    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 698   score: 0.0   memory length: 128853   epsilon: 0.9428690800012403    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 699   score: 2.0   memory length: 129070   epsilon: 0.9424394200012496    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 700   score: 3.0   memory length: 129337   epsilon: 0.9419107600012611    steps: 267    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 701   score: 0.0   memory length: 129459   epsilon: 0.9416692000012663    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 702   score: 2.0   memory length: 129674   epsilon: 0.9412435000012755    steps: 215    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 703   score: 0.0   memory length: 129796   epsilon: 0.9410019400012808    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 704   score: 4.0   memory length: 130051   epsilon: 0.9404970400012918    steps: 255    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 705   score: 1.0   memory length: 130219   epsilon: 0.940164400001299    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 706   score: 2.0   memory length: 130416   epsilon: 0.9397743400013074    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 707   score: 3.0   memory length: 130681   epsilon: 0.9392496400013188    steps: 265    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 708   score: 1.0   memory length: 130851   epsilon: 0.9389130400013261    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 709   score: 0.0   memory length: 130974   epsilon: 0.9386695000013314    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 710   score: 1.0   memory length: 131145   epsilon: 0.9383309200013388    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 711   score: 0.0   memory length: 131267   epsilon: 0.938089360001344    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 712   score: 1.0   memory length: 131436   epsilon: 0.9377547400013513    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 713   score: 7.0   memory length: 131842   epsilon: 0.9369508600013687    steps: 406    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 714   score: 1.0   memory length: 132011   epsilon: 0.936616240001376    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 715   score: 0.0   memory length: 132133   epsilon: 0.9363746800013812    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 716   score: 0.0   memory length: 132255   epsilon: 0.9361331200013865    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 717   score: 2.0   memory length: 132439   epsilon: 0.9357688000013944    steps: 184    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 718   score: 3.0   memory length: 132691   epsilon: 0.9352698400014052    steps: 252    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 719   score: 1.0   memory length: 132842   epsilon: 0.9349708600014117    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 720   score: 2.0   memory length: 133060   epsilon: 0.9345392200014211    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 721   score: 2.0   memory length: 133245   epsilon: 0.934172920001429    steps: 185    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 722   score: 0.0   memory length: 133368   epsilon: 0.9339293800014343    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 723   score: 2.0   memory length: 133566   epsilon: 0.9335373400014428    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 724   score: 1.0   memory length: 133736   epsilon: 0.9332007400014501    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 725   score: 0.0   memory length: 133859   epsilon: 0.9329572000014554    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 726   score: 0.0   memory length: 133982   epsilon: 0.9327136600014607    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 727   score: 2.0   memory length: 134200   epsilon: 0.9322820200014701    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 728   score: 0.0   memory length: 134323   epsilon: 0.9320384800014754    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 729   score: 2.0   memory length: 134521   epsilon: 0.9316464400014839    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 730   score: 2.0   memory length: 134719   epsilon: 0.9312544000014924    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 731   score: 1.0   memory length: 134889   epsilon: 0.9309178000014997    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 732   score: 2.0   memory length: 135087   epsilon: 0.9305257600015082    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 733   score: 2.0   memory length: 135305   epsilon: 0.9300941200015176    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 734   score: 3.0   memory length: 135533   epsilon: 0.9296426800015274    steps: 228    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 735   score: 3.0   memory length: 135781   epsilon: 0.929151640001538    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 736   score: 5.0   memory length: 136116   epsilon: 0.9284883400015524    steps: 335    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 737   score: 2.0   memory length: 136334   epsilon: 0.9280567000015618    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 738   score: 2.0   memory length: 136531   epsilon: 0.9276666400015703    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 739   score: 1.0   memory length: 136701   epsilon: 0.9273300400015776    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 740   score: 3.0   memory length: 136949   epsilon: 0.9268390000015883    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 741   score: 4.0   memory length: 137226   epsilon: 0.9262905400016002    steps: 277    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 742   score: 0.0   memory length: 137348   epsilon: 0.9260489800016054    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 743   score: 2.0   memory length: 137566   epsilon: 0.9256173400016148    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 744   score: 2.0   memory length: 137783   epsilon: 0.9251876800016241    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 745   score: 4.0   memory length: 138059   epsilon: 0.924641200001636    steps: 276    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 746   score: 0.0   memory length: 138182   epsilon: 0.9243976600016413    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 747   score: 2.0   memory length: 138400   epsilon: 0.9239660200016506    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 748   score: 1.0   memory length: 138570   epsilon: 0.9236294200016579    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 749   score: 2.0   memory length: 138768   epsilon: 0.9232373800016664    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 750   score: 7.0   memory length: 139068   epsilon: 0.9226433800016793    steps: 300    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 751   score: 1.0   memory length: 139219   epsilon: 0.9223444000016858    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 752   score: 2.0   memory length: 139435   epsilon: 0.9219167200016951    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 753   score: 1.0   memory length: 139587   epsilon: 0.9216157600017016    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 754   score: 3.0   memory length: 139816   epsilon: 0.9211623400017115    steps: 229    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 755   score: 2.0   memory length: 140014   epsilon: 0.92077030000172    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 756   score: 1.0   memory length: 140184   epsilon: 0.9204337000017273    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 757   score: 0.0   memory length: 140307   epsilon: 0.9201901600017326    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 758   score: 1.0   memory length: 140478   epsilon: 0.9198515800017399    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 759   score: 4.0   memory length: 140766   epsilon: 0.9192813400017523    steps: 288    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 760   score: 0.0   memory length: 140888   epsilon: 0.9190397800017576    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 761   score: 0.0   memory length: 141011   epsilon: 0.9187962400017629    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 762   score: 1.0   memory length: 141180   epsilon: 0.9184616200017701    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 763   score: 3.0   memory length: 141405   epsilon: 0.9180161200017798    steps: 225    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 764   score: 3.0   memory length: 141632   epsilon: 0.9175666600017895    steps: 227    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 765   score: 0.0   memory length: 141754   epsilon: 0.9173251000017948    steps: 122    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 766   score: 2.0   memory length: 141951   epsilon: 0.9169350400018033    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 767   score: 4.0   memory length: 142242   epsilon: 0.9163588600018158    steps: 291    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 768   score: 0.0   memory length: 142365   epsilon: 0.916115320001821    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 769   score: 5.0   memory length: 142671   epsilon: 0.9155094400018342    steps: 306    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 770   score: 2.0   memory length: 142853   epsilon: 0.915149080001842    steps: 182    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 771   score: 3.0   memory length: 143099   epsilon: 0.9146620000018526    steps: 246    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 772   score: 0.0   memory length: 143222   epsilon: 0.9144184600018579    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 773   score: 3.0   memory length: 143430   epsilon: 0.9140066200018668    steps: 208    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 774   score: 1.0   memory length: 143602   epsilon: 0.9136660600018742    steps: 172    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 775   score: 1.0   memory length: 143772   epsilon: 0.9133294600018815    steps: 170    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 776   score: 5.0   memory length: 144086   epsilon: 0.912707740001895    steps: 314    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 777   score: 3.0   memory length: 144331   epsilon: 0.9122226400019056    steps: 245    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 778   score: 0.0   memory length: 144454   epsilon: 0.9119791000019108    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 779   score: 1.0   memory length: 144624   epsilon: 0.9116425000019182    steps: 170    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 780   score: 0.0   memory length: 144746   epsilon: 0.9114009400019234    steps: 122    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 781   score: 1.0   memory length: 144916   epsilon: 0.9110643400019307    steps: 170    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 782   score: 6.0   memory length: 145293   epsilon: 0.9103178800019469    steps: 377    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 783   score: 1.0   memory length: 145462   epsilon: 0.9099832600019542    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 784   score: 2.0   memory length: 145660   epsilon: 0.9095912200019627    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 785   score: 0.0   memory length: 145783   epsilon: 0.909347680001968    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 786   score: 5.0   memory length: 146103   epsilon: 0.9087140800019817    steps: 320    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 787   score: 1.0   memory length: 146273   epsilon: 0.908377480001989    steps: 170    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 788   score: 0.0   memory length: 146395   epsilon: 0.9081359200019943    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 789   score: 3.0   memory length: 146642   epsilon: 0.9076468600020049    steps: 247    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 790   score: 2.0   memory length: 146840   epsilon: 0.9072548200020134    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 791   score: 3.0   memory length: 147089   epsilon: 0.9067618000020241    steps: 249    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 792   score: 3.0   memory length: 147336   epsilon: 0.9062727400020347    steps: 247    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 793   score: 4.0   memory length: 147597   epsilon: 0.905755960002046    steps: 261    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 794   score: 0.0   memory length: 147720   epsilon: 0.9055124200020512    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 795   score: 2.0   memory length: 147917   epsilon: 0.9051223600020597    steps: 197    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 796   score: 0.0   memory length: 148039   epsilon: 0.9048808000020649    steps: 122    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 797   score: 1.0   memory length: 148209   epsilon: 0.9045442000020723    steps: 170    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 798   score: 1.0   memory length: 148359   epsilon: 0.9042472000020787    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 799   score: 3.0   memory length: 148605   epsilon: 0.9037601200020893    steps: 246    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 800   score: 1.0   memory length: 148774   epsilon: 0.9034255000020965    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 801   score: 1.0   memory length: 148943   epsilon: 0.9030908800021038    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 802   score: 0.0   memory length: 149066   epsilon: 0.9028473400021091    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 803   score: 2.0   memory length: 149285   epsilon: 0.9024137200021185    steps: 219    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 804   score: 5.0   memory length: 149609   epsilon: 0.9017722000021324    steps: 324    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 805   score: 2.0   memory length: 149809   epsilon: 0.901376200002141    steps: 200    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 806   score: 3.0   memory length: 150039   epsilon: 0.9009208000021509    steps: 230    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 807   score: 3.0   memory length: 150265   epsilon: 0.9004733200021606    steps: 226    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 808   score: 1.0   memory length: 150433   epsilon: 0.9001406800021678    steps: 168    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 809   score: 4.0   memory length: 150748   epsilon: 0.8995169800021814    steps: 315    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 810   score: 3.0   memory length: 150998   epsilon: 0.8990219800021921    steps: 250    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 811   score: 2.0   memory length: 151196   epsilon: 0.8986299400022006    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 812   score: 3.0   memory length: 151462   epsilon: 0.8981032600022121    steps: 266    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 813   score: 2.0   memory length: 151660   epsilon: 0.8977112200022206    steps: 198    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 814   score: 1.0   memory length: 151831   epsilon: 0.8973726400022279    steps: 171    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 815   score: 1.0   memory length: 152003   epsilon: 0.8970320800022353    steps: 172    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 816   score: 3.0   memory length: 152271   epsilon: 0.8965014400022469    steps: 268    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 817   score: 2.0   memory length: 152470   epsilon: 0.8961074200022554    steps: 199    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 818   score: 2.0   memory length: 152668   epsilon: 0.8957153800022639    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 819   score: 2.0   memory length: 152866   epsilon: 0.8953233400022724    steps: 198    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 820   score: 2.0   memory length: 153084   epsilon: 0.8948917000022818    steps: 218    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 821   score: 1.0   memory length: 153253   epsilon: 0.8945570800022891    steps: 169    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 822   score: 1.0   memory length: 153422   epsilon: 0.8942224600022963    steps: 169    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 823   score: 3.0   memory length: 153668   epsilon: 0.8937353800023069    steps: 246    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 824   score: 2.0   memory length: 153866   epsilon: 0.8933433400023154    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 825   score: 2.0   memory length: 154064   epsilon: 0.8929513000023239    steps: 198    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 826   score: 1.0   memory length: 154233   epsilon: 0.8926166800023312    steps: 169    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 827   score: 2.0   memory length: 154430   epsilon: 0.8922266200023397    steps: 197    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 828   score: 1.0   memory length: 154583   epsilon: 0.8919236800023462    steps: 153    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 829   score: 3.0   memory length: 154808   epsilon: 0.8914781800023559    steps: 225    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 830   score: 0.0   memory length: 154931   epsilon: 0.8912346400023612    steps: 123    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 831   score: 0.0   memory length: 155053   epsilon: 0.8909930800023664    steps: 122    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 832   score: 2.0   memory length: 155251   epsilon: 0.8906010400023749    steps: 198    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 833   score: 2.0   memory length: 155448   epsilon: 0.8902109800023834    steps: 197    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 834   score: 1.0   memory length: 155618   epsilon: 0.8898743800023907    steps: 170    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 835   score: 1.0   memory length: 155769   epsilon: 0.8895754000023972    steps: 151    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 836   score: 5.0   memory length: 156073   epsilon: 0.8889734800024103    steps: 304    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 837   score: 2.0   memory length: 156271   epsilon: 0.8885814400024188    steps: 198    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 838   score: 2.0   memory length: 156490   epsilon: 0.8881478200024282    steps: 219    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 839   score: 0.0   memory length: 156613   epsilon: 0.8879042800024335    steps: 123    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 840   score: 2.0   memory length: 156831   epsilon: 0.8874726400024429    steps: 218    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 841   score: 2.0   memory length: 157028   epsilon: 0.8870825800024513    steps: 197    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 842   score: 1.0   memory length: 157179   epsilon: 0.8867836000024578    steps: 151    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 843   score: 1.0   memory length: 157348   epsilon: 0.8864489800024651    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 844   score: 3.0   memory length: 157615   epsilon: 0.8859203200024766    steps: 267    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 845   score: 2.0   memory length: 157834   epsilon: 0.885486700002486    steps: 219    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 846   score: 1.0   memory length: 158003   epsilon: 0.8851520800024932    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 847   score: 2.0   memory length: 158185   epsilon: 0.8847917200025011    steps: 182    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 848   score: 1.0   memory length: 158354   epsilon: 0.8844571000025083    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 849   score: 0.0   memory length: 158477   epsilon: 0.8842135600025136    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 850   score: 1.0   memory length: 158627   epsilon: 0.8839165600025201    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 851   score: 0.0   memory length: 158750   epsilon: 0.8836730200025253    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 852   score: 3.0   memory length: 159001   epsilon: 0.8831760400025361    steps: 251    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 853   score: 2.0   memory length: 159217   epsilon: 0.8827483600025454    steps: 216    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 854   score: 1.0   memory length: 159386   epsilon: 0.8824137400025527    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 855   score: 4.0   memory length: 159677   epsilon: 0.8818375600025652    steps: 291    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 856   score: 0.0   memory length: 159800   epsilon: 0.8815940200025705    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 857   score: 4.0   memory length: 160079   epsilon: 0.8810416000025825    steps: 279    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 858   score: 5.0   memory length: 160419   epsilon: 0.8803684000025971    steps: 340    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 859   score: 1.0   memory length: 160570   epsilon: 0.8800694200026036    steps: 151    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 860   score: 1.0   memory length: 160739   epsilon: 0.8797348000026108    steps: 169    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 861   score: 4.0   memory length: 161013   epsilon: 0.8791922800026226    steps: 274    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 862   score: 6.0   memory length: 161398   epsilon: 0.8784299800026392    steps: 385    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 863   score: 4.0   memory length: 161692   epsilon: 0.8778478600026518    steps: 294    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 864   score: 2.0   memory length: 161890   epsilon: 0.8774558200026603    steps: 198    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 865   score: 1.0   memory length: 162059   epsilon: 0.8771212000026676    steps: 169    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 866   score: 0.0   memory length: 162182   epsilon: 0.8768776600026729    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 867   score: 1.0   memory length: 162333   epsilon: 0.8765786800026794    steps: 151    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 868   score: 2.0   memory length: 162531   epsilon: 0.8761866400026879    steps: 198    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 869   score: 0.0   memory length: 162654   epsilon: 0.8759431000026932    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 870   score: 0.0   memory length: 162777   epsilon: 0.8756995600026984    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 871   score: 2.0   memory length: 162963   epsilon: 0.8753312800027064    steps: 186    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 872   score: 5.0   memory length: 163308   epsilon: 0.8746481800027213    steps: 345    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 873   score: 2.0   memory length: 163506   epsilon: 0.8742561400027298    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 874   score: 1.0   memory length: 163676   epsilon: 0.8739195400027371    steps: 170    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 875   score: 5.0   memory length: 164001   epsilon: 0.873276040002751    steps: 325    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 876   score: 0.0   memory length: 164124   epsilon: 0.8730325000027563    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 877   score: 2.0   memory length: 164342   epsilon: 0.8726008600027657    steps: 218    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 878   score: 1.0   memory length: 164514   epsilon: 0.8722603000027731    steps: 172    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 879   score: 2.0   memory length: 164734   epsilon: 0.8718247000027826    steps: 220    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 880   score: 1.0   memory length: 164884   epsilon: 0.871527700002789    steps: 150    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 881   score: 2.0   memory length: 165082   epsilon: 0.8711356600027975    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 882   score: 4.0   memory length: 165397   epsilon: 0.8705119600028111    steps: 315    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 883   score: 3.0   memory length: 165623   epsilon: 0.8700644800028208    steps: 226    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 884   score: 4.0   memory length: 165918   epsilon: 0.8694803800028335    steps: 295    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 885   score: 2.0   memory length: 166116   epsilon: 0.869088340002842    steps: 198    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 886   score: 3.0   memory length: 166361   epsilon: 0.8686032400028525    steps: 245    lr: 0.0001     evaluation reward: 1.95\n",
            "episode: 887   score: 2.0   memory length: 166559   epsilon: 0.868211200002861    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 888   score: 3.0   memory length: 166803   epsilon: 0.8677280800028715    steps: 244    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 889   score: 4.0   memory length: 167063   epsilon: 0.8672132800028827    steps: 260    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 890   score: 3.0   memory length: 167329   epsilon: 0.8666866000028941    steps: 266    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 891   score: 3.0   memory length: 167577   epsilon: 0.8661955600029048    steps: 248    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 892   score: 5.0   memory length: 167893   epsilon: 0.8655698800029183    steps: 316    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 893   score: 1.0   memory length: 168063   epsilon: 0.8652332800029257    steps: 170    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 894   score: 0.0   memory length: 168186   epsilon: 0.8649897400029309    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 895   score: 4.0   memory length: 168443   epsilon: 0.864480880002942    steps: 257    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 896   score: 3.0   memory length: 168655   epsilon: 0.8640611200029511    steps: 212    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 897   score: 6.0   memory length: 169024   epsilon: 0.863330500002967    steps: 369    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 898   score: 5.0   memory length: 169347   epsilon: 0.8626909600029808    steps: 323    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 899   score: 0.0   memory length: 169470   epsilon: 0.8624474200029861    steps: 123    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 900   score: 3.0   memory length: 169699   epsilon: 0.861994000002996    steps: 229    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 901   score: 4.0   memory length: 169943   epsilon: 0.8615108800030065    steps: 244    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 902   score: 2.0   memory length: 170140   epsilon: 0.8611208200030149    steps: 197    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 903   score: 2.0   memory length: 170340   epsilon: 0.8607248200030235    steps: 200    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 904   score: 2.0   memory length: 170538   epsilon: 0.860332780003032    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 905   score: 1.0   memory length: 170707   epsilon: 0.8599981600030393    steps: 169    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 906   score: 3.0   memory length: 170933   epsilon: 0.859550680003049    steps: 226    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 907   score: 4.0   memory length: 171194   epsilon: 0.8590339000030602    steps: 261    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 908   score: 4.0   memory length: 171471   epsilon: 0.8584854400030721    steps: 277    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 909   score: 5.0   memory length: 171796   epsilon: 0.8578419400030861    steps: 325    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 910   score: 3.0   memory length: 172064   epsilon: 0.8573113000030976    steps: 268    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 911   score: 2.0   memory length: 172262   epsilon: 0.8569192600031061    steps: 198    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 912   score: 5.0   memory length: 172629   epsilon: 0.8561926000031219    steps: 367    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 913   score: 1.0   memory length: 172801   epsilon: 0.8558520400031293    steps: 172    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 914   score: 2.0   memory length: 173022   epsilon: 0.8554144600031388    steps: 221    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 915   score: 1.0   memory length: 173173   epsilon: 0.8551154800031453    steps: 151    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 916   score: 1.0   memory length: 173324   epsilon: 0.8548165000031518    steps: 151    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 917   score: 3.0   memory length: 173536   epsilon: 0.8543967400031609    steps: 212    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 918   score: 0.0   memory length: 173659   epsilon: 0.8541532000031662    steps: 123    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 919   score: 3.0   memory length: 173885   epsilon: 0.8537057200031759    steps: 226    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 920   score: 6.0   memory length: 174258   epsilon: 0.8529671800031919    steps: 373    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 921   score: 3.0   memory length: 174525   epsilon: 0.8524385200032034    steps: 267    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 922   score: 1.0   memory length: 174676   epsilon: 0.8521395400032099    steps: 151    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 923   score: 1.0   memory length: 174845   epsilon: 0.8518049200032172    steps: 169    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 924   score: 7.0   memory length: 175222   epsilon: 0.8510584600032334    steps: 377    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 925   score: 1.0   memory length: 175392   epsilon: 0.8507218600032407    steps: 170    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 926   score: 0.0   memory length: 175515   epsilon: 0.850478320003246    steps: 123    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 927   score: 2.0   memory length: 175733   epsilon: 0.8500466800032553    steps: 218    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 928   score: 1.0   memory length: 175884   epsilon: 0.8497477000032618    steps: 151    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 929   score: 4.0   memory length: 176141   epsilon: 0.8492388400032729    steps: 257    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 930   score: 2.0   memory length: 176339   epsilon: 0.8488468000032814    steps: 198    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 931   score: 4.0   memory length: 176614   epsilon: 0.8483023000032932    steps: 275    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 932   score: 0.0   memory length: 176736   epsilon: 0.8480607400032985    steps: 122    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 933   score: 5.0   memory length: 177081   epsilon: 0.8473776400033133    steps: 345    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 934   score: 5.0   memory length: 177425   epsilon: 0.8466965200033281    steps: 344    lr: 0.0001     evaluation reward: 2.38\n",
            "episode: 935   score: 4.0   memory length: 177721   epsilon: 0.8461104400033408    steps: 296    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 936   score: 1.0   memory length: 177871   epsilon: 0.8458134400033472    steps: 150    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 937   score: 4.0   memory length: 178171   epsilon: 0.8452194400033601    steps: 300    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 938   score: 7.0   memory length: 178563   epsilon: 0.844443280003377    steps: 392    lr: 0.0001     evaluation reward: 2.44\n",
            "episode: 939   score: 6.0   memory length: 178931   epsilon: 0.8437146400033928    steps: 368    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 940   score: 3.0   memory length: 179162   epsilon: 0.8432572600034027    steps: 231    lr: 0.0001     evaluation reward: 2.51\n",
            "episode: 941   score: 6.0   memory length: 179538   epsilon: 0.8425127800034189    steps: 376    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 942   score: 2.0   memory length: 179754   epsilon: 0.8420851000034282    steps: 216    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 943   score: 1.0   memory length: 179904   epsilon: 0.8417881000034346    steps: 150    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 944   score: 0.0   memory length: 180027   epsilon: 0.8415445600034399    steps: 123    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 945   score: 2.0   memory length: 180225   epsilon: 0.8411525200034484    steps: 198    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 946   score: 4.0   memory length: 180500   epsilon: 0.8406080200034602    steps: 275    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 947   score: 2.0   memory length: 180717   epsilon: 0.8401783600034696    steps: 217    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 948   score: 3.0   memory length: 180963   epsilon: 0.8396912800034801    steps: 246    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 949   score: 1.0   memory length: 181113   epsilon: 0.8393942800034866    steps: 150    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 950   score: 2.0   memory length: 181311   epsilon: 0.8390022400034951    steps: 198    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 951   score: 5.0   memory length: 181659   epsilon: 0.8383132000035101    steps: 348    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 952   score: 7.0   memory length: 182080   epsilon: 0.8374796200035282    steps: 421    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 953   score: 3.0   memory length: 182326   epsilon: 0.8369925400035387    steps: 246    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 954   score: 2.0   memory length: 182548   epsilon: 0.8365529800035483    steps: 222    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 955   score: 1.0   memory length: 182716   epsilon: 0.8362203400035555    steps: 168    lr: 0.0001     evaluation reward: 2.68\n",
            "episode: 956   score: 3.0   memory length: 182941   epsilon: 0.8357748400035652    steps: 225    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 957   score: 2.0   memory length: 183156   epsilon: 0.8353491400035744    steps: 215    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 958   score: 0.0   memory length: 183279   epsilon: 0.8351056000035797    steps: 123    lr: 0.0001     evaluation reward: 2.64\n",
            "episode: 959   score: 2.0   memory length: 183476   epsilon: 0.8347155400035882    steps: 197    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 960   score: 3.0   memory length: 183702   epsilon: 0.8342680600035979    steps: 226    lr: 0.0001     evaluation reward: 2.67\n",
            "episode: 961   score: 3.0   memory length: 183971   epsilon: 0.8337354400036094    steps: 269    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 962   score: 1.0   memory length: 184141   epsilon: 0.8333988400036167    steps: 170    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 963   score: 2.0   memory length: 184338   epsilon: 0.8330087800036252    steps: 197    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 964   score: 5.0   memory length: 184655   epsilon: 0.8323811200036388    steps: 317    lr: 0.0001     evaluation reward: 2.62\n",
            "episode: 965   score: 3.0   memory length: 184902   epsilon: 0.8318920600036495    steps: 247    lr: 0.0001     evaluation reward: 2.64\n",
            "episode: 966   score: 1.0   memory length: 185074   epsilon: 0.8315515000036569    steps: 172    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 967   score: 2.0   memory length: 185257   epsilon: 0.8311891600036647    steps: 183    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 968   score: 3.0   memory length: 185504   epsilon: 0.8307001000036753    steps: 247    lr: 0.0001     evaluation reward: 2.67\n",
            "episode: 969   score: 4.0   memory length: 185800   epsilon: 0.8301140200036881    steps: 296    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 970   score: 2.0   memory length: 185998   epsilon: 0.8297219800036966    steps: 198    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 971   score: 1.0   memory length: 186151   epsilon: 0.8294190400037031    steps: 153    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 972   score: 3.0   memory length: 186421   epsilon: 0.8288844400037148    steps: 270    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 973   score: 5.0   memory length: 186747   epsilon: 0.8282389600037288    steps: 326    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 974   score: 4.0   memory length: 187024   epsilon: 0.8276905000037407    steps: 277    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 975   score: 3.0   memory length: 187250   epsilon: 0.8272430200037504    steps: 226    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 976   score: 5.0   memory length: 187554   epsilon: 0.8266411000037635    steps: 304    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 977   score: 3.0   memory length: 187783   epsilon: 0.8261876800037733    steps: 229    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 978   score: 2.0   memory length: 187998   epsilon: 0.8257619800037825    steps: 215    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 979   score: 4.0   memory length: 188273   epsilon: 0.8252174800037944    steps: 275    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 980   score: 0.0   memory length: 188396   epsilon: 0.8249739400037996    steps: 123    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 981   score: 3.0   memory length: 188641   epsilon: 0.8244888400038102    steps: 245    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 982   score: 4.0   memory length: 188919   epsilon: 0.8239384000038221    steps: 278    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 983   score: 0.0   memory length: 189042   epsilon: 0.8236948600038274    steps: 123    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 984   score: 2.0   memory length: 189240   epsilon: 0.8233028200038359    steps: 198    lr: 0.0001     evaluation reward: 2.78\n",
            "episode: 985   score: 1.0   memory length: 189410   epsilon: 0.8229662200038432    steps: 170    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 986   score: 2.0   memory length: 189627   epsilon: 0.8225365600038526    steps: 217    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 987   score: 2.0   memory length: 189826   epsilon: 0.8221425400038611    steps: 199    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 988   score: 5.0   memory length: 190153   epsilon: 0.8214950800038752    steps: 327    lr: 0.0001     evaluation reward: 2.78\n",
            "episode: 989   score: 1.0   memory length: 190305   epsilon: 0.8211941200038817    steps: 152    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 990   score: 9.0   memory length: 190683   epsilon: 0.820445680003898    steps: 378    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 991   score: 1.0   memory length: 190835   epsilon: 0.8201447200039045    steps: 152    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 992   score: 2.0   memory length: 191053   epsilon: 0.8197130800039139    steps: 218    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 993   score: 2.0   memory length: 191268   epsilon: 0.8192873800039231    steps: 215    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 994   score: 4.0   memory length: 191544   epsilon: 0.818740900003935    steps: 276    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 995   score: 0.0   memory length: 191667   epsilon: 0.8184973600039402    steps: 123    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 996   score: 3.0   memory length: 191931   epsilon: 0.8179746400039516    steps: 264    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 997   score: 5.0   memory length: 192254   epsilon: 0.8173351000039655    steps: 323    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 998   score: 1.0   memory length: 192425   epsilon: 0.8169965200039728    steps: 171    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 999   score: 4.0   memory length: 192698   epsilon: 0.8164559800039846    steps: 273    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 1000   score: 4.0   memory length: 192955   epsilon: 0.8159471200039956    steps: 257    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 1001   score: 7.0   memory length: 193380   epsilon: 0.8151056200040139    steps: 425    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 1002   score: 3.0   memory length: 193611   epsilon: 0.8146482400040238    steps: 231    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1003   score: 2.0   memory length: 193809   epsilon: 0.8142562000040323    steps: 198    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1004   score: 3.0   memory length: 194034   epsilon: 0.813810700004042    steps: 225    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 1005   score: 0.0   memory length: 194157   epsilon: 0.8135671600040473    steps: 123    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1006   score: 5.0   memory length: 194452   epsilon: 0.81298306000406    steps: 295    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1007   score: 2.0   memory length: 194650   epsilon: 0.8125910200040685    steps: 198    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1008   score: 0.0   memory length: 194772   epsilon: 0.8123494600040737    steps: 122    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 1009   score: 3.0   memory length: 195042   epsilon: 0.8118148600040853    steps: 270    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1010   score: 2.0   memory length: 195239   epsilon: 0.8114248000040938    steps: 197    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1011   score: 3.0   memory length: 195503   epsilon: 0.8109020800041051    steps: 264    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1012   score: 1.0   memory length: 195672   epsilon: 0.8105674600041124    steps: 169    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 1013   score: 1.0   memory length: 195842   epsilon: 0.8102308600041197    steps: 170    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 1014   score: 3.0   memory length: 196067   epsilon: 0.8097853600041294    steps: 225    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1015   score: 2.0   memory length: 196266   epsilon: 0.8093913400041379    steps: 199    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1016   score: 1.0   memory length: 196417   epsilon: 0.8090923600041444    steps: 151    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1017   score: 2.0   memory length: 196615   epsilon: 0.8087003200041529    steps: 198    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1018   score: 3.0   memory length: 196823   epsilon: 0.8082884800041619    steps: 208    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1019   score: 1.0   memory length: 196991   epsilon: 0.8079558400041691    steps: 168    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1020   score: 5.0   memory length: 197357   epsilon: 0.8072311600041848    steps: 366    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1021   score: 0.0   memory length: 197479   epsilon: 0.8069896000041901    steps: 122    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1022   score: 6.0   memory length: 197851   epsilon: 0.8062530400042061    steps: 372    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1023   score: 2.0   memory length: 198049   epsilon: 0.8058610000042146    steps: 198    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1024   score: 1.0   memory length: 198219   epsilon: 0.8055244000042219    steps: 170    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1025   score: 0.0   memory length: 198342   epsilon: 0.8052808600042272    steps: 123    lr: 0.0001     evaluation reward: 2.68\n",
            "episode: 1026   score: 2.0   memory length: 198539   epsilon: 0.8048908000042356    steps: 197    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1027   score: 2.0   memory length: 198737   epsilon: 0.8044987600042441    steps: 198    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1028   score: 3.0   memory length: 198963   epsilon: 0.8040512800042539    steps: 226    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1029   score: 1.0   memory length: 199114   epsilon: 0.8037523000042603    steps: 151    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1030   score: 3.0   memory length: 199343   epsilon: 0.8032988800042702    steps: 229    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1031   score: 4.0   memory length: 199618   epsilon: 0.802754380004282    steps: 275    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1032   score: 7.0   memory length: 200040   epsilon: 0.8019188200043001    steps: 422    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 1033   score: 4.0   memory length: 200315   epsilon: 0.801374320004312    steps: 275    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 1034   score: 6.0   memory length: 200688   epsilon: 0.800635780004328    steps: 373    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 1035   score: 2.0   memory length: 200886   epsilon: 0.8002437400043365    steps: 198    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1036   score: 5.0   memory length: 201233   epsilon: 0.7995566800043514    steps: 347    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1037   score: 3.0   memory length: 201498   epsilon: 0.7990319800043628    steps: 265    lr: 0.0001     evaluation reward: 2.78\n",
            "episode: 1038   score: 2.0   memory length: 201680   epsilon: 0.7986716200043706    steps: 182    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1039   score: 3.0   memory length: 201908   epsilon: 0.7982201800043804    steps: 228    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1040   score: 4.0   memory length: 202183   epsilon: 0.7976756800043923    steps: 275    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 1041   score: 3.0   memory length: 202409   epsilon: 0.797228200004402    steps: 226    lr: 0.0001     evaluation reward: 2.68\n",
            "episode: 1042   score: 3.0   memory length: 202680   epsilon: 0.7966916200044136    steps: 271    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1043   score: 4.0   memory length: 202956   epsilon: 0.7961451400044255    steps: 276    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1044   score: 3.0   memory length: 203222   epsilon: 0.7956184600044369    steps: 266    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1045   score: 1.0   memory length: 203372   epsilon: 0.7953214600044434    steps: 150    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1046   score: 4.0   memory length: 203648   epsilon: 0.7947749800044552    steps: 276    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1047   score: 1.0   memory length: 203819   epsilon: 0.7944364000044626    steps: 171    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1048   score: 2.0   memory length: 203998   epsilon: 0.7940819800044703    steps: 179    lr: 0.0001     evaluation reward: 2.72\n",
            "episode: 1049   score: 3.0   memory length: 204223   epsilon: 0.79363648000448    steps: 225    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1050   score: 1.0   memory length: 204392   epsilon: 0.7933018600044872    steps: 169    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1051   score: 1.0   memory length: 204561   epsilon: 0.7929672400044945    steps: 169    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1052   score: 4.0   memory length: 204858   epsilon: 0.7923791800045072    steps: 297    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 1053   score: 3.0   memory length: 205103   epsilon: 0.7918940800045178    steps: 245    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 1054   score: 3.0   memory length: 205351   epsilon: 0.7914030400045284    steps: 248    lr: 0.0001     evaluation reward: 2.67\n",
            "episode: 1055   score: 4.0   memory length: 205647   epsilon: 0.7908169600045412    steps: 296    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1056   score: 2.0   memory length: 205867   epsilon: 0.7903813600045506    steps: 220    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 1057   score: 3.0   memory length: 206093   epsilon: 0.7899338800045603    steps: 226    lr: 0.0001     evaluation reward: 2.7\n",
            "episode: 1058   score: 3.0   memory length: 206320   epsilon: 0.7894844200045701    steps: 227    lr: 0.0001     evaluation reward: 2.73\n",
            "episode: 1059   score: 4.0   memory length: 206577   epsilon: 0.7889755600045811    steps: 257    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1060   score: 2.0   memory length: 206774   epsilon: 0.7885855000045896    steps: 197    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1061   score: 3.0   memory length: 207001   epsilon: 0.7881360400045994    steps: 227    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 1062   score: 5.0   memory length: 207328   epsilon: 0.7874885800046134    steps: 327    lr: 0.0001     evaluation reward: 2.78\n",
            "episode: 1063   score: 5.0   memory length: 207621   epsilon: 0.786908440004626    steps: 293    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1064   score: 4.0   memory length: 207935   epsilon: 0.7862867200046395    steps: 314    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 1065   score: 2.0   memory length: 208114   epsilon: 0.7859323000046472    steps: 179    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1066   score: 5.0   memory length: 208461   epsilon: 0.7852452400046621    steps: 347    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1067   score: 3.0   memory length: 208688   epsilon: 0.7847957800046719    steps: 227    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 1068   score: 2.0   memory length: 208906   epsilon: 0.7843641400046812    steps: 218    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1069   score: 5.0   memory length: 209232   epsilon: 0.7837186600046953    steps: 326    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 1070   score: 2.0   memory length: 209429   epsilon: 0.7833286000047037    steps: 197    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 1071   score: 0.0   memory length: 209552   epsilon: 0.783085060004709    steps: 123    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1072   score: 4.0   memory length: 209831   epsilon: 0.782532640004721    steps: 279    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 1073   score: 2.0   memory length: 210033   epsilon: 0.7821326800047297    steps: 202    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1074   score: 4.0   memory length: 210292   epsilon: 0.7816198600047408    steps: 259    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1075   score: 5.0   memory length: 210636   epsilon: 0.7809387400047556    steps: 344    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1076   score: 3.0   memory length: 210883   epsilon: 0.7804496800047662    steps: 247    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1077   score: 1.0   memory length: 211052   epsilon: 0.7801150600047735    steps: 169    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1078   score: 2.0   memory length: 211250   epsilon: 0.779723020004782    steps: 198    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1079   score: 2.0   memory length: 211429   epsilon: 0.7793686000047897    steps: 179    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 1080   score: 2.0   memory length: 211646   epsilon: 0.778938940004799    steps: 217    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1081   score: 3.0   memory length: 211895   epsilon: 0.7784459200048097    steps: 249    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 1082   score: 1.0   memory length: 212065   epsilon: 0.778109320004817    steps: 170    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 1083   score: 5.0   memory length: 212389   epsilon: 0.777467800004831    steps: 324    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1084   score: 2.0   memory length: 212588   epsilon: 0.7770737800048395    steps: 199    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1085   score: 3.0   memory length: 212833   epsilon: 0.77658868000485    steps: 245    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 1086   score: 1.0   memory length: 212984   epsilon: 0.7762897000048565    steps: 151    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 1087   score: 5.0   memory length: 213308   epsilon: 0.7756481800048705    steps: 324    lr: 0.0001     evaluation reward: 2.85\n",
            "episode: 1088   score: 1.0   memory length: 213477   epsilon: 0.7753135600048777    steps: 169    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1089   score: 2.0   memory length: 213675   epsilon: 0.7749215200048862    steps: 198    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 1090   score: 2.0   memory length: 213873   epsilon: 0.7745294800048947    steps: 198    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1091   score: 1.0   memory length: 214043   epsilon: 0.774192880004902    steps: 170    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 1092   score: 7.0   memory length: 214401   epsilon: 0.7734840400049174    steps: 358    lr: 0.0001     evaluation reward: 2.8\n",
            "episode: 1093   score: 3.0   memory length: 214664   epsilon: 0.7729633000049287    steps: 263    lr: 0.0001     evaluation reward: 2.81\n",
            "episode: 1094   score: 13.0   memory length: 215086   epsilon: 0.7721277400049469    steps: 422    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1095   score: 1.0   memory length: 215255   epsilon: 0.7717931200049541    steps: 169    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1096   score: 4.0   memory length: 215547   epsilon: 0.7712149600049667    steps: 292    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 1097   score: 2.0   memory length: 215747   epsilon: 0.7708189600049753    steps: 200    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1098   score: 4.0   memory length: 216043   epsilon: 0.770232880004988    steps: 296    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 1099   score: 5.0   memory length: 216384   epsilon: 0.7695577000050027    steps: 341    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1100   score: 4.0   memory length: 216652   epsilon: 0.7690270600050142    steps: 268    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1101   score: 4.0   memory length: 216948   epsilon: 0.7684409800050269    steps: 296    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1102   score: 3.0   memory length: 217194   epsilon: 0.7679539000050375    steps: 246    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1103   score: 1.0   memory length: 217365   epsilon: 0.7676153200050448    steps: 171    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1104   score: 3.0   memory length: 217591   epsilon: 0.7671678400050546    steps: 226    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1105   score: 2.0   memory length: 217789   epsilon: 0.7667758000050631    steps: 198    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1106   score: 4.0   memory length: 218084   epsilon: 0.7661917000050757    steps: 295    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1107   score: 5.0   memory length: 218448   epsilon: 0.7654709800050914    steps: 364    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1108   score: 0.0   memory length: 218570   epsilon: 0.7652294200050966    steps: 122    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1109   score: 6.0   memory length: 218913   epsilon: 0.7645502800051114    steps: 343    lr: 0.0001     evaluation reward: 2.96\n",
            "episode: 1110   score: 3.0   memory length: 219156   epsilon: 0.7640691400051218    steps: 243    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1111   score: 3.0   memory length: 219404   epsilon: 0.7635781000051325    steps: 248    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1112   score: 2.0   memory length: 219622   epsilon: 0.7631464600051419    steps: 218    lr: 0.0001     evaluation reward: 2.98\n",
            "episode: 1113   score: 0.0   memory length: 219745   epsilon: 0.7629029200051471    steps: 123    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1114   score: 3.0   memory length: 219993   epsilon: 0.7624118800051578    steps: 248    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1115   score: 1.0   memory length: 220144   epsilon: 0.7621129000051643    steps: 151    lr: 0.0001     evaluation reward: 2.96\n",
            "episode: 1116   score: 2.0   memory length: 220343   epsilon: 0.7617188800051728    steps: 199    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1117   score: 5.0   memory length: 220649   epsilon: 0.761113000005186    steps: 306    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 1118   score: 3.0   memory length: 220895   epsilon: 0.7606259200051966    steps: 246    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 1119   score: 0.0   memory length: 221018   epsilon: 0.7603823800052019    steps: 123    lr: 0.0001     evaluation reward: 2.99\n",
            "episode: 1120   score: 3.0   memory length: 221265   epsilon: 0.7598933200052125    steps: 247    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1121   score: 6.0   memory length: 221625   epsilon: 0.759180520005228    steps: 360    lr: 0.0001     evaluation reward: 3.03\n",
            "episode: 1122   score: 3.0   memory length: 221887   epsilon: 0.7586617600052392    steps: 262    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 1123   score: 3.0   memory length: 222134   epsilon: 0.7581727000052498    steps: 247    lr: 0.0001     evaluation reward: 3.01\n",
            "episode: 1124   score: 4.0   memory length: 222409   epsilon: 0.7576282000052617    steps: 275    lr: 0.0001     evaluation reward: 3.04\n",
            "episode: 1125   score: 5.0   memory length: 222735   epsilon: 0.7569827200052757    steps: 326    lr: 0.0001     evaluation reward: 3.09\n",
            "episode: 1126   score: 5.0   memory length: 223082   epsilon: 0.7562956600052906    steps: 347    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1127   score: 3.0   memory length: 223332   epsilon: 0.7558006600053013    steps: 250    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1128   score: 2.0   memory length: 223530   epsilon: 0.7554086200053098    steps: 198    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1129   score: 6.0   memory length: 223912   epsilon: 0.7546522600053263    steps: 382    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1130   score: 3.0   memory length: 224141   epsilon: 0.7541988400053361    steps: 229    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1131   score: 7.0   memory length: 224511   epsilon: 0.753466240005352    steps: 370    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1132   score: 2.0   memory length: 224709   epsilon: 0.7530742000053605    steps: 198    lr: 0.0001     evaluation reward: 3.15\n",
            "episode: 1133   score: 6.0   memory length: 225084   epsilon: 0.7523317000053766    steps: 375    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1134   score: 6.0   memory length: 225480   epsilon: 0.7515476200053937    steps: 396    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1135   score: 4.0   memory length: 225775   epsilon: 0.7509635200054063    steps: 295    lr: 0.0001     evaluation reward: 3.19\n",
            "episode: 1136   score: 4.0   memory length: 226070   epsilon: 0.750379420005419    steps: 295    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1137   score: 2.0   memory length: 226287   epsilon: 0.7499497600054283    steps: 217    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1138   score: 2.0   memory length: 226484   epsilon: 0.7495597000054368    steps: 197    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1139   score: 4.0   memory length: 226741   epsilon: 0.7490508400054479    steps: 257    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1140   score: 7.0   memory length: 227167   epsilon: 0.7482073600054662    steps: 426    lr: 0.0001     evaluation reward: 3.21\n",
            "episode: 1141   score: 7.0   memory length: 227573   epsilon: 0.7474034800054836    steps: 406    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1142   score: 3.0   memory length: 227838   epsilon: 0.746878780005495    steps: 265    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1143   score: 7.0   memory length: 228264   epsilon: 0.7460353000055133    steps: 426    lr: 0.0001     evaluation reward: 3.28\n",
            "episode: 1144   score: 6.0   memory length: 228634   epsilon: 0.7453027000055292    steps: 370    lr: 0.0001     evaluation reward: 3.31\n",
            "episode: 1145   score: 3.0   memory length: 228879   epsilon: 0.7448176000055398    steps: 245    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1146   score: 3.0   memory length: 229124   epsilon: 0.7443325000055503    steps: 245    lr: 0.0001     evaluation reward: 3.32\n",
            "episode: 1147   score: 1.0   memory length: 229277   epsilon: 0.7440295600055569    steps: 153    lr: 0.0001     evaluation reward: 3.32\n",
            "episode: 1148   score: 4.0   memory length: 229553   epsilon: 0.7434830800055687    steps: 276    lr: 0.0001     evaluation reward: 3.34\n",
            "episode: 1149   score: 3.0   memory length: 229779   epsilon: 0.7430356000055784    steps: 226    lr: 0.0001     evaluation reward: 3.34\n",
            "episode: 1150   score: 5.0   memory length: 230103   epsilon: 0.7423940800055924    steps: 324    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1151   score: 3.0   memory length: 230333   epsilon: 0.7419386800056023    steps: 230    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1152   score: 4.0   memory length: 230632   epsilon: 0.7413466600056151    steps: 299    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1153   score: 2.0   memory length: 230850   epsilon: 0.7409150200056245    steps: 218    lr: 0.0001     evaluation reward: 3.39\n",
            "episode: 1154   score: 4.0   memory length: 231143   epsilon: 0.7403348800056371    steps: 293    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1155   score: 4.0   memory length: 231422   epsilon: 0.7397824600056491    steps: 279    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1156   score: 4.0   memory length: 231718   epsilon: 0.7391963800056618    steps: 296    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1157   score: 3.0   memory length: 231966   epsilon: 0.7387053400056725    steps: 248    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1158   score: 6.0   memory length: 232293   epsilon: 0.7380578800056865    steps: 327    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1159   score: 4.0   memory length: 232553   epsilon: 0.7375430800056977    steps: 260    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1160   score: 3.0   memory length: 232779   epsilon: 0.7370956000057074    steps: 226    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1161   score: 3.0   memory length: 233046   epsilon: 0.7365669400057189    steps: 267    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1162   score: 0.0   memory length: 233169   epsilon: 0.7363234000057242    steps: 123    lr: 0.0001     evaluation reward: 3.41\n",
            "episode: 1163   score: 2.0   memory length: 233388   epsilon: 0.7358897800057336    steps: 219    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1164   score: 3.0   memory length: 233615   epsilon: 0.7354403200057433    steps: 227    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1165   score: 6.0   memory length: 233986   epsilon: 0.7347057400057593    steps: 371    lr: 0.0001     evaluation reward: 3.41\n",
            "episode: 1166   score: 1.0   memory length: 234137   epsilon: 0.7344067600057658    steps: 151    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1167   score: 3.0   memory length: 234366   epsilon: 0.7339533400057756    steps: 229    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1168   score: 3.0   memory length: 234609   epsilon: 0.7334722000057861    steps: 243    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1169   score: 4.0   memory length: 234884   epsilon: 0.7329277000057979    steps: 275    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1170   score: 4.0   memory length: 235159   epsilon: 0.7323832000058097    steps: 275    lr: 0.0001     evaluation reward: 3.39\n",
            "episode: 1171   score: 3.0   memory length: 235424   epsilon: 0.7318585000058211    steps: 265    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1172   score: 5.0   memory length: 235751   epsilon: 0.7312110400058351    steps: 327    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1173   score: 3.0   memory length: 235981   epsilon: 0.730755640005845    steps: 230    lr: 0.0001     evaluation reward: 3.44\n",
            "episode: 1174   score: 3.0   memory length: 236209   epsilon: 0.7303042000058548    steps: 228    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1175   score: 4.0   memory length: 236487   epsilon: 0.7297537600058668    steps: 278    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1176   score: 6.0   memory length: 236855   epsilon: 0.7290251200058826    steps: 368    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1177   score: 3.0   memory length: 237081   epsilon: 0.7285776400058923    steps: 226    lr: 0.0001     evaluation reward: 3.47\n",
            "episode: 1178   score: 4.0   memory length: 237341   epsilon: 0.7280628400059035    steps: 260    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1179   score: 2.0   memory length: 237523   epsilon: 0.7277024800059113    steps: 182    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1180   score: 3.0   memory length: 237770   epsilon: 0.7272134200059219    steps: 247    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1181   score: 3.0   memory length: 237995   epsilon: 0.7267679200059316    steps: 225    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1182   score: 5.0   memory length: 238321   epsilon: 0.7261224400059456    steps: 326    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1183   score: 2.0   memory length: 238519   epsilon: 0.7257304000059541    steps: 198    lr: 0.0001     evaluation reward: 3.51\n",
            "episode: 1184   score: 3.0   memory length: 238745   epsilon: 0.7252829200059638    steps: 226    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1185   score: 3.0   memory length: 238971   epsilon: 0.7248354400059736    steps: 226    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1186   score: 3.0   memory length: 239197   epsilon: 0.7243879600059833    steps: 226    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1187   score: 0.0   memory length: 239319   epsilon: 0.7241464000059885    steps: 122    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1188   score: 2.0   memory length: 239501   epsilon: 0.7237860400059963    steps: 182    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1189   score: 4.0   memory length: 239799   epsilon: 0.7231960000060091    steps: 298    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1190   score: 6.0   memory length: 240191   epsilon: 0.722419840006026    steps: 392    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1191   score: 6.0   memory length: 240566   epsilon: 0.7216773400060421    steps: 375    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1192   score: 3.0   memory length: 240792   epsilon: 0.7212298600060518    steps: 226    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1193   score: 2.0   memory length: 240973   epsilon: 0.7208714800060596    steps: 181    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1194   score: 4.0   memory length: 241235   epsilon: 0.7203527200060709    steps: 262    lr: 0.0001     evaluation reward: 3.47\n",
            "episode: 1195   score: 3.0   memory length: 241461   epsilon: 0.7199052400060806    steps: 226    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1196   score: 2.0   memory length: 241659   epsilon: 0.7195132000060891    steps: 198    lr: 0.0001     evaluation reward: 3.47\n",
            "episode: 1197   score: 1.0   memory length: 241828   epsilon: 0.7191785800060964    steps: 169    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1198   score: 3.0   memory length: 242073   epsilon: 0.7186934800061069    steps: 245    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1199   score: 5.0   memory length: 242418   epsilon: 0.7180103800061217    steps: 345    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1200   score: 3.0   memory length: 242644   epsilon: 0.7175629000061314    steps: 226    lr: 0.0001     evaluation reward: 3.44\n",
            "episode: 1201   score: 2.0   memory length: 242842   epsilon: 0.7171708600061399    steps: 198    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1202   score: 3.0   memory length: 243089   epsilon: 0.7166818000061506    steps: 247    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1203   score: 3.0   memory length: 243336   epsilon: 0.7161927400061612    steps: 247    lr: 0.0001     evaluation reward: 3.44\n",
            "episode: 1204   score: 5.0   memory length: 243659   epsilon: 0.7155532000061751    steps: 323    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1205   score: 3.0   memory length: 243885   epsilon: 0.7151057200061848    steps: 226    lr: 0.0001     evaluation reward: 3.47\n",
            "episode: 1206   score: 5.0   memory length: 244212   epsilon: 0.7144582600061988    steps: 327    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1207   score: 8.0   memory length: 244633   epsilon: 0.7136246800062169    steps: 421    lr: 0.0001     evaluation reward: 3.51\n",
            "episode: 1208   score: 5.0   memory length: 244977   epsilon: 0.7129435600062317    steps: 344    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1209   score: 4.0   memory length: 245253   epsilon: 0.7123970800062436    steps: 276    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1210   score: 4.0   memory length: 245518   epsilon: 0.711872380006255    steps: 265    lr: 0.0001     evaluation reward: 3.55\n",
            "episode: 1211   score: 2.0   memory length: 245718   epsilon: 0.7114763800062636    steps: 200    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1212   score: 1.0   memory length: 245868   epsilon: 0.71117938000627    steps: 150    lr: 0.0001     evaluation reward: 3.53\n",
            "episode: 1213   score: 4.0   memory length: 246127   epsilon: 0.7106665600062811    steps: 259    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1214   score: 4.0   memory length: 246402   epsilon: 0.710122060006293    steps: 275    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1215   score: 4.0   memory length: 246677   epsilon: 0.7095775600063048    steps: 275    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1216   score: 2.0   memory length: 246857   epsilon: 0.7092211600063125    steps: 180    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1217   score: 5.0   memory length: 247182   epsilon: 0.7085776600063265    steps: 325    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1218   score: 3.0   memory length: 247427   epsilon: 0.708092560006337    steps: 245    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1219   score: 7.0   memory length: 247850   epsilon: 0.7072550200063552    steps: 423    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1220   score: 3.0   memory length: 248096   epsilon: 0.7067679400063658    steps: 246    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1221   score: 4.0   memory length: 248413   epsilon: 0.7061402800063794    steps: 317    lr: 0.0001     evaluation reward: 3.66\n",
            "episode: 1222   score: 5.0   memory length: 248721   epsilon: 0.7055304400063926    steps: 308    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1223   score: 4.0   memory length: 249018   epsilon: 0.7049423800064054    steps: 297    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1224   score: 8.0   memory length: 249436   epsilon: 0.7041147400064234    steps: 418    lr: 0.0001     evaluation reward: 3.73\n",
            "episode: 1225   score: 4.0   memory length: 249732   epsilon: 0.7035286600064361    steps: 296    lr: 0.0001     evaluation reward: 3.72\n",
            "episode: 1226   score: 4.0   memory length: 250027   epsilon: 0.7029445600064488    steps: 295    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1227   score: 3.0   memory length: 250273   epsilon: 0.7024574800064594    steps: 246    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1228   score: 4.0   memory length: 250568   epsilon: 0.701873380006472    steps: 295    lr: 0.0001     evaluation reward: 3.73\n",
            "episode: 1229   score: 2.0   memory length: 250787   epsilon: 0.7014397600064814    steps: 219    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1230   score: 4.0   memory length: 251081   epsilon: 0.7008576400064941    steps: 294    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1231   score: 2.0   memory length: 251278   epsilon: 0.7004675800065026    steps: 197    lr: 0.0001     evaluation reward: 3.65\n",
            "episode: 1232   score: 6.0   memory length: 251656   epsilon: 0.6997191400065188    steps: 378    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1233   score: 6.0   memory length: 252019   epsilon: 0.6990004000065344    steps: 363    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1234   score: 4.0   memory length: 252264   epsilon: 0.6985153000065449    steps: 245    lr: 0.0001     evaluation reward: 3.67\n",
            "episode: 1235   score: 5.0   memory length: 252592   epsilon: 0.697865860006559    steps: 328    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1236   score: 5.0   memory length: 252917   epsilon: 0.697222360006573    steps: 325    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1237   score: 4.0   memory length: 253192   epsilon: 0.6966778600065848    steps: 275    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1238   score: 4.0   memory length: 253469   epsilon: 0.6961294000065967    steps: 277    lr: 0.0001     evaluation reward: 3.73\n",
            "episode: 1239   score: 2.0   memory length: 253667   epsilon: 0.6957373600066052    steps: 198    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1240   score: 4.0   memory length: 253944   epsilon: 0.6951889000066171    steps: 277    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1241   score: 5.0   memory length: 254265   epsilon: 0.694553320006631    steps: 321    lr: 0.0001     evaluation reward: 3.66\n",
            "episode: 1242   score: 4.0   memory length: 254519   epsilon: 0.6940504000066419    steps: 254    lr: 0.0001     evaluation reward: 3.67\n",
            "episode: 1243   score: 2.0   memory length: 254720   epsilon: 0.6936524200066505    steps: 201    lr: 0.0001     evaluation reward: 3.62\n",
            "episode: 1244   score: 3.0   memory length: 254947   epsilon: 0.6932029600066603    steps: 227    lr: 0.0001     evaluation reward: 3.59\n",
            "episode: 1245   score: 2.0   memory length: 255145   epsilon: 0.6928109200066688    steps: 198    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1246   score: 2.0   memory length: 255342   epsilon: 0.6924208600066772    steps: 197    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1247   score: 1.0   memory length: 255492   epsilon: 0.6921238600066837    steps: 150    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1248   score: 4.0   memory length: 255806   epsilon: 0.6915021400066972    steps: 314    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1249   score: 3.0   memory length: 256054   epsilon: 0.6910111000067078    steps: 248    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1250   score: 3.0   memory length: 256300   epsilon: 0.6905240200067184    steps: 246    lr: 0.0001     evaluation reward: 3.55\n",
            "episode: 1251   score: 6.0   memory length: 256634   epsilon: 0.6898627000067328    steps: 334    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1252   score: 2.0   memory length: 256852   epsilon: 0.6894310600067421    steps: 218    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1253   score: 3.0   memory length: 257099   epsilon: 0.6889420000067528    steps: 247    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1254   score: 3.0   memory length: 257325   epsilon: 0.6884945200067625    steps: 226    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1255   score: 5.0   memory length: 257649   epsilon: 0.6878530000067764    steps: 324    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1256   score: 6.0   memory length: 258042   epsilon: 0.6870748600067933    steps: 393    lr: 0.0001     evaluation reward: 3.59\n",
            "episode: 1257   score: 4.0   memory length: 258357   epsilon: 0.6864511600068068    steps: 315    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1258   score: 8.0   memory length: 258800   epsilon: 0.6855740200068259    steps: 443    lr: 0.0001     evaluation reward: 3.62\n",
            "episode: 1259   score: 2.0   memory length: 259018   epsilon: 0.6851423800068352    steps: 218    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1260   score: 3.0   memory length: 259265   epsilon: 0.6846533200068459    steps: 247    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1261   score: 5.0   memory length: 259591   epsilon: 0.6840078400068599    steps: 326    lr: 0.0001     evaluation reward: 3.62\n",
            "episode: 1262   score: 6.0   memory length: 259984   epsilon: 0.6832297000068768    steps: 393    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1263   score: 3.0   memory length: 260231   epsilon: 0.6827406400068874    steps: 247    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1264   score: 5.0   memory length: 260577   epsilon: 0.6820555600069023    steps: 346    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1265   score: 3.0   memory length: 260824   epsilon: 0.6815665000069129    steps: 247    lr: 0.0001     evaluation reward: 3.68\n",
            "episode: 1266   score: 2.0   memory length: 261042   epsilon: 0.6811348600069222    steps: 218    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1267   score: 4.0   memory length: 261335   epsilon: 0.6805547200069348    steps: 293    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1268   score: 3.0   memory length: 261560   epsilon: 0.6801092200069445    steps: 225    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1269   score: 4.0   memory length: 261837   epsilon: 0.6795607600069564    steps: 277    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1270   score: 4.0   memory length: 262114   epsilon: 0.6790123000069683    steps: 277    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1271   score: 6.0   memory length: 262508   epsilon: 0.6782321800069853    steps: 394    lr: 0.0001     evaluation reward: 3.73\n",
            "episode: 1272   score: 3.0   memory length: 262751   epsilon: 0.6777510400069957    steps: 243    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1273   score: 3.0   memory length: 262977   epsilon: 0.6773035600070054    steps: 226    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1274   score: 3.0   memory length: 263205   epsilon: 0.6768521200070152    steps: 228    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1275   score: 4.0   memory length: 263485   epsilon: 0.6762977200070273    steps: 280    lr: 0.0001     evaluation reward: 3.71\n",
            "episode: 1276   score: 12.0   memory length: 263963   epsilon: 0.6753512800070478    steps: 478    lr: 0.0001     evaluation reward: 3.77\n",
            "episode: 1277   score: 1.0   memory length: 264114   epsilon: 0.6750523000070543    steps: 151    lr: 0.0001     evaluation reward: 3.75\n",
            "episode: 1278   score: 2.0   memory length: 264312   epsilon: 0.6746602600070628    steps: 198    lr: 0.0001     evaluation reward: 3.73\n",
            "episode: 1279   score: 3.0   memory length: 264537   epsilon: 0.6742147600070725    steps: 225    lr: 0.0001     evaluation reward: 3.74\n",
            "episode: 1280   score: 7.0   memory length: 264978   epsilon: 0.6733415800070914    steps: 441    lr: 0.0001     evaluation reward: 3.78\n",
            "episode: 1281   score: 5.0   memory length: 265281   epsilon: 0.6727416400071045    steps: 303    lr: 0.0001     evaluation reward: 3.8\n",
            "episode: 1282   score: 5.0   memory length: 265588   epsilon: 0.6721337800071177    steps: 307    lr: 0.0001     evaluation reward: 3.8\n",
            "episode: 1283   score: 5.0   memory length: 265930   epsilon: 0.6714566200071324    steps: 342    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1284   score: 3.0   memory length: 266198   epsilon: 0.6709259800071439    steps: 268    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1285   score: 3.0   memory length: 266427   epsilon: 0.6704725600071537    steps: 229    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1286   score: 6.0   memory length: 266808   epsilon: 0.6697181800071701    steps: 381    lr: 0.0001     evaluation reward: 3.86\n",
            "episode: 1287   score: 3.0   memory length: 267034   epsilon: 0.6692707000071798    steps: 226    lr: 0.0001     evaluation reward: 3.89\n",
            "episode: 1288   score: 4.0   memory length: 267330   epsilon: 0.6686846200071925    steps: 296    lr: 0.0001     evaluation reward: 3.91\n",
            "episode: 1289   score: 2.0   memory length: 267547   epsilon: 0.6682549600072019    steps: 217    lr: 0.0001     evaluation reward: 3.89\n",
            "episode: 1290   score: 3.0   memory length: 267794   epsilon: 0.6677659000072125    steps: 247    lr: 0.0001     evaluation reward: 3.86\n",
            "episode: 1291   score: 4.0   memory length: 268090   epsilon: 0.6671798200072252    steps: 296    lr: 0.0001     evaluation reward: 3.84\n",
            "episode: 1292   score: 1.0   memory length: 268241   epsilon: 0.6668808400072317    steps: 151    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1293   score: 3.0   memory length: 268466   epsilon: 0.6664353400072414    steps: 225    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1294   score: 3.0   memory length: 268693   epsilon: 0.6659858800072511    steps: 227    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1295   score: 8.0   memory length: 269108   epsilon: 0.665164180007269    steps: 415    lr: 0.0001     evaluation reward: 3.87\n",
            "episode: 1296   score: 4.0   memory length: 269383   epsilon: 0.6646196800072808    steps: 275    lr: 0.0001     evaluation reward: 3.89\n",
            "episode: 1297   score: 6.0   memory length: 269739   epsilon: 0.6639148000072961    steps: 356    lr: 0.0001     evaluation reward: 3.94\n",
            "episode: 1298   score: 4.0   memory length: 270033   epsilon: 0.6633326800073087    steps: 294    lr: 0.0001     evaluation reward: 3.95\n",
            "episode: 1299   score: 5.0   memory length: 270357   epsilon: 0.6626911600073226    steps: 324    lr: 0.0001     evaluation reward: 3.95\n",
            "episode: 1300   score: 4.0   memory length: 270633   epsilon: 0.6621446800073345    steps: 276    lr: 0.0001     evaluation reward: 3.96\n",
            "episode: 1301   score: 8.0   memory length: 271105   epsilon: 0.6612101200073548    steps: 472    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1302   score: 2.0   memory length: 271284   epsilon: 0.6608557000073625    steps: 179    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1303   score: 3.0   memory length: 271510   epsilon: 0.6604082200073722    steps: 226    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1304   score: 5.0   memory length: 271837   epsilon: 0.6597607600073863    steps: 327    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1305   score: 5.0   memory length: 272162   epsilon: 0.6591172600074002    steps: 325    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1306   score: 11.0   memory length: 272558   epsilon: 0.6583331800074173    steps: 396    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1307   score: 5.0   memory length: 272903   epsilon: 0.6576500800074321    steps: 345    lr: 0.0001     evaluation reward: 4.06\n",
            "episode: 1308   score: 1.0   memory length: 273072   epsilon: 0.6573154600074393    steps: 169    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1309   score: 2.0   memory length: 273270   epsilon: 0.6569234200074479    steps: 198    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1310   score: 3.0   memory length: 273538   epsilon: 0.6563927800074594    steps: 268    lr: 0.0001     evaluation reward: 3.99\n",
            "episode: 1311   score: 5.0   memory length: 273862   epsilon: 0.6557512600074733    steps: 324    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1312   score: 3.0   memory length: 274109   epsilon: 0.6552622000074839    steps: 247    lr: 0.0001     evaluation reward: 4.04\n",
            "episode: 1313   score: 5.0   memory length: 274412   epsilon: 0.6546622600074969    steps: 303    lr: 0.0001     evaluation reward: 4.05\n",
            "episode: 1314   score: 2.0   memory length: 274610   epsilon: 0.6542702200075055    steps: 198    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1315   score: 4.0   memory length: 274910   epsilon: 0.6536762200075183    steps: 300    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1316   score: 2.0   memory length: 275126   epsilon: 0.6532485400075276    steps: 216    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1317   score: 3.0   memory length: 275371   epsilon: 0.6527634400075382    steps: 245    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1318   score: 2.0   memory length: 275568   epsilon: 0.6523733800075466    steps: 197    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1319   score: 6.0   memory length: 275942   epsilon: 0.6516328600075627    steps: 374    lr: 0.0001     evaluation reward: 3.99\n",
            "episode: 1320   score: 5.0   memory length: 276265   epsilon: 0.6509933200075766    steps: 323    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1321   score: 5.0   memory length: 276569   epsilon: 0.6503914000075897    steps: 304    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1322   score: 8.0   memory length: 277018   epsilon: 0.649502380007609    steps: 449    lr: 0.0001     evaluation reward: 4.05\n",
            "episode: 1323   score: 4.0   memory length: 277293   epsilon: 0.6489578800076208    steps: 275    lr: 0.0001     evaluation reward: 4.05\n",
            "episode: 1324   score: 4.0   memory length: 277590   epsilon: 0.6483698200076335    steps: 297    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1325   score: 7.0   memory length: 278011   epsilon: 0.6475362400076516    steps: 421    lr: 0.0001     evaluation reward: 4.04\n",
            "episode: 1326   score: 3.0   memory length: 278258   epsilon: 0.6470471800076623    steps: 247    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1327   score: 6.0   memory length: 278635   epsilon: 0.6463007200076785    steps: 377    lr: 0.0001     evaluation reward: 4.06\n",
            "episode: 1328   score: 7.0   memory length: 279072   epsilon: 0.6454354600076972    steps: 437    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1329   score: 4.0   memory length: 279331   epsilon: 0.6449226400077084    steps: 259    lr: 0.0001     evaluation reward: 4.11\n",
            "episode: 1330   score: 5.0   memory length: 279676   epsilon: 0.6442395400077232    steps: 345    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1331   score: 3.0   memory length: 279903   epsilon: 0.643790080007733    steps: 227    lr: 0.0001     evaluation reward: 4.13\n",
            "episode: 1332   score: 5.0   memory length: 280217   epsilon: 0.6431683600077465    steps: 314    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1333   score: 2.0   memory length: 280434   epsilon: 0.6427387000077558    steps: 217    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1334   score: 5.0   memory length: 280736   epsilon: 0.6421407400077688    steps: 302    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1335   score: 4.0   memory length: 281003   epsilon: 0.6416120800077802    steps: 267    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1336   score: 4.0   memory length: 281279   epsilon: 0.6410656000077921    steps: 276    lr: 0.0001     evaluation reward: 4.07\n",
            "episode: 1337   score: 5.0   memory length: 281605   epsilon: 0.6404201200078061    steps: 326    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1338   score: 4.0   memory length: 281902   epsilon: 0.6398320600078189    steps: 297    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1339   score: 5.0   memory length: 282213   epsilon: 0.6392162800078323    steps: 311    lr: 0.0001     evaluation reward: 4.11\n",
            "episode: 1340   score: 2.0   memory length: 282410   epsilon: 0.6388262200078407    steps: 197    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1341   score: 4.0   memory length: 282685   epsilon: 0.6382817200078525    steps: 275    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1342   score: 1.0   memory length: 282836   epsilon: 0.637982740007859    steps: 151    lr: 0.0001     evaluation reward: 4.05\n",
            "episode: 1343   score: 7.0   memory length: 283134   epsilon: 0.6373927000078718    steps: 298    lr: 0.0001     evaluation reward: 4.1\n",
            "episode: 1344   score: 3.0   memory length: 283360   epsilon: 0.6369452200078816    steps: 226    lr: 0.0001     evaluation reward: 4.1\n",
            "episode: 1345   score: 4.0   memory length: 283656   epsilon: 0.6363591400078943    steps: 296    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1346   score: 5.0   memory length: 283997   epsilon: 0.6356839600079089    steps: 341    lr: 0.0001     evaluation reward: 4.15\n",
            "episode: 1347   score: 5.0   memory length: 284321   epsilon: 0.6350424400079229    steps: 324    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1348   score: 4.0   memory length: 284616   epsilon: 0.6344583400079356    steps: 295    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1349   score: 5.0   memory length: 284923   epsilon: 0.6338504800079487    steps: 307    lr: 0.0001     evaluation reward: 4.21\n",
            "episode: 1350   score: 5.0   memory length: 285232   epsilon: 0.633238660007962    steps: 309    lr: 0.0001     evaluation reward: 4.23\n",
            "episode: 1351   score: 5.0   memory length: 285555   epsilon: 0.6325991200079759    steps: 323    lr: 0.0001     evaluation reward: 4.22\n",
            "episode: 1352   score: 2.0   memory length: 285754   epsilon: 0.6322051000079845    steps: 199    lr: 0.0001     evaluation reward: 4.22\n",
            "episode: 1353   score: 3.0   memory length: 286000   epsilon: 0.631718020007995    steps: 246    lr: 0.0001     evaluation reward: 4.22\n",
            "episode: 1354   score: 2.0   memory length: 286218   epsilon: 0.6312863800080044    steps: 218    lr: 0.0001     evaluation reward: 4.21\n",
            "episode: 1355   score: 4.0   memory length: 286495   epsilon: 0.6307379200080163    steps: 277    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1356   score: 4.0   memory length: 286775   epsilon: 0.6301835200080284    steps: 280    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1357   score: 4.0   memory length: 287071   epsilon: 0.6295974400080411    steps: 296    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1358   score: 4.0   memory length: 287348   epsilon: 0.629048980008053    steps: 277    lr: 0.0001     evaluation reward: 4.14\n",
            "episode: 1359   score: 3.0   memory length: 287597   epsilon: 0.6285559600080637    steps: 249    lr: 0.0001     evaluation reward: 4.15\n",
            "episode: 1360   score: 7.0   memory length: 287989   epsilon: 0.6277798000080805    steps: 392    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1361   score: 3.0   memory length: 288235   epsilon: 0.6272927200080911    steps: 246    lr: 0.0001     evaluation reward: 4.17\n",
            "episode: 1362   score: 5.0   memory length: 288579   epsilon: 0.6266116000081059    steps: 344    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1363   score: 3.0   memory length: 288822   epsilon: 0.6261304600081163    steps: 243    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1364   score: 5.0   memory length: 289125   epsilon: 0.6255305200081294    steps: 303    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1365   score: 6.0   memory length: 289478   epsilon: 0.6248315800081445    steps: 353    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1366   score: 3.0   memory length: 289725   epsilon: 0.6243425200081552    steps: 247    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1367   score: 4.0   memory length: 290020   epsilon: 0.6237584200081678    steps: 295    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1368   score: 3.0   memory length: 290246   epsilon: 0.6233109400081775    steps: 226    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1369   score: 0.0   memory length: 290368   epsilon: 0.6230693800081828    steps: 122    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1370   score: 2.0   memory length: 290567   epsilon: 0.6226753600081913    steps: 199    lr: 0.0001     evaluation reward: 4.14\n",
            "episode: 1371   score: 6.0   memory length: 290942   epsilon: 0.6219328600082075    steps: 375    lr: 0.0001     evaluation reward: 4.14\n",
            "episode: 1372   score: 5.0   memory length: 291266   epsilon: 0.6212913400082214    steps: 324    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1373   score: 5.0   memory length: 291582   epsilon: 0.620665660008235    steps: 316    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1374   score: 4.0   memory length: 291839   epsilon: 0.620156800008246    steps: 257    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1375   score: 6.0   memory length: 292207   epsilon: 0.6194281600082618    steps: 368    lr: 0.0001     evaluation reward: 4.21\n",
            "episode: 1376   score: 6.0   memory length: 292544   epsilon: 0.6187609000082763    steps: 337    lr: 0.0001     evaluation reward: 4.15\n",
            "episode: 1377   score: 6.0   memory length: 292900   epsilon: 0.6180560200082916    steps: 356    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1378   score: 4.0   memory length: 293196   epsilon: 0.6174699400083044    steps: 296    lr: 0.0001     evaluation reward: 4.22\n",
            "episode: 1379   score: 4.0   memory length: 293473   epsilon: 0.6169214800083163    steps: 277    lr: 0.0001     evaluation reward: 4.23\n",
            "episode: 1380   score: 1.0   memory length: 293624   epsilon: 0.6166225000083227    steps: 151    lr: 0.0001     evaluation reward: 4.17\n",
            "episode: 1381   score: 6.0   memory length: 293943   epsilon: 0.6159908800083365    steps: 319    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1382   score: 2.0   memory length: 294141   epsilon: 0.615598840008345    steps: 198    lr: 0.0001     evaluation reward: 4.15\n",
            "episode: 1383   score: 6.0   memory length: 294485   epsilon: 0.6149177200083598    steps: 344    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1384   score: 3.0   memory length: 294713   epsilon: 0.6144662800083696    steps: 228    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1385   score: 5.0   memory length: 295058   epsilon: 0.6137831800083844    steps: 345    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1386   score: 7.0   memory length: 295425   epsilon: 0.6130565200084002    steps: 367    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1387   score: 5.0   memory length: 295749   epsilon: 0.6124150000084141    steps: 324    lr: 0.0001     evaluation reward: 4.21\n",
            "episode: 1388   score: 6.0   memory length: 296070   epsilon: 0.6117794200084279    steps: 321    lr: 0.0001     evaluation reward: 4.23\n",
            "episode: 1389   score: 7.0   memory length: 296448   epsilon: 0.6110309800084441    steps: 378    lr: 0.0001     evaluation reward: 4.28\n",
            "episode: 1390   score: 3.0   memory length: 296674   epsilon: 0.6105835000084538    steps: 226    lr: 0.0001     evaluation reward: 4.28\n",
            "episode: 1391   score: 7.0   memory length: 297012   epsilon: 0.6099142600084684    steps: 338    lr: 0.0001     evaluation reward: 4.31\n",
            "episode: 1392   score: 5.0   memory length: 297322   epsilon: 0.6093004600084817    steps: 310    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1393   score: 2.0   memory length: 297520   epsilon: 0.6089084200084902    steps: 198    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1394   score: 6.0   memory length: 297884   epsilon: 0.6081877000085059    steps: 364    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1395   score: 8.0   memory length: 298214   epsilon: 0.60753430000852    steps: 330    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1396   score: 4.0   memory length: 298458   epsilon: 0.6070511800085305    steps: 244    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1397   score: 3.0   memory length: 298687   epsilon: 0.6065977600085404    steps: 229    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1398   score: 5.0   memory length: 299028   epsilon: 0.605922580008555    steps: 341    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1399   score: 5.0   memory length: 299333   epsilon: 0.6053186800085681    steps: 305    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1400   score: 2.0   memory length: 299531   epsilon: 0.6049266400085767    steps: 198    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1401   score: 10.0   memory length: 300015   epsilon: 0.6039683200085975    steps: 484    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1402   score: 4.0   memory length: 300293   epsilon: 0.6034178800086094    steps: 278    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1403   score: 6.0   memory length: 300630   epsilon: 0.6027506200086239    steps: 337    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1404   score: 5.0   memory length: 300916   epsilon: 0.6021843400086362    steps: 286    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1405   score: 4.0   memory length: 301212   epsilon: 0.6015982600086489    steps: 296    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1406   score: 5.0   memory length: 301528   epsilon: 0.6009725800086625    steps: 316    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1407   score: 4.0   memory length: 301823   epsilon: 0.6003884800086752    steps: 295    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1408   score: 9.0   memory length: 302313   epsilon: 0.5994182800086962    steps: 490    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1409   score: 7.0   memory length: 302687   epsilon: 0.5986777600087123    steps: 374    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1410   score: 5.0   memory length: 302983   epsilon: 0.598091680008725    steps: 296    lr: 0.0001     evaluation reward: 4.47\n",
            "episode: 1411   score: 4.0   memory length: 303261   epsilon: 0.597541240008737    steps: 278    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1412   score: 3.0   memory length: 303486   epsilon: 0.5970957400087467    steps: 225    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1413   score: 4.0   memory length: 303787   epsilon: 0.5964997600087596    steps: 301    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1414   score: 4.0   memory length: 304042   epsilon: 0.5959948600087706    steps: 255    lr: 0.0001     evaluation reward: 4.47\n",
            "episode: 1415   score: 3.0   memory length: 304268   epsilon: 0.5955473800087803    steps: 226    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1416   score: 4.0   memory length: 304564   epsilon: 0.594961300008793    steps: 296    lr: 0.0001     evaluation reward: 4.48\n",
            "episode: 1417   score: 6.0   memory length: 304901   epsilon: 0.5942940400088075    steps: 337    lr: 0.0001     evaluation reward: 4.51\n",
            "episode: 1418   score: 4.0   memory length: 305180   epsilon: 0.5937416200088195    steps: 279    lr: 0.0001     evaluation reward: 4.53\n",
            "episode: 1419   score: 7.0   memory length: 305567   epsilon: 0.5929753600088361    steps: 387    lr: 0.0001     evaluation reward: 4.54\n",
            "episode: 1420   score: 3.0   memory length: 305779   epsilon: 0.5925556000088452    steps: 212    lr: 0.0001     evaluation reward: 4.52\n",
            "episode: 1421   score: 4.0   memory length: 306038   epsilon: 0.5920427800088564    steps: 259    lr: 0.0001     evaluation reward: 4.51\n",
            "episode: 1422   score: 3.0   memory length: 306263   epsilon: 0.591597280008866    steps: 225    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1423   score: 3.0   memory length: 306510   epsilon: 0.5911082200088766    steps: 247    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1424   score: 4.0   memory length: 306784   epsilon: 0.5905657000088884    steps: 274    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1425   score: 3.0   memory length: 306994   epsilon: 0.5901499000088974    steps: 210    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1426   score: 4.0   memory length: 307270   epsilon: 0.5896034200089093    steps: 276    lr: 0.0001     evaluation reward: 4.42\n",
            "episode: 1427   score: 3.0   memory length: 307514   epsilon: 0.5891203000089198    steps: 244    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1428   score: 4.0   memory length: 307789   epsilon: 0.5885758000089316    steps: 275    lr: 0.0001     evaluation reward: 4.36\n",
            "episode: 1429   score: 7.0   memory length: 308178   epsilon: 0.5878055800089483    steps: 389    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1430   score: 6.0   memory length: 308531   epsilon: 0.5871066400089635    steps: 353    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1431   score: 4.0   memory length: 308793   epsilon: 0.5865878800089748    steps: 262    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1432   score: 2.0   memory length: 308991   epsilon: 0.5861958400089833    steps: 198    lr: 0.0001     evaluation reward: 4.38\n",
            "episode: 1433   score: 9.0   memory length: 309484   epsilon: 0.5852197000090045    steps: 493    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1434   score: 4.0   memory length: 309780   epsilon: 0.5846336200090172    steps: 296    lr: 0.0001     evaluation reward: 4.44\n",
            "episode: 1435   score: 5.0   memory length: 310107   epsilon: 0.5839861600090313    steps: 327    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1436   score: 5.0   memory length: 310415   epsilon: 0.5833763200090445    steps: 308    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1437   score: 5.0   memory length: 310781   epsilon: 0.5826516400090602    steps: 366    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1438   score: 10.0   memory length: 311276   epsilon: 0.5816715400090815    steps: 495    lr: 0.0001     evaluation reward: 4.52\n",
            "episode: 1439   score: 8.0   memory length: 311738   epsilon: 0.5807567800091014    steps: 462    lr: 0.0001     evaluation reward: 4.55\n",
            "episode: 1440   score: 5.0   memory length: 312044   epsilon: 0.5801509000091145    steps: 306    lr: 0.0001     evaluation reward: 4.58\n",
            "episode: 1441   score: 5.0   memory length: 312368   epsilon: 0.5795093800091284    steps: 324    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1442   score: 5.0   memory length: 312677   epsilon: 0.5788975600091417    steps: 309    lr: 0.0001     evaluation reward: 4.63\n",
            "episode: 1443   score: 5.0   memory length: 313001   epsilon: 0.5782560400091556    steps: 324    lr: 0.0001     evaluation reward: 4.61\n",
            "episode: 1444   score: 6.0   memory length: 313371   epsilon: 0.5775234400091716    steps: 370    lr: 0.0001     evaluation reward: 4.64\n",
            "episode: 1445   score: 2.0   memory length: 313551   epsilon: 0.5771670400091793    steps: 180    lr: 0.0001     evaluation reward: 4.62\n",
            "episode: 1446   score: 4.0   memory length: 313810   epsilon: 0.5766542200091904    steps: 259    lr: 0.0001     evaluation reward: 4.61\n",
            "episode: 1447   score: 6.0   memory length: 314154   epsilon: 0.5759731000092052    steps: 344    lr: 0.0001     evaluation reward: 4.62\n",
            "episode: 1448   score: 4.0   memory length: 314395   epsilon: 0.5754959200092156    steps: 241    lr: 0.0001     evaluation reward: 4.62\n",
            "episode: 1449   score: 2.0   memory length: 314612   epsilon: 0.5750662600092249    steps: 217    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1450   score: 2.0   memory length: 314794   epsilon: 0.5747059000092327    steps: 182    lr: 0.0001     evaluation reward: 4.56\n",
            "episode: 1451   score: 8.0   memory length: 315246   epsilon: 0.5738109400092521    steps: 452    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1452   score: 3.0   memory length: 315471   epsilon: 0.5733654400092618    steps: 225    lr: 0.0001     evaluation reward: 4.6\n",
            "episode: 1453   score: 6.0   memory length: 315863   epsilon: 0.5725892800092787    steps: 392    lr: 0.0001     evaluation reward: 4.63\n",
            "episode: 1454   score: 5.0   memory length: 316187   epsilon: 0.5719477600092926    steps: 324    lr: 0.0001     evaluation reward: 4.66\n",
            "episode: 1455   score: 7.0   memory length: 316577   epsilon: 0.5711755600093094    steps: 390    lr: 0.0001     evaluation reward: 4.69\n",
            "episode: 1456   score: 7.0   memory length: 316987   epsilon: 0.570363760009327    steps: 410    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1457   score: 5.0   memory length: 317312   epsilon: 0.569720260009341    steps: 325    lr: 0.0001     evaluation reward: 4.73\n",
            "episode: 1458   score: 1.0   memory length: 317463   epsilon: 0.5694212800093474    steps: 151    lr: 0.0001     evaluation reward: 4.7\n",
            "episode: 1459   score: 4.0   memory length: 317761   epsilon: 0.5688312400093603    steps: 298    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1460   score: 4.0   memory length: 318036   epsilon: 0.5682867400093721    steps: 275    lr: 0.0001     evaluation reward: 4.68\n",
            "episode: 1461   score: 4.0   memory length: 318331   epsilon: 0.5677026400093848    steps: 295    lr: 0.0001     evaluation reward: 4.69\n",
            "episode: 1462   score: 3.0   memory length: 318595   epsilon: 0.5671799200093961    steps: 264    lr: 0.0001     evaluation reward: 4.67\n",
            "episode: 1463   score: 4.0   memory length: 318912   epsilon: 0.5665522600094097    steps: 317    lr: 0.0001     evaluation reward: 4.68\n",
            "episode: 1464   score: 4.0   memory length: 319226   epsilon: 0.5659305400094232    steps: 314    lr: 0.0001     evaluation reward: 4.67\n",
            "episode: 1465   score: 2.0   memory length: 319423   epsilon: 0.5655404800094317    steps: 197    lr: 0.0001     evaluation reward: 4.63\n",
            "episode: 1466   score: 4.0   memory length: 319721   epsilon: 0.5649504400094445    steps: 298    lr: 0.0001     evaluation reward: 4.64\n",
            "episode: 1467   score: 3.0   memory length: 319947   epsilon: 0.5645029600094542    steps: 226    lr: 0.0001     evaluation reward: 4.63\n",
            "episode: 1468   score: 6.0   memory length: 320292   epsilon: 0.563819860009469    steps: 345    lr: 0.0001     evaluation reward: 4.66\n",
            "episode: 1469   score: 8.0   memory length: 320722   epsilon: 0.5629684600094875    steps: 430    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1470   score: 3.0   memory length: 320932   epsilon: 0.5625526600094966    steps: 210    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1471   score: 6.0   memory length: 321287   epsilon: 0.5618497600095118    steps: 355    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1472   score: 4.0   memory length: 321565   epsilon: 0.5612993200095238    steps: 278    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1473   score: 3.0   memory length: 321778   epsilon: 0.5608775800095329    steps: 213    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1474   score: 4.0   memory length: 322053   epsilon: 0.5603330800095447    steps: 275    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1475   score: 5.0   memory length: 322397   epsilon: 0.5596519600095595    steps: 344    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1476   score: 6.0   memory length: 322793   epsilon: 0.5588678800095765    steps: 396    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1477   score: 9.0   memory length: 323278   epsilon: 0.5579075800095974    steps: 485    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1478   score: 11.0   memory length: 323851   epsilon: 0.556773040009622    steps: 573    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1479   score: 4.0   memory length: 324128   epsilon: 0.5562245800096339    steps: 277    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1480   score: 5.0   memory length: 324456   epsilon: 0.555575140009648    steps: 328    lr: 0.0001     evaluation reward: 4.85\n",
            "episode: 1481   score: 4.0   memory length: 324730   epsilon: 0.5550326200096598    steps: 274    lr: 0.0001     evaluation reward: 4.83\n",
            "episode: 1482   score: 11.0   memory length: 325189   epsilon: 0.5541238000096795    steps: 459    lr: 0.0001     evaluation reward: 4.92\n",
            "episode: 1483   score: 2.0   memory length: 325370   epsilon: 0.5537654200096873    steps: 181    lr: 0.0001     evaluation reward: 4.88\n",
            "episode: 1484   score: 5.0   memory length: 325713   epsilon: 0.5530862800097021    steps: 343    lr: 0.0001     evaluation reward: 4.9\n",
            "episode: 1485   score: 2.0   memory length: 325894   epsilon: 0.5527279000097098    steps: 181    lr: 0.0001     evaluation reward: 4.87\n",
            "episode: 1486   score: 6.0   memory length: 326241   epsilon: 0.5520408400097248    steps: 347    lr: 0.0001     evaluation reward: 4.86\n",
            "episode: 1487   score: 4.0   memory length: 326483   epsilon: 0.5515616800097352    steps: 242    lr: 0.0001     evaluation reward: 4.85\n",
            "episode: 1488   score: 2.0   memory length: 326683   epsilon: 0.5511656800097438    steps: 200    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1489   score: 2.0   memory length: 326883   epsilon: 0.5507696800097523    steps: 200    lr: 0.0001     evaluation reward: 4.76\n",
            "episode: 1490   score: 4.0   memory length: 327179   epsilon: 0.5501836000097651    steps: 296    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1491   score: 4.0   memory length: 327477   epsilon: 0.5495935600097779    steps: 298    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1492   score: 11.0   memory length: 327995   epsilon: 0.5485679200098001    steps: 518    lr: 0.0001     evaluation reward: 4.8\n",
            "episode: 1493   score: 2.0   memory length: 328212   epsilon: 0.5481382600098095    steps: 217    lr: 0.0001     evaluation reward: 4.8\n",
            "episode: 1494   score: 3.0   memory length: 328456   epsilon: 0.54765514000982    steps: 244    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1495   score: 4.0   memory length: 328713   epsilon: 0.547146280009831    steps: 257    lr: 0.0001     evaluation reward: 4.73\n",
            "episode: 1496   score: 5.0   memory length: 329039   epsilon: 0.546500800009845    steps: 326    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1497   score: 6.0   memory length: 329382   epsilon: 0.5458216600098598    steps: 343    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1498   score: 7.0   memory length: 329750   epsilon: 0.5450930200098756    steps: 368    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1499   score: 2.0   memory length: 329950   epsilon: 0.5446970200098842    steps: 200    lr: 0.0001     evaluation reward: 4.76\n",
            "episode: 1500   score: 4.0   memory length: 330211   epsilon: 0.5441802400098954    steps: 261    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1501   score: 4.0   memory length: 330491   epsilon: 0.5436258400099074    steps: 280    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1502   score: 5.0   memory length: 330800   epsilon: 0.5430140200099207    steps: 309    lr: 0.0001     evaluation reward: 4.73\n",
            "episode: 1503   score: 8.0   memory length: 331221   epsilon: 0.5421804400099388    steps: 421    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1504   score: 4.0   memory length: 331462   epsilon: 0.5417032600099492    steps: 241    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1505   score: 4.0   memory length: 331736   epsilon: 0.541160740009961    steps: 274    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1506   score: 3.0   memory length: 332001   epsilon: 0.5406360400099723    steps: 265    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1507   score: 1.0   memory length: 332152   epsilon: 0.5403370600099788    steps: 151    lr: 0.0001     evaluation reward: 4.69\n",
            "episode: 1508   score: 4.0   memory length: 332409   epsilon: 0.5398282000099899    steps: 257    lr: 0.0001     evaluation reward: 4.64\n",
            "episode: 1509   score: 4.0   memory length: 332652   epsilon: 0.5393470600100003    steps: 243    lr: 0.0001     evaluation reward: 4.61\n",
            "episode: 1510   score: 11.0   memory length: 333054   epsilon: 0.5385511000100176    steps: 402    lr: 0.0001     evaluation reward: 4.67\n",
            "episode: 1511   score: 4.0   memory length: 333331   epsilon: 0.5380026400100295    steps: 277    lr: 0.0001     evaluation reward: 4.67\n",
            "episode: 1512   score: 4.0   memory length: 333588   epsilon: 0.5374937800100406    steps: 257    lr: 0.0001     evaluation reward: 4.68\n",
            "episode: 1513   score: 6.0   memory length: 333940   epsilon: 0.5367968200100557    steps: 352    lr: 0.0001     evaluation reward: 4.7\n",
            "episode: 1514   score: 5.0   memory length: 334265   epsilon: 0.5361533200100697    steps: 325    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1515   score: 6.0   memory length: 334675   epsilon: 0.5353415200100873    steps: 410    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1516   score: 4.0   memory length: 334930   epsilon: 0.5348366200100982    steps: 255    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1517   score: 3.0   memory length: 335159   epsilon: 0.5343832000101081    steps: 229    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1518   score: 5.0   memory length: 335469   epsilon: 0.5337694000101214    steps: 310    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1519   score: 3.0   memory length: 335697   epsilon: 0.5333179600101312    steps: 228    lr: 0.0001     evaluation reward: 4.68\n",
            "episode: 1520   score: 5.0   memory length: 336041   epsilon: 0.532636840010146    steps: 344    lr: 0.0001     evaluation reward: 4.7\n",
            "episode: 1521   score: 5.0   memory length: 336346   epsilon: 0.5320329400101591    steps: 305    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1522   score: 9.0   memory length: 336822   epsilon: 0.5310904600101796    steps: 476    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1523   score: 3.0   memory length: 337068   epsilon: 0.5306033800101901    steps: 246    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1524   score: 4.0   memory length: 337342   epsilon: 0.5300608600102019    steps: 274    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1525   score: 5.0   memory length: 337639   epsilon: 0.5294728000102147    steps: 297    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1526   score: 6.0   memory length: 338033   epsilon: 0.5286926800102316    steps: 394    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1527   score: 7.0   memory length: 338455   epsilon: 0.5278571200102498    steps: 422    lr: 0.0001     evaluation reward: 4.85\n",
            "episode: 1528   score: 3.0   memory length: 338680   epsilon: 0.5274116200102594    steps: 225    lr: 0.0001     evaluation reward: 4.84\n",
            "episode: 1529   score: 4.0   memory length: 338959   epsilon: 0.5268592000102714    steps: 279    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1530   score: 5.0   memory length: 339267   epsilon: 0.5262493600102847    steps: 308    lr: 0.0001     evaluation reward: 4.8\n",
            "episode: 1531   score: 5.0   memory length: 339571   epsilon: 0.5256474400102977    steps: 304    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1532   score: 3.0   memory length: 339818   epsilon: 0.5251583800103083    steps: 247    lr: 0.0001     evaluation reward: 4.82\n",
            "episode: 1533   score: 8.0   memory length: 340234   epsilon: 0.5243347000103262    steps: 416    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1534   score: 6.0   memory length: 340604   epsilon: 0.5236021000103421    steps: 370    lr: 0.0001     evaluation reward: 4.83\n",
            "episode: 1535   score: 6.0   memory length: 340975   epsilon: 0.5228675200103581    steps: 371    lr: 0.0001     evaluation reward: 4.84\n",
            "episode: 1536   score: 4.0   memory length: 341252   epsilon: 0.52231906001037    steps: 277    lr: 0.0001     evaluation reward: 4.83\n",
            "episode: 1537   score: 5.0   memory length: 341577   epsilon: 0.521675560010384    steps: 325    lr: 0.0001     evaluation reward: 4.83\n",
            "episode: 1538   score: 7.0   memory length: 341975   epsilon: 0.5208875200104011    steps: 398    lr: 0.0001     evaluation reward: 4.8\n",
            "episode: 1539   score: 4.0   memory length: 342254   epsilon: 0.520335100010413    steps: 279    lr: 0.0001     evaluation reward: 4.76\n",
            "episode: 1540   score: 7.0   memory length: 342662   epsilon: 0.5195272600104306    steps: 408    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1541   score: 5.0   memory length: 342987   epsilon: 0.5188837600104446    steps: 325    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1542   score: 4.0   memory length: 343282   epsilon: 0.5182996600104572    steps: 295    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1543   score: 6.0   memory length: 343674   epsilon: 0.5175235000104741    steps: 392    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1544   score: 4.0   memory length: 343970   epsilon: 0.5169374200104868    steps: 296    lr: 0.0001     evaluation reward: 4.76\n",
            "episode: 1545   score: 7.0   memory length: 344362   epsilon: 0.5161612600105037    steps: 392    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1546   score: 6.0   memory length: 344735   epsilon: 0.5154227200105197    steps: 373    lr: 0.0001     evaluation reward: 4.83\n",
            "episode: 1547   score: 4.0   memory length: 344995   epsilon: 0.5149079200105309    steps: 260    lr: 0.0001     evaluation reward: 4.81\n",
            "episode: 1548   score: 2.0   memory length: 345176   epsilon: 0.5145495400105387    steps: 181    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1549   score: 2.0   memory length: 345375   epsilon: 0.5141555200105472    steps: 199    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1550   score: 9.0   memory length: 345847   epsilon: 0.5132209600105675    steps: 472    lr: 0.0001     evaluation reward: 4.86\n",
            "episode: 1551   score: 6.0   memory length: 346217   epsilon: 0.5124883600105834    steps: 370    lr: 0.0001     evaluation reward: 4.84\n",
            "episode: 1552   score: 5.0   memory length: 346491   epsilon: 0.5119458400105952    steps: 274    lr: 0.0001     evaluation reward: 4.86\n",
            "episode: 1553   score: 10.0   memory length: 347023   epsilon: 0.510892480010618    steps: 532    lr: 0.0001     evaluation reward: 4.9\n",
            "episode: 1554   score: 4.0   memory length: 347316   epsilon: 0.5103123400106306    steps: 293    lr: 0.0001     evaluation reward: 4.89\n",
            "episode: 1555   score: 6.0   memory length: 347675   epsilon: 0.5096015200106461    steps: 359    lr: 0.0001     evaluation reward: 4.88\n",
            "episode: 1556   score: 9.0   memory length: 348123   epsilon: 0.5087144800106653    steps: 448    lr: 0.0001     evaluation reward: 4.9\n",
            "episode: 1557   score: 4.0   memory length: 348396   epsilon: 0.5081739400106771    steps: 273    lr: 0.0001     evaluation reward: 4.89\n",
            "episode: 1558   score: 3.0   memory length: 348608   epsilon: 0.5077541800106862    steps: 212    lr: 0.0001     evaluation reward: 4.91\n",
            "episode: 1559   score: 4.0   memory length: 348884   epsilon: 0.507207700010698    steps: 276    lr: 0.0001     evaluation reward: 4.91\n",
            "episode: 1560   score: 4.0   memory length: 349141   epsilon: 0.5066988400107091    steps: 257    lr: 0.0001     evaluation reward: 4.91\n",
            "episode: 1561   score: 3.0   memory length: 349386   epsilon: 0.5062137400107196    steps: 245    lr: 0.0001     evaluation reward: 4.9\n",
            "episode: 1562   score: 7.0   memory length: 349745   epsilon: 0.505502920010735    steps: 359    lr: 0.0001     evaluation reward: 4.94\n",
            "episode: 1563   score: 8.0   memory length: 350179   epsilon: 0.5046436000107537    steps: 434    lr: 0.0001     evaluation reward: 4.98\n",
            "episode: 1564   score: 6.0   memory length: 350532   epsilon: 0.5039446600107689    steps: 353    lr: 0.0001     evaluation reward: 5.0\n",
            "episode: 1565   score: 4.0   memory length: 350798   epsilon: 0.5034179800107803    steps: 266    lr: 0.0001     evaluation reward: 5.02\n",
            "episode: 1566   score: 5.0   memory length: 351103   epsilon: 0.5028140800107934    steps: 305    lr: 0.0001     evaluation reward: 5.03\n",
            "episode: 1567   score: 8.0   memory length: 351507   epsilon: 0.5020141600108108    steps: 404    lr: 0.0001     evaluation reward: 5.08\n",
            "episode: 1568   score: 7.0   memory length: 351869   epsilon: 0.5012974000108263    steps: 362    lr: 0.0001     evaluation reward: 5.09\n",
            "episode: 1569   score: 5.0   memory length: 352213   epsilon: 0.5006162800108411    steps: 344    lr: 0.0001     evaluation reward: 5.06\n",
            "episode: 1570   score: 4.0   memory length: 352506   epsilon: 0.5000361400108537    steps: 293    lr: 0.0001     evaluation reward: 5.07\n",
            "episode: 1571   score: 5.0   memory length: 352795   epsilon: 0.4994639200108511    steps: 289    lr: 0.0001     evaluation reward: 5.06\n",
            "episode: 1572   score: 10.0   memory length: 353284   epsilon: 0.498495700010845    steps: 489    lr: 0.0001     evaluation reward: 5.12\n",
            "episode: 1573   score: 3.0   memory length: 353514   epsilon: 0.4980403000108421    steps: 230    lr: 0.0001     evaluation reward: 5.12\n",
            "episode: 1574   score: 8.0   memory length: 353910   epsilon: 0.49725622001083714    steps: 396    lr: 0.0001     evaluation reward: 5.16\n",
            "episode: 1575   score: 5.0   memory length: 354237   epsilon: 0.49660876001083304    steps: 327    lr: 0.0001     evaluation reward: 5.16\n",
            "episode: 1576   score: 5.0   memory length: 354532   epsilon: 0.49602466001082934    steps: 295    lr: 0.0001     evaluation reward: 5.15\n",
            "episode: 1577   score: 6.0   memory length: 354875   epsilon: 0.49534552001082505    steps: 343    lr: 0.0001     evaluation reward: 5.12\n",
            "episode: 1578   score: 6.0   memory length: 355234   epsilon: 0.49463470001082055    steps: 359    lr: 0.0001     evaluation reward: 5.07\n",
            "episode: 1579   score: 5.0   memory length: 355560   epsilon: 0.49398922001081647    steps: 326    lr: 0.0001     evaluation reward: 5.08\n",
            "episode: 1580   score: 4.0   memory length: 355855   epsilon: 0.49340512001081277    steps: 295    lr: 0.0001     evaluation reward: 5.07\n",
            "episode: 1581   score: 6.0   memory length: 356189   epsilon: 0.4927438000108086    steps: 334    lr: 0.0001     evaluation reward: 5.09\n",
            "episode: 1582   score: 5.0   memory length: 356512   epsilon: 0.49210426001080454    steps: 323    lr: 0.0001     evaluation reward: 5.03\n",
            "episode: 1583   score: 6.0   memory length: 356851   epsilon: 0.4914330400108003    steps: 339    lr: 0.0001     evaluation reward: 5.07\n",
            "episode: 1584   score: 6.0   memory length: 357225   epsilon: 0.4906925200107956    steps: 374    lr: 0.0001     evaluation reward: 5.08\n",
            "episode: 1585   score: 6.0   memory length: 357568   epsilon: 0.4900133800107913    steps: 343    lr: 0.0001     evaluation reward: 5.12\n",
            "episode: 1586   score: 4.0   memory length: 357884   epsilon: 0.48938770001078735    steps: 316    lr: 0.0001     evaluation reward: 5.1\n",
            "episode: 1587   score: 9.0   memory length: 358377   epsilon: 0.4884115600107812    steps: 493    lr: 0.0001     evaluation reward: 5.15\n",
            "episode: 1588   score: 8.0   memory length: 358753   epsilon: 0.48766708001077647    steps: 376    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1589   score: 5.0   memory length: 359078   epsilon: 0.4870235800107724    steps: 325    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1590   score: 5.0   memory length: 359349   epsilon: 0.486487000010769    steps: 271    lr: 0.0001     evaluation reward: 5.25\n",
            "episode: 1591   score: 3.0   memory length: 359579   epsilon: 0.4860316000107661    steps: 230    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1592   score: 5.0   memory length: 359921   epsilon: 0.48535444001076183    steps: 342    lr: 0.0001     evaluation reward: 5.18\n",
            "episode: 1593   score: 5.0   memory length: 360245   epsilon: 0.4847129200107578    steps: 324    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1594   score: 5.0   memory length: 360532   epsilon: 0.4841446600107542    steps: 287    lr: 0.0001     evaluation reward: 5.23\n",
            "episode: 1595   score: 5.0   memory length: 360859   epsilon: 0.4834972000107501    steps: 327    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1596   score: 5.0   memory length: 361163   epsilon: 0.4828952800107463    steps: 304    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1597   score: 6.0   memory length: 361482   epsilon: 0.4822636600107423    steps: 319    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1598   score: 7.0   memory length: 361857   epsilon: 0.4815211600107376    steps: 375    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1599   score: 5.0   memory length: 362199   epsilon: 0.4808440000107333    steps: 342    lr: 0.0001     evaluation reward: 5.27\n",
            "episode: 1600   score: 4.0   memory length: 362457   epsilon: 0.48033316001073006    steps: 258    lr: 0.0001     evaluation reward: 5.27\n",
            "episode: 1601   score: 5.0   memory length: 362781   epsilon: 0.479691640010726    steps: 324    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1602   score: 4.0   memory length: 363058   epsilon: 0.47914318001072254    steps: 277    lr: 0.0001     evaluation reward: 5.27\n",
            "episode: 1603   score: 10.0   memory length: 363549   epsilon: 0.4781710000107164    steps: 491    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1604   score: 4.0   memory length: 363825   epsilon: 0.4776245200107129    steps: 276    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1605   score: 3.0   memory length: 364072   epsilon: 0.47713546001070983    steps: 247    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1606   score: 4.0   memory length: 364329   epsilon: 0.4766266000107066    steps: 257    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1607   score: 5.0   memory length: 364603   epsilon: 0.4760840800107032    steps: 274    lr: 0.0001     evaluation reward: 5.33\n",
            "episode: 1608   score: 11.0   memory length: 365126   epsilon: 0.47504854001069663    steps: 523    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1609   score: 5.0   memory length: 365416   epsilon: 0.474474340010693    steps: 290    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1610   score: 5.0   memory length: 365725   epsilon: 0.4738625200106891    steps: 309    lr: 0.0001     evaluation reward: 5.35\n",
            "episode: 1611   score: 5.0   memory length: 366053   epsilon: 0.473213080010685    steps: 328    lr: 0.0001     evaluation reward: 5.36\n",
            "episode: 1612   score: 6.0   memory length: 366417   epsilon: 0.47249236001068046    steps: 364    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1613   score: 5.0   memory length: 366721   epsilon: 0.47189044001067665    steps: 304    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1614   score: 6.0   memory length: 367059   epsilon: 0.4712212000106724    steps: 338    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1615   score: 5.0   memory length: 367350   epsilon: 0.47064502001066877    steps: 291    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1616   score: 6.0   memory length: 367692   epsilon: 0.4699678600106645    steps: 342    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1617   score: 4.0   memory length: 367966   epsilon: 0.46942534001066105    steps: 274    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1618   score: 5.0   memory length: 368269   epsilon: 0.46882540001065726    steps: 303    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1619   score: 4.0   memory length: 368525   epsilon: 0.46831852001065405    steps: 256    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1620   score: 6.0   memory length: 368889   epsilon: 0.4675978000106495    steps: 364    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1621   score: 7.0   memory length: 369271   epsilon: 0.4668414400106447    steps: 382    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1622   score: 5.0   memory length: 369557   epsilon: 0.4662751600106411    steps: 286    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1623   score: 6.0   memory length: 369882   epsilon: 0.46563166001063705    steps: 325    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1624   score: 6.0   memory length: 370219   epsilon: 0.46496440001063283    steps: 337    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1625   score: 4.0   memory length: 370477   epsilon: 0.4644535600106296    steps: 258    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1626   score: 4.0   memory length: 370771   epsilon: 0.4638714400106259    steps: 294    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1627   score: 4.0   memory length: 371050   epsilon: 0.4633190200106224    steps: 279    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1628   score: 7.0   memory length: 371472   epsilon: 0.46248346001061713    steps: 422    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1629   score: 7.0   memory length: 371843   epsilon: 0.4617488800106125    steps: 371    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1630   score: 7.0   memory length: 372233   epsilon: 0.4609766800106076    steps: 390    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1631   score: 5.0   memory length: 372580   epsilon: 0.46028962001060325    steps: 347    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1632   score: 5.0   memory length: 372870   epsilon: 0.4597154200105996    steps: 290    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1633   score: 6.0   memory length: 373222   epsilon: 0.4590184600105952    steps: 352    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1634   score: 3.0   memory length: 373453   epsilon: 0.4585610800105923    steps: 231    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1635   score: 6.0   memory length: 373788   epsilon: 0.4578977800105881    steps: 335    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1636   score: 5.0   memory length: 374100   epsilon: 0.4572800200105842    steps: 312    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1637   score: 4.0   memory length: 374354   epsilon: 0.456777100010581    steps: 254    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1638   score: 4.0   memory length: 374646   epsilon: 0.45619894001057737    steps: 292    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1639   score: 5.0   memory length: 374948   epsilon: 0.4556009800105736    steps: 302    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1640   score: 7.0   memory length: 375321   epsilon: 0.4548624400105689    steps: 373    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1641   score: 7.0   memory length: 375700   epsilon: 0.45411202001056417    steps: 379    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1642   score: 3.0   memory length: 375930   epsilon: 0.4536566200105613    steps: 230    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1643   score: 5.0   memory length: 376253   epsilon: 0.45301708001055724    steps: 323    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1644   score: 5.0   memory length: 376598   epsilon: 0.4523339800105529    steps: 345    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1645   score: 8.0   memory length: 377052   epsilon: 0.45143506001054723    steps: 454    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1646   score: 6.0   memory length: 377399   epsilon: 0.4507480000105429    steps: 347    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1647   score: 18.0   memory length: 378023   epsilon: 0.44951248001053506    steps: 624    lr: 0.0001     evaluation reward: 5.59\n",
            "episode: 1648   score: 5.0   memory length: 378308   epsilon: 0.4489481800105315    steps: 285    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1649   score: 7.0   memory length: 378709   epsilon: 0.44815420001052647    steps: 401    lr: 0.0001     evaluation reward: 5.67\n",
            "episode: 1650   score: 3.0   memory length: 378919   epsilon: 0.44773840001052384    steps: 210    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1651   score: 5.0   memory length: 379226   epsilon: 0.44713054001052    steps: 307    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1652   score: 4.0   memory length: 379524   epsilon: 0.44654050001051626    steps: 298    lr: 0.0001     evaluation reward: 5.59\n",
            "episode: 1653   score: 1.0   memory length: 379675   epsilon: 0.44624152001051437    steps: 151    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1654   score: 7.0   memory length: 380050   epsilon: 0.44549902001050967    steps: 375    lr: 0.0001     evaluation reward: 5.53\n",
            "episode: 1655   score: 4.0   memory length: 380331   epsilon: 0.44494264001050615    steps: 281    lr: 0.0001     evaluation reward: 5.51\n",
            "episode: 1656   score: 6.0   memory length: 380665   epsilon: 0.44428132001050197    steps: 334    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1657   score: 3.0   memory length: 380876   epsilon: 0.4438635400104993    steps: 211    lr: 0.0001     evaluation reward: 5.47\n",
            "episode: 1658   score: 4.0   memory length: 381118   epsilon: 0.4433843800104963    steps: 242    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1659   score: 5.0   memory length: 381434   epsilon: 0.44275870001049233    steps: 316    lr: 0.0001     evaluation reward: 5.49\n",
            "episode: 1660   score: 5.0   memory length: 381758   epsilon: 0.4421171800104883    steps: 324    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1661   score: 5.0   memory length: 382053   epsilon: 0.4415330800104846    steps: 295    lr: 0.0001     evaluation reward: 5.52\n",
            "episode: 1662   score: 8.0   memory length: 382459   epsilon: 0.4407292000104795    steps: 406    lr: 0.0001     evaluation reward: 5.53\n",
            "episode: 1663   score: 4.0   memory length: 382721   epsilon: 0.4402104400104762    steps: 262    lr: 0.0001     evaluation reward: 5.49\n",
            "episode: 1664   score: 4.0   memory length: 383033   epsilon: 0.4395926800104723    steps: 312    lr: 0.0001     evaluation reward: 5.47\n",
            "episode: 1665   score: 7.0   memory length: 383444   epsilon: 0.43877890001046715    steps: 411    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1666   score: 5.0   memory length: 383790   epsilon: 0.4380938200104628    steps: 346    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1667   score: 4.0   memory length: 384047   epsilon: 0.4375849600104596    steps: 257    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1668   score: 5.0   memory length: 384350   epsilon: 0.4369850200104558    steps: 303    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1669   score: 1.0   memory length: 384519   epsilon: 0.4366504000104537    steps: 169    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1670   score: 5.0   memory length: 384858   epsilon: 0.43597918001044944    steps: 339    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1671   score: 6.0   memory length: 385205   epsilon: 0.4352921200104451    steps: 347    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1672   score: 7.0   memory length: 385599   epsilon: 0.43451200001044016    steps: 394    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1673   score: 4.0   memory length: 385879   epsilon: 0.43395760001043665    steps: 280    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1674   score: 4.0   memory length: 386157   epsilon: 0.43340716001043317    steps: 278    lr: 0.0001     evaluation reward: 5.36\n",
            "episode: 1675   score: 5.0   memory length: 386466   epsilon: 0.4327953400104293    steps: 309    lr: 0.0001     evaluation reward: 5.36\n",
            "episode: 1676   score: 3.0   memory length: 386678   epsilon: 0.43237558001042664    steps: 212    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1677   score: 4.0   memory length: 386938   epsilon: 0.4318607800104234    steps: 260    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1678   score: 4.0   memory length: 387233   epsilon: 0.4312766800104197    steps: 295    lr: 0.0001     evaluation reward: 5.3\n",
            "episode: 1679   score: 4.0   memory length: 387498   epsilon: 0.43075198001041637    steps: 265    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1680   score: 3.0   memory length: 387709   epsilon: 0.4303342000104137    steps: 211    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1681   score: 4.0   memory length: 387951   epsilon: 0.4298550400104107    steps: 242    lr: 0.0001     evaluation reward: 5.26\n",
            "episode: 1682   score: 4.0   memory length: 388246   epsilon: 0.429270940010407    steps: 295    lr: 0.0001     evaluation reward: 5.25\n",
            "episode: 1683   score: 5.0   memory length: 388611   epsilon: 0.4285482400104024    steps: 365    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1684   score: 5.0   memory length: 388940   epsilon: 0.4278968200103983    steps: 329    lr: 0.0001     evaluation reward: 5.23\n",
            "episode: 1685   score: 7.0   memory length: 389346   epsilon: 0.4270929400103932    steps: 406    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1686   score: 3.0   memory length: 389556   epsilon: 0.4266771400103906    steps: 210    lr: 0.0001     evaluation reward: 5.23\n",
            "episode: 1687   score: 4.0   memory length: 389839   epsilon: 0.42611680001038704    steps: 283    lr: 0.0001     evaluation reward: 5.18\n",
            "episode: 1688   score: 5.0   memory length: 390112   epsilon: 0.4255762600103836    steps: 273    lr: 0.0001     evaluation reward: 5.15\n",
            "episode: 1689   score: 7.0   memory length: 390357   epsilon: 0.42509116001038055    steps: 245    lr: 0.0001     evaluation reward: 5.17\n",
            "episode: 1690   score: 4.0   memory length: 390641   epsilon: 0.424528840010377    steps: 284    lr: 0.0001     evaluation reward: 5.16\n",
            "episode: 1691   score: 10.0   memory length: 391160   epsilon: 0.4235012200103705    steps: 519    lr: 0.0001     evaluation reward: 5.23\n",
            "episode: 1692   score: 7.0   memory length: 391580   epsilon: 0.42266962001036523    steps: 420    lr: 0.0001     evaluation reward: 5.25\n",
            "episode: 1693   score: 9.0   memory length: 392071   epsilon: 0.4216974400103591    steps: 491    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1694   score: 7.0   memory length: 392450   epsilon: 0.42094702001035433    steps: 379    lr: 0.0001     evaluation reward: 5.31\n",
            "episode: 1695   score: 3.0   memory length: 392678   epsilon: 0.4204955800103515    steps: 228    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1696   score: 7.0   memory length: 393069   epsilon: 0.4197214000103466    steps: 391    lr: 0.0001     evaluation reward: 5.31\n",
            "episode: 1697   score: 4.0   memory length: 393329   epsilon: 0.4192066000103433    steps: 260    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1698   score: 6.0   memory length: 393693   epsilon: 0.41848588001033876    steps: 364    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1699   score: 3.0   memory length: 393958   epsilon: 0.41796118001033544    steps: 265    lr: 0.0001     evaluation reward: 5.26\n",
            "episode: 1700   score: 6.0   memory length: 394287   epsilon: 0.4173097600103313    steps: 329    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1701   score: 6.0   memory length: 394665   epsilon: 0.4165613200103266    steps: 378    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1702   score: 6.0   memory length: 395029   epsilon: 0.415840600010322    steps: 364    lr: 0.0001     evaluation reward: 5.31\n",
            "episode: 1703   score: 8.0   memory length: 395482   epsilon: 0.41494366001031635    steps: 453    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1704   score: 9.0   memory length: 395936   epsilon: 0.41404474001031066    steps: 454    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1705   score: 3.0   memory length: 396145   epsilon: 0.41363092001030805    steps: 209    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1706   score: 5.0   memory length: 396469   epsilon: 0.412989400010304    steps: 324    lr: 0.0001     evaluation reward: 5.35\n",
            "episode: 1707   score: 8.0   memory length: 396943   epsilon: 0.41205088001029805    steps: 474    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1708   score: 6.0   memory length: 397274   epsilon: 0.4113955000102939    steps: 331    lr: 0.0001     evaluation reward: 5.33\n",
            "episode: 1709   score: 4.0   memory length: 397550   epsilon: 0.41084902001029044    steps: 276    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1710   score: 10.0   memory length: 398063   epsilon: 0.409833280010284    steps: 513    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1711   score: 5.0   memory length: 398390   epsilon: 0.4091858200102799    steps: 327    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1712   score: 6.0   memory length: 398767   epsilon: 0.4084393600102752    steps: 377    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1713   score: 6.0   memory length: 399110   epsilon: 0.4077602200102709    steps: 343    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1714   score: 8.0   memory length: 399584   epsilon: 0.40682170001026496    steps: 474    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1715   score: 6.0   memory length: 399915   epsilon: 0.4061663200102608    steps: 331    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1716   score: 4.0   memory length: 400192   epsilon: 0.40561786001025735    steps: 277    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1717   score: 11.0   memory length: 400589   epsilon: 0.4048318000102524    steps: 397    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1718   score: 4.0   memory length: 400844   epsilon: 0.4043269000102492    steps: 255    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1719   score: 4.0   memory length: 401101   epsilon: 0.40381804001024596    steps: 257    lr: 0.0001     evaluation reward: 5.45\n",
            "episode: 1720   score: 12.0   memory length: 401737   epsilon: 0.402558760010238    steps: 636    lr: 0.0001     evaluation reward: 5.51\n",
            "episode: 1721   score: 8.0   memory length: 402144   epsilon: 0.4017529000102329    steps: 407    lr: 0.0001     evaluation reward: 5.52\n",
            "episode: 1722   score: 3.0   memory length: 402371   epsilon: 0.40130344001023005    steps: 227    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1723   score: 6.0   memory length: 402721   epsilon: 0.40061044001022567    steps: 350    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1724   score: 7.0   memory length: 403077   epsilon: 0.3999055600102212    steps: 356    lr: 0.0001     evaluation reward: 5.51\n",
            "episode: 1725   score: 4.0   memory length: 403319   epsilon: 0.3994264000102182    steps: 242    lr: 0.0001     evaluation reward: 5.51\n",
            "episode: 1726   score: 6.0   memory length: 403644   epsilon: 0.3987829000102141    steps: 325    lr: 0.0001     evaluation reward: 5.53\n",
            "episode: 1727   score: 11.0   memory length: 404202   epsilon: 0.3976780600102071    steps: 558    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1728   score: 7.0   memory length: 404623   epsilon: 0.39684448001020184    steps: 421    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1729   score: 1.0   memory length: 404794   epsilon: 0.3965059000101997    steps: 171    lr: 0.0001     evaluation reward: 5.54\n",
            "episode: 1730   score: 7.0   memory length: 405212   epsilon: 0.39567826001019446    steps: 418    lr: 0.0001     evaluation reward: 5.54\n",
            "episode: 1731   score: 3.0   memory length: 405424   epsilon: 0.3952585000101918    steps: 212    lr: 0.0001     evaluation reward: 5.52\n",
            "episode: 1732   score: 10.0   memory length: 405900   epsilon: 0.39431602001018584    steps: 476    lr: 0.0001     evaluation reward: 5.57\n",
            "episode: 1733   score: 12.0   memory length: 406339   epsilon: 0.39344680001018034    steps: 439    lr: 0.0001     evaluation reward: 5.63\n",
            "episode: 1734   score: 4.0   memory length: 406616   epsilon: 0.39289834001017687    steps: 277    lr: 0.0001     evaluation reward: 5.64\n",
            "episode: 1735   score: 8.0   memory length: 407017   epsilon: 0.39210436001017185    steps: 401    lr: 0.0001     evaluation reward: 5.66\n",
            "episode: 1736   score: 9.0   memory length: 407506   epsilon: 0.3911361400101657    steps: 489    lr: 0.0001     evaluation reward: 5.7\n",
            "episode: 1737   score: 3.0   memory length: 407718   epsilon: 0.39071638001016307    steps: 212    lr: 0.0001     evaluation reward: 5.69\n",
            "episode: 1738   score: 6.0   memory length: 408061   epsilon: 0.39003724001015877    steps: 343    lr: 0.0001     evaluation reward: 5.71\n",
            "episode: 1739   score: 8.0   memory length: 408475   epsilon: 0.3892175200101536    steps: 414    lr: 0.0001     evaluation reward: 5.74\n",
            "episode: 1740   score: 2.0   memory length: 408673   epsilon: 0.3888254800101511    steps: 198    lr: 0.0001     evaluation reward: 5.69\n",
            "episode: 1741   score: 6.0   memory length: 409044   epsilon: 0.38809090001014646    steps: 371    lr: 0.0001     evaluation reward: 5.68\n",
            "episode: 1742   score: 10.0   memory length: 409489   epsilon: 0.3872098000101409    steps: 445    lr: 0.0001     evaluation reward: 5.75\n",
            "episode: 1743   score: 3.0   memory length: 409719   epsilon: 0.386754400010138    steps: 230    lr: 0.0001     evaluation reward: 5.73\n",
            "episode: 1744   score: 7.0   memory length: 410081   epsilon: 0.38603764001013346    steps: 362    lr: 0.0001     evaluation reward: 5.75\n",
            "episode: 1745   score: 8.0   memory length: 410508   epsilon: 0.3851921800101281    steps: 427    lr: 0.0001     evaluation reward: 5.75\n",
            "episode: 1746   score: 7.0   memory length: 410923   epsilon: 0.3843704800101229    steps: 415    lr: 0.0001     evaluation reward: 5.76\n",
            "episode: 1747   score: 9.0   memory length: 411359   epsilon: 0.38350720001011745    steps: 436    lr: 0.0001     evaluation reward: 5.67\n",
            "episode: 1748   score: 7.0   memory length: 411732   epsilon: 0.3827686600101128    steps: 373    lr: 0.0001     evaluation reward: 5.69\n",
            "episode: 1749   score: 6.0   memory length: 412085   epsilon: 0.38206972001010836    steps: 353    lr: 0.0001     evaluation reward: 5.68\n",
            "episode: 1750   score: 8.0   memory length: 412522   epsilon: 0.3812044600101029    steps: 437    lr: 0.0001     evaluation reward: 5.73\n",
            "episode: 1751   score: 10.0   memory length: 413072   epsilon: 0.380115460010096    steps: 550    lr: 0.0001     evaluation reward: 5.78\n",
            "episode: 1752   score: 6.0   memory length: 413407   epsilon: 0.3794521600100918    steps: 335    lr: 0.0001     evaluation reward: 5.8\n",
            "episode: 1753   score: 10.0   memory length: 413869   epsilon: 0.378537400010086    steps: 462    lr: 0.0001     evaluation reward: 5.89\n",
            "episode: 1754   score: 8.0   memory length: 414166   epsilon: 0.3779493400100823    steps: 297    lr: 0.0001     evaluation reward: 5.9\n",
            "episode: 1755   score: 15.0   memory length: 414757   epsilon: 0.3767791600100749    steps: 591    lr: 0.0001     evaluation reward: 6.01\n",
            "episode: 1756   score: 6.0   memory length: 415129   epsilon: 0.3760426000100702    steps: 372    lr: 0.0001     evaluation reward: 6.01\n",
            "episode: 1757   score: 10.0   memory length: 415614   epsilon: 0.37508230001006415    steps: 485    lr: 0.0001     evaluation reward: 6.08\n",
            "episode: 1758   score: 9.0   memory length: 416077   epsilon: 0.37416556001005835    steps: 463    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1759   score: 5.0   memory length: 416385   epsilon: 0.3735557200100545    steps: 308    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1760   score: 6.0   memory length: 416747   epsilon: 0.37283896001004996    steps: 362    lr: 0.0001     evaluation reward: 6.14\n",
            "episode: 1761   score: 6.0   memory length: 417146   epsilon: 0.37204894001004496    steps: 399    lr: 0.0001     evaluation reward: 6.15\n",
            "episode: 1762   score: 6.0   memory length: 417508   epsilon: 0.3713321800100404    steps: 362    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1763   score: 7.0   memory length: 417754   epsilon: 0.37084510001003734    steps: 246    lr: 0.0001     evaluation reward: 6.16\n",
            "episode: 1764   score: 3.0   memory length: 417980   epsilon: 0.3703976200100345    steps: 226    lr: 0.0001     evaluation reward: 6.15\n",
            "episode: 1765   score: 8.0   memory length: 418403   epsilon: 0.3695600800100292    steps: 423    lr: 0.0001     evaluation reward: 6.16\n",
            "episode: 1766   score: 5.0   memory length: 418693   epsilon: 0.3689858800100256    steps: 290    lr: 0.0001     evaluation reward: 6.16\n",
            "episode: 1767   score: 9.0   memory length: 419165   epsilon: 0.36805132001001967    steps: 472    lr: 0.0001     evaluation reward: 6.21\n",
            "episode: 1768   score: 6.0   memory length: 419500   epsilon: 0.36738802001001547    steps: 335    lr: 0.0001     evaluation reward: 6.22\n",
            "episode: 1769   score: 11.0   memory length: 420025   epsilon: 0.3663485200100089    steps: 525    lr: 0.0001     evaluation reward: 6.32\n",
            "episode: 1770   score: 6.0   memory length: 420401   epsilon: 0.3656040400100042    steps: 376    lr: 0.0001     evaluation reward: 6.33\n",
            "episode: 1771   score: 5.0   memory length: 420746   epsilon: 0.36492094000999986    steps: 345    lr: 0.0001     evaluation reward: 6.32\n",
            "episode: 1772   score: 9.0   memory length: 421216   epsilon: 0.363990340009994    steps: 470    lr: 0.0001     evaluation reward: 6.34\n",
            "episode: 1773   score: 6.0   memory length: 421559   epsilon: 0.3633112000099897    steps: 343    lr: 0.0001     evaluation reward: 6.36\n",
            "episode: 1774   score: 4.0   memory length: 421834   epsilon: 0.36276670000998623    steps: 275    lr: 0.0001     evaluation reward: 6.36\n",
            "episode: 1775   score: 7.0   memory length: 422221   epsilon: 0.3620004400099814    steps: 387    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1776   score: 7.0   memory length: 422643   epsilon: 0.3611648800099761    steps: 422    lr: 0.0001     evaluation reward: 6.42\n",
            "episode: 1777   score: 8.0   memory length: 423070   epsilon: 0.36031942000997075    steps: 427    lr: 0.0001     evaluation reward: 6.46\n",
            "episode: 1778   score: 7.0   memory length: 423461   epsilon: 0.35954524000996585    steps: 391    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1779   score: 6.0   memory length: 423855   epsilon: 0.3587651200099609    steps: 394    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1780   score: 3.0   memory length: 424064   epsilon: 0.3583513000099583    steps: 209    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1781   score: 7.0   memory length: 424483   epsilon: 0.35752168000995305    steps: 419    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1782   score: 7.0   memory length: 424867   epsilon: 0.35676136000994824    steps: 384    lr: 0.0001     evaluation reward: 6.57\n",
            "episode: 1783   score: 6.0   memory length: 425220   epsilon: 0.3560624200099438    steps: 353    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1784   score: 4.0   memory length: 425474   epsilon: 0.35555950000994063    steps: 254    lr: 0.0001     evaluation reward: 6.57\n",
            "episode: 1785   score: 4.0   memory length: 425731   epsilon: 0.3550506400099374    steps: 257    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1786   score: 9.0   memory length: 426172   epsilon: 0.3541774600099319    steps: 441    lr: 0.0001     evaluation reward: 6.6\n",
            "episode: 1787   score: 6.0   memory length: 426551   epsilon: 0.35342704000992714    steps: 379    lr: 0.0001     evaluation reward: 6.62\n",
            "episode: 1788   score: 5.0   memory length: 426857   epsilon: 0.3528211600099233    steps: 306    lr: 0.0001     evaluation reward: 6.62\n",
            "episode: 1789   score: 6.0   memory length: 427219   epsilon: 0.35210440000991877    steps: 362    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1790   score: 8.0   memory length: 427622   epsilon: 0.3513064600099137    steps: 403    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1791   score: 4.0   memory length: 427879   epsilon: 0.3507976000099105    steps: 257    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1792   score: 7.0   memory length: 428250   epsilon: 0.35006302000990586    steps: 371    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1793   score: 4.0   memory length: 428507   epsilon: 0.34955416000990264    steps: 257    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1794   score: 11.0   memory length: 429001   epsilon: 0.34857604000989645    steps: 494    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1795   score: 4.0   memory length: 429296   epsilon: 0.34799194000989275    steps: 295    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1796   score: 6.0   memory length: 429633   epsilon: 0.34732468000988853    steps: 337    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1797   score: 9.0   memory length: 430084   epsilon: 0.3464317000098829    steps: 451    lr: 0.0001     evaluation reward: 6.63\n",
            "episode: 1798   score: 4.0   memory length: 430359   epsilon: 0.34588720000987944    steps: 275    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1799   score: 13.0   memory length: 430976   epsilon: 0.3446655400098717    steps: 617    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1800   score: 7.0   memory length: 431396   epsilon: 0.34383394000986645    steps: 420    lr: 0.0001     evaluation reward: 6.72\n",
            "episode: 1801   score: 4.0   memory length: 431671   epsilon: 0.343289440009863    steps: 275    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1802   score: 6.0   memory length: 432035   epsilon: 0.34256872000985844    steps: 364    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1803   score: 9.0   memory length: 432519   epsilon: 0.3416104000098524    steps: 484    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1804   score: 4.0   memory length: 432760   epsilon: 0.34113322000984936    steps: 241    lr: 0.0001     evaluation reward: 6.66\n",
            "episode: 1805   score: 8.0   memory length: 433196   epsilon: 0.3402699400098439    steps: 436    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1806   score: 5.0   memory length: 433540   epsilon: 0.3395888200098396    steps: 344    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1807   score: 11.0   memory length: 434009   epsilon: 0.3386602000098337    steps: 469    lr: 0.0001     evaluation reward: 6.74\n",
            "episode: 1808   score: 4.0   memory length: 434270   epsilon: 0.33814342000983044    steps: 261    lr: 0.0001     evaluation reward: 6.72\n",
            "episode: 1809   score: 8.0   memory length: 434714   epsilon: 0.3372643000098249    steps: 444    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1810   score: 8.0   memory length: 435142   epsilon: 0.3364168600098195    steps: 428    lr: 0.0001     evaluation reward: 6.74\n",
            "episode: 1811   score: 9.0   memory length: 435595   epsilon: 0.33551992000981384    steps: 453    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1812   score: 6.0   memory length: 435933   epsilon: 0.3348506800098096    steps: 338    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1813   score: 3.0   memory length: 436161   epsilon: 0.33439924000980675    steps: 228    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1814   score: 6.0   memory length: 436541   epsilon: 0.333646840009802    steps: 380    lr: 0.0001     evaluation reward: 6.73\n",
            "episode: 1815   score: 9.0   memory length: 437017   epsilon: 0.33270436000979603    steps: 476    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1816   score: 9.0   memory length: 437502   epsilon: 0.33174406000978995    steps: 485    lr: 0.0001     evaluation reward: 6.81\n",
            "episode: 1817   score: 8.0   memory length: 437972   epsilon: 0.33081346000978407    steps: 470    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1818   score: 12.0   memory length: 438545   epsilon: 0.3296789200097769    steps: 573    lr: 0.0001     evaluation reward: 6.86\n",
            "episode: 1819   score: 5.0   memory length: 438853   epsilon: 0.32906908000977303    steps: 308    lr: 0.0001     evaluation reward: 6.87\n",
            "episode: 1820   score: 6.0   memory length: 439188   epsilon: 0.32840578000976883    steps: 335    lr: 0.0001     evaluation reward: 6.81\n",
            "episode: 1821   score: 5.0   memory length: 439512   epsilon: 0.3277642600097648    steps: 324    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1822   score: 8.0   memory length: 439964   epsilon: 0.3268693000097591    steps: 452    lr: 0.0001     evaluation reward: 6.83\n",
            "episode: 1823   score: 7.0   memory length: 440325   epsilon: 0.3261545200097546    steps: 361    lr: 0.0001     evaluation reward: 6.84\n",
            "episode: 1824   score: 6.0   memory length: 440667   epsilon: 0.3254773600097503    steps: 342    lr: 0.0001     evaluation reward: 6.83\n",
            "episode: 1825   score: 7.0   memory length: 441071   epsilon: 0.32467744000974524    steps: 404    lr: 0.0001     evaluation reward: 6.86\n",
            "episode: 1826   score: 8.0   memory length: 441494   epsilon: 0.32383990000973994    steps: 423    lr: 0.0001     evaluation reward: 6.88\n",
            "episode: 1827   score: 7.0   memory length: 441868   epsilon: 0.32309938000973526    steps: 374    lr: 0.0001     evaluation reward: 6.84\n",
            "episode: 1828   score: 6.0   memory length: 442227   epsilon: 0.32238856000973076    steps: 359    lr: 0.0001     evaluation reward: 6.83\n",
            "episode: 1829   score: 7.0   memory length: 442601   epsilon: 0.3216480400097261    steps: 374    lr: 0.0001     evaluation reward: 6.89\n",
            "episode: 1830   score: 8.0   memory length: 443018   epsilon: 0.32082238000972085    steps: 417    lr: 0.0001     evaluation reward: 6.9\n",
            "episode: 1831   score: 5.0   memory length: 443341   epsilon: 0.3201828400097168    steps: 323    lr: 0.0001     evaluation reward: 6.92\n",
            "episode: 1832   score: 4.0   memory length: 443615   epsilon: 0.3196403200097134    steps: 274    lr: 0.0001     evaluation reward: 6.86\n",
            "episode: 1833   score: 4.0   memory length: 443877   epsilon: 0.3191215600097101    steps: 262    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1834   score: 5.0   memory length: 444185   epsilon: 0.31851172000970623    steps: 308    lr: 0.0001     evaluation reward: 6.79\n",
            "episode: 1835   score: 4.0   memory length: 444443   epsilon: 0.318000880009703    steps: 258    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1836   score: 4.0   memory length: 444720   epsilon: 0.31745242000969953    steps: 277    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1837   score: 4.0   memory length: 444978   epsilon: 0.3169415800096963    steps: 258    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1838   score: 5.0   memory length: 445268   epsilon: 0.31636738000969267    steps: 290    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1839   score: 5.0   memory length: 445580   epsilon: 0.31574962000968876    steps: 312    lr: 0.0001     evaluation reward: 6.67\n",
            "episode: 1840   score: 12.0   memory length: 446155   epsilon: 0.31461112000968156    steps: 575    lr: 0.0001     evaluation reward: 6.77\n",
            "episode: 1841   score: 4.0   memory length: 446431   epsilon: 0.3140646400096781    steps: 276    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1842   score: 7.0   memory length: 446805   epsilon: 0.3133241200096734    steps: 374    lr: 0.0001     evaluation reward: 6.72\n",
            "episode: 1843   score: 9.0   memory length: 447261   epsilon: 0.3124212400096677    steps: 456    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1844   score: 7.0   memory length: 447608   epsilon: 0.31173418000966335    steps: 347    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1845   score: 6.0   memory length: 447981   epsilon: 0.3109956400096587    steps: 373    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1846   score: 7.0   memory length: 448381   epsilon: 0.31020364000965367    steps: 400    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1847   score: 5.0   memory length: 448678   epsilon: 0.30961558000964995    steps: 297    lr: 0.0001     evaluation reward: 6.72\n",
            "episode: 1848   score: 5.0   memory length: 448986   epsilon: 0.3090057400096461    steps: 308    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1849   score: 6.0   memory length: 449348   epsilon: 0.30828898000964156    steps: 362    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1850   score: 6.0   memory length: 449713   epsilon: 0.307566280009637    steps: 365    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1851   score: 3.0   memory length: 449924   epsilon: 0.30714850000963434    steps: 211    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1852   score: 7.0   memory length: 450281   epsilon: 0.30644164000962987    steps: 357    lr: 0.0001     evaluation reward: 6.62\n",
            "episode: 1853   score: 6.0   memory length: 450655   epsilon: 0.3057011200096252    steps: 374    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1854   score: 8.0   memory length: 451064   epsilon: 0.30489130000962006    steps: 409    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1855   score: 7.0   memory length: 451440   epsilon: 0.30414682000961535    steps: 376    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1856   score: 4.0   memory length: 451696   epsilon: 0.30363994000961214    steps: 256    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1857   score: 6.0   memory length: 452035   epsilon: 0.3029687200096079    steps: 339    lr: 0.0001     evaluation reward: 6.44\n",
            "episode: 1858   score: 7.0   memory length: 452424   epsilon: 0.302198500009603    steps: 389    lr: 0.0001     evaluation reward: 6.42\n",
            "episode: 1859   score: 10.0   memory length: 452945   epsilon: 0.3011669200095965    steps: 521    lr: 0.0001     evaluation reward: 6.47\n",
            "episode: 1860   score: 13.0   memory length: 453447   epsilon: 0.3001729600095902    steps: 502    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1861   score: 16.0   memory length: 454060   epsilon: 0.2989592200095825    steps: 613    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1862   score: 5.0   memory length: 454331   epsilon: 0.29842264000957913    steps: 271    lr: 0.0001     evaluation reward: 6.63\n",
            "episode: 1863   score: 13.0   memory length: 454827   epsilon: 0.2974405600095729    steps: 496    lr: 0.0001     evaluation reward: 6.69\n",
            "episode: 1864   score: 4.0   memory length: 455104   epsilon: 0.29689210000956945    steps: 277    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1865   score: 6.0   memory length: 455451   epsilon: 0.2962050400095651    steps: 347    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1866   score: 12.0   memory length: 455893   epsilon: 0.29532988000955956    steps: 442    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1867   score: 11.0   memory length: 456322   epsilon: 0.2944804600095542    steps: 429    lr: 0.0001     evaluation reward: 6.77\n",
            "episode: 1868   score: 4.0   memory length: 456583   epsilon: 0.2939636800095509    steps: 261    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1869   score: 7.0   memory length: 456958   epsilon: 0.2932211800095462    steps: 375    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1870   score: 8.0   memory length: 457405   epsilon: 0.2923361200095406    steps: 447    lr: 0.0001     evaluation reward: 6.73\n",
            "episode: 1871   score: 9.0   memory length: 457880   epsilon: 0.2913956200095347    steps: 475    lr: 0.0001     evaluation reward: 6.77\n",
            "episode: 1872   score: 6.0   memory length: 458198   epsilon: 0.2907659800095307    steps: 318    lr: 0.0001     evaluation reward: 6.74\n",
            "episode: 1873   score: 10.0   memory length: 458525   epsilon: 0.2901185200095266    steps: 327    lr: 0.0001     evaluation reward: 6.78\n",
            "episode: 1874   score: 5.0   memory length: 458834   epsilon: 0.2895067000095227    steps: 309    lr: 0.0001     evaluation reward: 6.79\n",
            "episode: 1875   score: 7.0   memory length: 459197   epsilon: 0.2887879600095182    steps: 363    lr: 0.0001     evaluation reward: 6.79\n",
            "episode: 1876   score: 9.0   memory length: 459629   epsilon: 0.28793260000951276    steps: 432    lr: 0.0001     evaluation reward: 6.81\n",
            "episode: 1877   score: 7.0   memory length: 460033   epsilon: 0.2871326800095077    steps: 404    lr: 0.0001     evaluation reward: 6.8\n",
            "episode: 1878   score: 11.0   memory length: 460587   epsilon: 0.28603576000950076    steps: 554    lr: 0.0001     evaluation reward: 6.84\n",
            "episode: 1879   score: 7.0   memory length: 460993   epsilon: 0.2852318800094957    steps: 406    lr: 0.0001     evaluation reward: 6.85\n",
            "episode: 1880   score: 15.0   memory length: 461558   epsilon: 0.2841131800094886    steps: 565    lr: 0.0001     evaluation reward: 6.97\n",
            "episode: 1881   score: 7.0   memory length: 461943   epsilon: 0.2833508800094838    steps: 385    lr: 0.0001     evaluation reward: 6.97\n",
            "episode: 1882   score: 5.0   memory length: 462266   epsilon: 0.28271134000947973    steps: 323    lr: 0.0001     evaluation reward: 6.95\n",
            "episode: 1883   score: 3.0   memory length: 462477   epsilon: 0.2822935600094771    steps: 211    lr: 0.0001     evaluation reward: 6.92\n",
            "episode: 1884   score: 7.0   memory length: 462883   epsilon: 0.281489680009472    steps: 406    lr: 0.0001     evaluation reward: 6.95\n",
            "episode: 1885   score: 4.0   memory length: 463142   epsilon: 0.28097686000946875    steps: 259    lr: 0.0001     evaluation reward: 6.95\n",
            "episode: 1886   score: 10.0   memory length: 463604   epsilon: 0.28006210000946297    steps: 462    lr: 0.0001     evaluation reward: 6.96\n",
            "episode: 1887   score: 6.0   memory length: 463945   epsilon: 0.2793869200094587    steps: 341    lr: 0.0001     evaluation reward: 6.96\n",
            "episode: 1888   score: 5.0   memory length: 464253   epsilon: 0.27877708000945484    steps: 308    lr: 0.0001     evaluation reward: 6.96\n",
            "episode: 1889   score: 5.0   memory length: 464602   epsilon: 0.27808606000945046    steps: 349    lr: 0.0001     evaluation reward: 6.95\n",
            "episode: 1890   score: 12.0   memory length: 465204   epsilon: 0.2768941000094429    steps: 602    lr: 0.0001     evaluation reward: 6.99\n",
            "episode: 1891   score: 13.0   memory length: 465823   epsilon: 0.27566848000943517    steps: 619    lr: 0.0001     evaluation reward: 7.08\n",
            "episode: 1892   score: 8.0   memory length: 466259   epsilon: 0.2748052000094297    steps: 436    lr: 0.0001     evaluation reward: 7.09\n",
            "episode: 1893   score: 3.0   memory length: 466471   epsilon: 0.27438544000942705    steps: 212    lr: 0.0001     evaluation reward: 7.08\n",
            "episode: 1894   score: 8.0   memory length: 466914   epsilon: 0.2735083000094215    steps: 443    lr: 0.0001     evaluation reward: 7.05\n",
            "episode: 1895   score: 14.0   memory length: 467362   epsilon: 0.2726212600094159    steps: 448    lr: 0.0001     evaluation reward: 7.15\n",
            "episode: 1896   score: 6.0   memory length: 467705   epsilon: 0.2719421200094116    steps: 343    lr: 0.0001     evaluation reward: 7.15\n",
            "episode: 1897   score: 5.0   memory length: 468028   epsilon: 0.27130258000940755    steps: 323    lr: 0.0001     evaluation reward: 7.11\n",
            "episode: 1898   score: 4.0   memory length: 468303   epsilon: 0.2707580800094041    steps: 275    lr: 0.0001     evaluation reward: 7.11\n",
            "episode: 1899   score: 5.0   memory length: 468615   epsilon: 0.2701403200094002    steps: 312    lr: 0.0001     evaluation reward: 7.03\n",
            "episode: 1900   score: 9.0   memory length: 469094   epsilon: 0.2691919000093942    steps: 479    lr: 0.0001     evaluation reward: 7.05\n",
            "episode: 1901   score: 10.0   memory length: 469611   epsilon: 0.2681682400093877    steps: 517    lr: 0.0001     evaluation reward: 7.11\n",
            "episode: 1902   score: 8.0   memory length: 470025   epsilon: 0.26734852000938253    steps: 414    lr: 0.0001     evaluation reward: 7.13\n",
            "episode: 1903   score: 6.0   memory length: 470365   epsilon: 0.26667532000937827    steps: 340    lr: 0.0001     evaluation reward: 7.1\n",
            "episode: 1904   score: 10.0   memory length: 470727   epsilon: 0.26595856000937373    steps: 362    lr: 0.0001     evaluation reward: 7.16\n",
            "episode: 1905   score: 5.0   memory length: 471051   epsilon: 0.2653170400093697    steps: 324    lr: 0.0001     evaluation reward: 7.13\n",
            "episode: 1906   score: 7.0   memory length: 471426   epsilon: 0.264574540009365    steps: 375    lr: 0.0001     evaluation reward: 7.15\n",
            "episode: 1907   score: 10.0   memory length: 471948   epsilon: 0.26354098000935844    steps: 522    lr: 0.0001     evaluation reward: 7.14\n",
            "episode: 1908   score: 7.0   memory length: 472323   epsilon: 0.26279848000935374    steps: 375    lr: 0.0001     evaluation reward: 7.17\n",
            "episode: 1909   score: 7.0   memory length: 472701   epsilon: 0.262050040009349    steps: 378    lr: 0.0001     evaluation reward: 7.16\n",
            "episode: 1910   score: 6.0   memory length: 473065   epsilon: 0.26132932000934445    steps: 364    lr: 0.0001     evaluation reward: 7.14\n",
            "episode: 1911   score: 8.0   memory length: 473507   epsilon: 0.2604541600093389    steps: 442    lr: 0.0001     evaluation reward: 7.13\n",
            "episode: 1912   score: 5.0   memory length: 473853   epsilon: 0.2597690800093346    steps: 346    lr: 0.0001     evaluation reward: 7.12\n",
            "episode: 1913   score: 14.0   memory length: 474381   epsilon: 0.25872364000932796    steps: 528    lr: 0.0001     evaluation reward: 7.23\n",
            "episode: 1914   score: 6.0   memory length: 474724   epsilon: 0.25804450000932366    steps: 343    lr: 0.0001     evaluation reward: 7.23\n",
            "episode: 1915   score: 6.0   memory length: 475077   epsilon: 0.25734556000931924    steps: 353    lr: 0.0001     evaluation reward: 7.2\n",
            "episode: 1916   score: 7.0   memory length: 475479   epsilon: 0.2565496000093142    steps: 402    lr: 0.0001     evaluation reward: 7.18\n",
            "episode: 1917   score: 11.0   memory length: 476040   epsilon: 0.2554388200093072    steps: 561    lr: 0.0001     evaluation reward: 7.21\n",
            "episode: 1918   score: 10.0   memory length: 476533   epsilon: 0.254462680009301    steps: 493    lr: 0.0001     evaluation reward: 7.19\n",
            "episode: 1919   score: 5.0   memory length: 476857   epsilon: 0.25382116000929694    steps: 324    lr: 0.0001     evaluation reward: 7.19\n",
            "episode: 1920   score: 4.0   memory length: 477119   epsilon: 0.25330240000929366    steps: 262    lr: 0.0001     evaluation reward: 7.17\n",
            "episode: 1921   score: 5.0   memory length: 477425   epsilon: 0.2526965200092898    steps: 306    lr: 0.0001     evaluation reward: 7.17\n",
            "episode: 1922   score: 5.0   memory length: 477716   epsilon: 0.2521203400092862    steps: 291    lr: 0.0001     evaluation reward: 7.14\n",
            "episode: 1923   score: 8.0   memory length: 478150   epsilon: 0.25126102000928074    steps: 434    lr: 0.0001     evaluation reward: 7.15\n",
            "episode: 1924   score: 7.0   memory length: 478539   epsilon: 0.25049080000927587    steps: 389    lr: 0.0001     evaluation reward: 7.16\n",
            "episode: 1925   score: 11.0   memory length: 479064   epsilon: 0.2494513000092693    steps: 525    lr: 0.0001     evaluation reward: 7.2\n",
            "episode: 1926   score: 10.0   memory length: 479541   epsilon: 0.24850684000926332    steps: 477    lr: 0.0001     evaluation reward: 7.22\n",
            "episode: 1927   score: 12.0   memory length: 480124   epsilon: 0.24735250000925602    steps: 583    lr: 0.0001     evaluation reward: 7.27\n",
            "episode: 1928   score: 10.0   memory length: 480617   epsilon: 0.24637636000924984    steps: 493    lr: 0.0001     evaluation reward: 7.31\n",
            "episode: 1929   score: 8.0   memory length: 481097   epsilon: 0.24542596000924383    steps: 480    lr: 0.0001     evaluation reward: 7.32\n",
            "episode: 1930   score: 5.0   memory length: 481424   epsilon: 0.24477850000923973    steps: 327    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 1931   score: 8.0   memory length: 481811   epsilon: 0.24401224000923488    steps: 387    lr: 0.0001     evaluation reward: 7.32\n",
            "episode: 1932   score: 6.0   memory length: 482184   epsilon: 0.2432737000092302    steps: 373    lr: 0.0001     evaluation reward: 7.34\n",
            "episode: 1933   score: 13.0   memory length: 482712   epsilon: 0.2422282600092236    steps: 528    lr: 0.0001     evaluation reward: 7.43\n",
            "episode: 1934   score: 6.0   memory length: 483039   epsilon: 0.2415808000092195    steps: 327    lr: 0.0001     evaluation reward: 7.44\n",
            "episode: 1935   score: 4.0   memory length: 483280   epsilon: 0.24110362000921648    steps: 241    lr: 0.0001     evaluation reward: 7.44\n",
            "episode: 1936   score: 4.0   memory length: 483519   epsilon: 0.24063040000921349    steps: 239    lr: 0.0001     evaluation reward: 7.44\n",
            "episode: 1937   score: 6.0   memory length: 483863   epsilon: 0.23994928000920918    steps: 344    lr: 0.0001     evaluation reward: 7.46\n",
            "episode: 1938   score: 6.0   memory length: 484183   epsilon: 0.23931568000920517    steps: 320    lr: 0.0001     evaluation reward: 7.47\n",
            "episode: 1939   score: 11.0   memory length: 484591   epsilon: 0.23850784000920006    steps: 408    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 1940   score: 5.0   memory length: 484919   epsilon: 0.23785840000919595    steps: 328    lr: 0.0001     evaluation reward: 7.46\n",
            "episode: 1941   score: 11.0   memory length: 485474   epsilon: 0.236759500009189    steps: 555    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 1942   score: 9.0   memory length: 485945   epsilon: 0.2358269200091831    steps: 471    lr: 0.0001     evaluation reward: 7.55\n",
            "episode: 1943   score: 9.0   memory length: 486353   epsilon: 0.23501908000917798    steps: 408    lr: 0.0001     evaluation reward: 7.55\n",
            "episode: 1944   score: 4.0   memory length: 486614   epsilon: 0.2345023000091747    steps: 261    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 1945   score: 5.0   memory length: 486924   epsilon: 0.23388850000917083    steps: 310    lr: 0.0001     evaluation reward: 7.51\n",
            "episode: 1946   score: 10.0   memory length: 487405   epsilon: 0.2329361200091648    steps: 481    lr: 0.0001     evaluation reward: 7.54\n",
            "episode: 1947   score: 8.0   memory length: 487827   epsilon: 0.23210056000915952    steps: 422    lr: 0.0001     evaluation reward: 7.57\n",
            "episode: 1948   score: 10.0   memory length: 488354   epsilon: 0.23105710000915292    steps: 527    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1949   score: 6.0   memory length: 488691   epsilon: 0.2303898400091487    steps: 337    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1950   score: 5.0   memory length: 488994   epsilon: 0.2297899000091449    steps: 303    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1951   score: 6.0   memory length: 489350   epsilon: 0.22908502000914044    steps: 356    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 1952   score: 5.0   memory length: 489659   epsilon: 0.22847320000913657    steps: 309    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1953   score: 8.0   memory length: 490102   epsilon: 0.22759606000913102    steps: 443    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 1954   score: 9.0   memory length: 490627   epsilon: 0.22655656000912444    steps: 525    lr: 0.0001     evaluation reward: 7.65\n",
            "episode: 1955   score: 5.0   memory length: 490951   epsilon: 0.22591504000912038    steps: 324    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1956   score: 8.0   memory length: 491421   epsilon: 0.2249844400091145    steps: 470    lr: 0.0001     evaluation reward: 7.67\n",
            "episode: 1957   score: 10.0   memory length: 491948   epsilon: 0.2239409800091079    steps: 527    lr: 0.0001     evaluation reward: 7.71\n",
            "episode: 1958   score: 4.0   memory length: 492208   epsilon: 0.22342618000910464    steps: 260    lr: 0.0001     evaluation reward: 7.68\n",
            "episode: 1959   score: 12.0   memory length: 492759   epsilon: 0.22233520000909773    steps: 551    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1960   score: 5.0   memory length: 493103   epsilon: 0.22165408000909342    steps: 344    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1961   score: 13.0   memory length: 493587   epsilon: 0.22069576000908736    steps: 484    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 1962   score: 10.0   memory length: 494090   epsilon: 0.21969982000908106    steps: 503    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 1963   score: 8.0   memory length: 494496   epsilon: 0.21889594000907597    steps: 406    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 1964   score: 6.0   memory length: 494871   epsilon: 0.21815344000907128    steps: 375    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1965   score: 10.0   memory length: 495226   epsilon: 0.21745054000906683    steps: 355    lr: 0.0001     evaluation reward: 7.65\n",
            "episode: 1966   score: 10.0   memory length: 495698   epsilon: 0.21651598000906092    steps: 472    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1967   score: 8.0   memory length: 496100   epsilon: 0.21572002000905588    steps: 402    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 1968   score: 13.0   memory length: 496613   epsilon: 0.21470428000904945    steps: 513    lr: 0.0001     evaluation reward: 7.69\n",
            "episode: 1969   score: 8.0   memory length: 497034   epsilon: 0.21387070000904418    steps: 421    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1970   score: 10.0   memory length: 497519   epsilon: 0.2129104000090381    steps: 485    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1971   score: 7.0   memory length: 497894   epsilon: 0.2121679000090334    steps: 375    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1972   score: 8.0   memory length: 498262   epsilon: 0.2114392600090288    steps: 368    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1973   score: 9.0   memory length: 498735   epsilon: 0.21050272000902287    steps: 473    lr: 0.0001     evaluation reward: 7.71\n",
            "episode: 1974   score: 6.0   memory length: 499113   epsilon: 0.20975428000901813    steps: 378    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1975   score: 9.0   memory length: 499605   epsilon: 0.20878012000901197    steps: 492    lr: 0.0001     evaluation reward: 7.74\n",
            "episode: 1976   score: 5.0   memory length: 499908   epsilon: 0.20818018000900818    steps: 303    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1977   score: 9.0   memory length: 500377   epsilon: 0.2072515600090023    steps: 469    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1978   score: 11.0   memory length: 500895   epsilon: 0.2062259200089958    steps: 518    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1979   score: 9.0   memory length: 501350   epsilon: 0.2053250200089901    steps: 455    lr: 0.0001     evaluation reward: 7.74\n",
            "episode: 1980   score: 3.0   memory length: 501561   epsilon: 0.20490724000898747    steps: 211    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1981   score: 8.0   memory length: 502015   epsilon: 0.20400832000898178    steps: 454    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1982   score: 8.0   memory length: 502310   epsilon: 0.20342422000897808    steps: 295    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 1983   score: 9.0   memory length: 502777   epsilon: 0.20249956000897223    steps: 467    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1984   score: 7.0   memory length: 503186   epsilon: 0.2016897400089671    steps: 409    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1985   score: 7.0   memory length: 503606   epsilon: 0.20085814000896185    steps: 420    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 1986   score: 5.0   memory length: 503878   epsilon: 0.20031958000895844    steps: 272    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1987   score: 5.0   memory length: 504150   epsilon: 0.19978102000895503    steps: 272    lr: 0.0001     evaluation reward: 7.69\n",
            "episode: 1988   score: 7.0   memory length: 504397   epsilon: 0.19929196000895194    steps: 247    lr: 0.0001     evaluation reward: 7.71\n",
            "episode: 1989   score: 6.0   memory length: 504771   epsilon: 0.19855144000894726    steps: 374    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 1990   score: 9.0   memory length: 505209   epsilon: 0.19768420000894177    steps: 438    lr: 0.0001     evaluation reward: 7.69\n",
            "episode: 1991   score: 8.0   memory length: 505642   epsilon: 0.19682686000893634    steps: 433    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 1992   score: 7.0   memory length: 505998   epsilon: 0.19612198000893188    steps: 356    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1993   score: 6.0   memory length: 506337   epsilon: 0.19545076000892764    steps: 339    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 1994   score: 4.0   memory length: 506631   epsilon: 0.19486864000892395    steps: 294    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1995   score: 11.0   memory length: 507203   epsilon: 0.1937360800089168    steps: 572    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 1996   score: 7.0   memory length: 507599   epsilon: 0.19295200000891183    steps: 396    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 1997   score: 5.0   memory length: 507925   epsilon: 0.19230652000890774    steps: 326    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 1998   score: 6.0   memory length: 508268   epsilon: 0.19162738000890345    steps: 343    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 1999   score: 4.0   memory length: 508530   epsilon: 0.19110862000890017    steps: 262    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 2000   score: 5.0   memory length: 508857   epsilon: 0.19046116000889607    steps: 327    lr: 0.0001     evaluation reward: 7.57\n",
            "episode: 2001   score: 3.0   memory length: 509086   epsilon: 0.1900077400088932    steps: 229    lr: 0.0001     evaluation reward: 7.5\n",
            "episode: 2002   score: 6.0   memory length: 509425   epsilon: 0.18933652000888895    steps: 339    lr: 0.0001     evaluation reward: 7.48\n",
            "episode: 2003   score: 15.0   memory length: 510016   epsilon: 0.18816634000888155    steps: 591    lr: 0.0001     evaluation reward: 7.57\n",
            "episode: 2004   score: 7.0   memory length: 510420   epsilon: 0.1873664200088765    steps: 404    lr: 0.0001     evaluation reward: 7.54\n",
            "episode: 2005   score: 11.0   memory length: 510993   epsilon: 0.1862318800088693    steps: 573    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 2006   score: 7.0   memory length: 511398   epsilon: 0.18542998000886424    steps: 405    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 2007   score: 9.0   memory length: 511838   epsilon: 0.18455878000885872    steps: 440    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 2008   score: 9.0   memory length: 512293   epsilon: 0.18365788000885303    steps: 455    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 2009   score: 12.0   memory length: 512782   epsilon: 0.1826896600088469    steps: 489    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 2010   score: 6.0   memory length: 513098   epsilon: 0.18206398000884294    steps: 316    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 2011   score: 8.0   memory length: 513490   epsilon: 0.18128782000883803    steps: 392    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 2012   score: 3.0   memory length: 513717   epsilon: 0.1808383600088352    steps: 227    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 2013   score: 4.0   memory length: 513974   epsilon: 0.18032950000883197    steps: 257    lr: 0.0001     evaluation reward: 7.54\n",
            "episode: 2014   score: 5.0   memory length: 514283   epsilon: 0.1797176800088281    steps: 309    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 2015   score: 3.0   memory length: 514533   epsilon: 0.17922268000882496    steps: 250    lr: 0.0001     evaluation reward: 7.5\n",
            "episode: 2016   score: 6.0   memory length: 514930   epsilon: 0.17843662000882    steps: 397    lr: 0.0001     evaluation reward: 7.49\n",
            "episode: 2017   score: 9.0   memory length: 515366   epsilon: 0.17757334000881453    steps: 436    lr: 0.0001     evaluation reward: 7.47\n",
            "episode: 2018   score: 10.0   memory length: 515842   epsilon: 0.17663086000880857    steps: 476    lr: 0.0001     evaluation reward: 7.47\n",
            "episode: 2019   score: 9.0   memory length: 516349   epsilon: 0.17562700000880221    steps: 507    lr: 0.0001     evaluation reward: 7.51\n",
            "episode: 2020   score: 5.0   memory length: 516640   epsilon: 0.17505082000879857    steps: 291    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 2021   score: 4.0   memory length: 516897   epsilon: 0.17454196000879535    steps: 257    lr: 0.0001     evaluation reward: 7.51\n",
            "episode: 2022   score: 9.0   memory length: 517356   epsilon: 0.1736331400087896    steps: 459    lr: 0.0001     evaluation reward: 7.55\n",
            "episode: 2023   score: 8.0   memory length: 517812   epsilon: 0.1727302600087839    steps: 456    lr: 0.0001     evaluation reward: 7.55\n",
            "episode: 2024   score: 4.0   memory length: 518087   epsilon: 0.17218576000878044    steps: 275    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 2025   score: 6.0   memory length: 518432   epsilon: 0.17150266000877612    steps: 345    lr: 0.0001     evaluation reward: 7.47\n",
            "episode: 2026   score: 7.0   memory length: 518823   epsilon: 0.17072848000877122    steps: 391    lr: 0.0001     evaluation reward: 7.44\n",
            "episode: 2027   score: 6.0   memory length: 519180   epsilon: 0.17002162000876675    steps: 357    lr: 0.0001     evaluation reward: 7.38\n",
            "episode: 2028   score: 6.0   memory length: 519560   epsilon: 0.169269220008762    steps: 380    lr: 0.0001     evaluation reward: 7.34\n",
            "episode: 2029   score: 3.0   memory length: 519773   epsilon: 0.16884748000875932    steps: 213    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 2030   score: 3.0   memory length: 519983   epsilon: 0.1684316800087567    steps: 210    lr: 0.0001     evaluation reward: 7.27\n",
            "episode: 2031   score: 8.0   memory length: 520453   epsilon: 0.1675010800087508    steps: 470    lr: 0.0001     evaluation reward: 7.27\n",
            "episode: 2032   score: 7.0   memory length: 520837   epsilon: 0.166740760008746    steps: 384    lr: 0.0001     evaluation reward: 7.28\n",
            "episode: 2033   score: 6.0   memory length: 521191   epsilon: 0.16603984000874156    steps: 354    lr: 0.0001     evaluation reward: 7.21\n",
            "episode: 2034   score: 9.0   memory length: 521601   epsilon: 0.16522804000873642    steps: 410    lr: 0.0001     evaluation reward: 7.24\n",
            "episode: 2035   score: 5.0   memory length: 521907   epsilon: 0.1646221600087326    steps: 306    lr: 0.0001     evaluation reward: 7.25\n",
            "episode: 2036   score: 8.0   memory length: 522340   epsilon: 0.16376482000872716    steps: 433    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 2037   score: 9.0   memory length: 522779   epsilon: 0.16289560000872166    steps: 439    lr: 0.0001     evaluation reward: 7.32\n",
            "episode: 2038   score: 9.0   memory length: 523271   epsilon: 0.1619214400087155    steps: 492    lr: 0.0001     evaluation reward: 7.35\n",
            "episode: 2039   score: 7.0   memory length: 523608   epsilon: 0.16125418000871128    steps: 337    lr: 0.0001     evaluation reward: 7.31\n",
            "episode: 2040   score: 5.0   memory length: 523881   epsilon: 0.16071364000870786    steps: 273    lr: 0.0001     evaluation reward: 7.31\n",
            "episode: 2041   score: 9.0   memory length: 524387   epsilon: 0.15971176000870152    steps: 506    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 2042   score: 11.0   memory length: 524765   epsilon: 0.15896332000869678    steps: 378    lr: 0.0001     evaluation reward: 7.31\n",
            "episode: 2043   score: 4.0   memory length: 525011   epsilon: 0.1584762400086937    steps: 246    lr: 0.0001     evaluation reward: 7.26\n",
            "episode: 2044   score: 7.0   memory length: 525438   epsilon: 0.15763078000868835    steps: 427    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 2045   score: 15.0   memory length: 526018   epsilon: 0.1564823800086811    steps: 580    lr: 0.0001     evaluation reward: 7.39\n",
            "episode: 2046   score: 10.0   memory length: 526550   epsilon: 0.15542902000867442    steps: 532    lr: 0.0001     evaluation reward: 7.39\n",
            "episode: 2047   score: 17.0   memory length: 527022   epsilon: 0.1544944600086685    steps: 472    lr: 0.0001     evaluation reward: 7.48\n",
            "episode: 2048   score: 5.0   memory length: 527348   epsilon: 0.15384898000866443    steps: 326    lr: 0.0001     evaluation reward: 7.43\n",
            "episode: 2049   score: 13.0   memory length: 527952   epsilon: 0.15265306000865686    steps: 604    lr: 0.0001     evaluation reward: 7.5\n",
            "episode: 2050   score: 7.0   memory length: 528347   epsilon: 0.1518709600086519    steps: 395    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 2051   score: 7.0   memory length: 528720   epsilon: 0.15113242000864724    steps: 373    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 2052   score: 8.0   memory length: 529146   epsilon: 0.1502889400086419    steps: 426    lr: 0.0001     evaluation reward: 7.56\n",
            "episode: 2053   score: 8.0   memory length: 529541   epsilon: 0.14950684000863695    steps: 395    lr: 0.0001     evaluation reward: 7.56\n",
            "episode: 2054   score: 5.0   memory length: 529815   epsilon: 0.14896432000863352    steps: 274    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 2055   score: 6.0   memory length: 530151   epsilon: 0.1482990400086293    steps: 336    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 2056   score: 6.0   memory length: 530484   epsilon: 0.14763970000862514    steps: 333    lr: 0.0001     evaluation reward: 7.51\n",
            "episode: 2057   score: 7.0   memory length: 530870   epsilon: 0.1468754200086203    steps: 386    lr: 0.0001     evaluation reward: 7.48\n",
            "episode: 2058   score: 5.0   memory length: 531194   epsilon: 0.14623390000861625    steps: 324    lr: 0.0001     evaluation reward: 7.49\n",
            "episode: 2059   score: 9.0   memory length: 531693   epsilon: 0.14524588000861    steps: 499    lr: 0.0001     evaluation reward: 7.46\n",
            "episode: 2060   score: 10.0   memory length: 532194   epsilon: 0.14425390000860372    steps: 501    lr: 0.0001     evaluation reward: 7.51\n",
            "episode: 2061   score: 15.0   memory length: 532785   epsilon: 0.14308372000859632    steps: 591    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 2062   score: 11.0   memory length: 533306   epsilon: 0.1420521400085898    steps: 521    lr: 0.0001     evaluation reward: 7.54\n",
            "episode: 2063   score: 6.0   memory length: 533663   epsilon: 0.14134528000858532    steps: 357    lr: 0.0001     evaluation reward: 7.52\n",
            "episode: 2064   score: 17.0   memory length: 534349   epsilon: 0.13998700000857672    steps: 686    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 2065   score: 12.0   memory length: 534899   epsilon: 0.13889800000856983    steps: 550    lr: 0.0001     evaluation reward: 7.65\n",
            "episode: 2066   score: 8.0   memory length: 535328   epsilon: 0.13804858000856446    steps: 429    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 2067   score: 8.0   memory length: 535779   epsilon: 0.1371556000085588    steps: 451    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 2068   score: 9.0   memory length: 536252   epsilon: 0.13621906000855288    steps: 473    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 2069   score: 11.0   memory length: 536783   epsilon: 0.13516768000854623    steps: 531    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 2070   score: 8.0   memory length: 537209   epsilon: 0.1343242000085409    steps: 426    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 2071   score: 6.0   memory length: 537565   epsilon: 0.13361932000853644    steps: 356    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 2072   score: 13.0   memory length: 538168   epsilon: 0.13242538000852888    steps: 603    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 2073   score: 9.0   memory length: 538635   epsilon: 0.13150072000852303    steps: 467    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 2074   score: 8.0   memory length: 539065   epsilon: 0.13064932000851764    steps: 430    lr: 0.0001     evaluation reward: 7.66\n",
            "episode: 2075   score: 4.0   memory length: 539340   epsilon: 0.1301048200085142    steps: 275    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 2076   score: 9.0   memory length: 539788   epsilon: 0.1292177800085086    steps: 448    lr: 0.0001     evaluation reward: 7.65\n",
            "episode: 2077   score: 7.0   memory length: 540166   epsilon: 0.12846934000850385    steps: 378    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 2078   score: 10.0   memory length: 540694   epsilon: 0.12742390000849724    steps: 528    lr: 0.0001     evaluation reward: 7.62\n",
            "episode: 2079   score: 10.0   memory length: 541245   epsilon: 0.12633292000849033    steps: 551    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 2080   score: 4.0   memory length: 541504   epsilon: 0.1258201000084871    steps: 259    lr: 0.0001     evaluation reward: 7.64\n",
            "episode: 2081   score: 13.0   memory length: 542103   epsilon: 0.12463408000848215    steps: 599    lr: 0.0001     evaluation reward: 7.69\n",
            "episode: 2082   score: 11.0   memory length: 542663   epsilon: 0.12352528000848291    steps: 560    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 2083   score: 5.0   memory length: 542953   epsilon: 0.1229510800084833    steps: 290    lr: 0.0001     evaluation reward: 7.68\n",
            "episode: 2084   score: 6.0   memory length: 543318   epsilon: 0.1222283800084838    steps: 365    lr: 0.0001     evaluation reward: 7.67\n",
            "episode: 2085   score: 10.0   memory length: 543673   epsilon: 0.12152548000848427    steps: 355    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 2086   score: 8.0   memory length: 544107   epsilon: 0.12066616000848486    steps: 434    lr: 0.0001     evaluation reward: 7.73\n",
            "episode: 2087   score: 7.0   memory length: 544495   epsilon: 0.11989792000848538    steps: 388    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 2088   score: 8.0   memory length: 544929   epsilon: 0.11903860000848597    steps: 434    lr: 0.0001     evaluation reward: 7.76\n",
            "episode: 2089   score: 8.0   memory length: 545371   epsilon: 0.11816344000848657    steps: 442    lr: 0.0001     evaluation reward: 7.78\n",
            "episode: 2090   score: 8.0   memory length: 545818   epsilon: 0.11727838000848717    steps: 447    lr: 0.0001     evaluation reward: 7.77\n",
            "episode: 2091   score: 9.0   memory length: 546266   epsilon: 0.11639134000848778    steps: 448    lr: 0.0001     evaluation reward: 7.78\n",
            "episode: 2092   score: 4.0   memory length: 546509   epsilon: 0.1159102000084881    steps: 243    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 2093   score: 6.0   memory length: 546866   epsilon: 0.11520334000848859    steps: 357    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 2094   score: 4.0   memory length: 547110   epsilon: 0.11472022000848892    steps: 244    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 2095   score: 8.0   memory length: 547536   epsilon: 0.11387674000848949    steps: 426    lr: 0.0001     evaluation reward: 7.72\n",
            "episode: 2096   score: 5.0   memory length: 547832   epsilon: 0.11329066000848989    steps: 296    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 2097   score: 12.0   memory length: 548429   epsilon: 0.1121086000084907    steps: 597    lr: 0.0001     evaluation reward: 7.77\n",
            "episode: 2098   score: 12.0   memory length: 549014   epsilon: 0.11095030000849149    steps: 585    lr: 0.0001     evaluation reward: 7.83\n",
            "episode: 2099   score: 9.0   memory length: 549486   epsilon: 0.11001574000849212    steps: 472    lr: 0.0001     evaluation reward: 7.88\n",
            "episode: 2100   score: 14.0   memory length: 550062   epsilon: 0.1088752600084929    steps: 576    lr: 0.0001     evaluation reward: 7.97\n",
            "episode: 2101   score: 9.0   memory length: 550513   epsilon: 0.10798228000849351    steps: 451    lr: 0.0001     evaluation reward: 8.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-14fb9157454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m### Change this save condition to whatever you prefer ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_reward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_reward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_eval_reward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./save_model/breakout_dqn.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mbest_eval_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './save_model/breakout_dqn.pth'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51UzWiOTbkGl"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVvAfV6pArad"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvVNwwnOArad"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBKwkh6yArad"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtuGGY7CArad"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK8DEF6bArad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "d5bfa77f-fee4-4d77-f4d4-e97c5861637b"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f58a5b19780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUNSq5qHArad"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}